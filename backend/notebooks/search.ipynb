{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_ingestion.vector_db import get_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['id12_0',\n",
       "  'id12_1',\n",
       "  'id12_2',\n",
       "  'id12_3',\n",
       "  'id12_4',\n",
       "  'id12_5',\n",
       "  'id12_6',\n",
       "  'id12_7',\n",
       "  'id12_8',\n",
       "  'id12_9',\n",
       "  'id12_10',\n",
       "  'id12_11',\n",
       "  'id14_0',\n",
       "  'id14_1',\n",
       "  'id14_2',\n",
       "  'id14_3',\n",
       "  'id14_4',\n",
       "  'id14_5',\n",
       "  'id14_6',\n",
       "  'id14_7',\n",
       "  'id14_8',\n",
       "  'id14_9',\n",
       "  'id14_10',\n",
       "  'id14_11',\n",
       "  'id4_0',\n",
       "  'id4_1',\n",
       "  'id4_2',\n",
       "  'id4_3',\n",
       "  'id4_4',\n",
       "  'id4_5',\n",
       "  'id4_6',\n",
       "  'id4_7',\n",
       "  'id4_8',\n",
       "  'id4_9',\n",
       "  'id4_10',\n",
       "  'id4_11',\n",
       "  'id4_12',\n",
       "  'id2_0',\n",
       "  'id2_1',\n",
       "  'id2_2',\n",
       "  'id2_3',\n",
       "  'id2_4',\n",
       "  'id2_5',\n",
       "  'id2_6',\n",
       "  'id2_7',\n",
       "  'id2_8',\n",
       "  'id2_9',\n",
       "  'id2_10',\n",
       "  'id2_11',\n",
       "  'id2_12',\n",
       "  'id2_13',\n",
       "  'id2_14',\n",
       "  'id10_0',\n",
       "  'id10_1',\n",
       "  'id10_2',\n",
       "  'id10_3',\n",
       "  'id10_4',\n",
       "  'id10_5',\n",
       "  'id10_6',\n",
       "  'id10_7',\n",
       "  'id10_8',\n",
       "  'id10_9',\n",
       "  'id10_10',\n",
       "  'id10_11',\n",
       "  'id10_12',\n",
       "  'id1_0',\n",
       "  'id1_1',\n",
       "  'id1_2',\n",
       "  'id1_3',\n",
       "  'id1_4',\n",
       "  'id1_5',\n",
       "  'id1_6',\n",
       "  'id1_7',\n",
       "  'id1_8',\n",
       "  'id1_9',\n",
       "  'id1_10',\n",
       "  'id1_11',\n",
       "  'id1_12',\n",
       "  'id1_13',\n",
       "  'id8_0',\n",
       "  'id8_1',\n",
       "  'id8_2',\n",
       "  'id8_3',\n",
       "  'id8_4',\n",
       "  'id8_5',\n",
       "  'id8_6',\n",
       "  'id8_7',\n",
       "  'id8_8',\n",
       "  'id8_9',\n",
       "  'id8_10',\n",
       "  'id8_11',\n",
       "  'id8_12',\n",
       "  'id8_13',\n",
       "  'id7_0',\n",
       "  'id7_1',\n",
       "  'id7_2',\n",
       "  'id7_3',\n",
       "  'id7_4',\n",
       "  'id7_5',\n",
       "  'id7_6',\n",
       "  'id7_7',\n",
       "  'id7_8',\n",
       "  'id7_9',\n",
       "  'id7_10',\n",
       "  'id7_11',\n",
       "  'id7_12',\n",
       "  'id11_0',\n",
       "  'id11_1',\n",
       "  'id11_2',\n",
       "  'id11_3',\n",
       "  'id11_4',\n",
       "  'id11_5',\n",
       "  'id11_6',\n",
       "  'id11_7',\n",
       "  'id11_8',\n",
       "  'id11_9',\n",
       "  'id11_10',\n",
       "  'id11_11',\n",
       "  'id11_12',\n",
       "  'id11_13',\n",
       "  'id5_0',\n",
       "  'id5_1',\n",
       "  'id5_2',\n",
       "  'id5_3',\n",
       "  'id5_4',\n",
       "  'id5_5',\n",
       "  'id5_6',\n",
       "  'id5_7',\n",
       "  'id5_8',\n",
       "  'id5_9',\n",
       "  'id5_10',\n",
       "  'id5_11',\n",
       "  'id5_12',\n",
       "  'id3_0',\n",
       "  'id3_1',\n",
       "  'id3_2',\n",
       "  'id3_3',\n",
       "  'id3_4',\n",
       "  'id3_5',\n",
       "  'id3_6',\n",
       "  'id3_7',\n",
       "  'id3_8',\n",
       "  'id3_9',\n",
       "  'id13_0',\n",
       "  'id13_1',\n",
       "  'id13_2',\n",
       "  'id13_3',\n",
       "  'id13_4',\n",
       "  'id13_5',\n",
       "  'id13_6',\n",
       "  'id13_7',\n",
       "  'id13_8',\n",
       "  'id13_9',\n",
       "  'id13_10',\n",
       "  'id15_0',\n",
       "  'id15_1',\n",
       "  'id15_2',\n",
       "  'id15_3',\n",
       "  'id15_4',\n",
       "  'id15_5',\n",
       "  'id15_6',\n",
       "  'id15_7',\n",
       "  'id15_8',\n",
       "  'id15_9',\n",
       "  'id15_10',\n",
       "  'id15_11',\n",
       "  'id6_0',\n",
       "  'id6_1',\n",
       "  'id6_2',\n",
       "  'id6_3',\n",
       "  'id6_4',\n",
       "  'id6_5',\n",
       "  'id6_6',\n",
       "  'id6_7',\n",
       "  'id6_8',\n",
       "  'id6_9',\n",
       "  'id6_10',\n",
       "  'id6_11',\n",
       "  'id9_0',\n",
       "  'id9_1',\n",
       "  'id9_2',\n",
       "  'id9_3',\n",
       "  'id9_4',\n",
       "  'id9_5',\n",
       "  'id9_6',\n",
       "  'id9_7',\n",
       "  'id9_8',\n",
       "  'id9_9',\n",
       "  'id9_10',\n",
       "  'id9_11',\n",
       "  'id9_12',\n",
       "  'id9_13',\n",
       "  'id0_0',\n",
       "  'id0_1',\n",
       "  'id0_2',\n",
       "  'id0_3',\n",
       "  'id0_4',\n",
       "  'id0_5',\n",
       "  'id0_6',\n",
       "  'id0_7',\n",
       "  'id0_8',\n",
       "  'id0_9',\n",
       "  'id0_10',\n",
       "  'id0_11',\n",
       "  'id0_12',\n",
       "  'id0_13',\n",
       "  'id0_14',\n",
       "  'id10_13',\n",
       "  'id1_14',\n",
       "  'id3_10',\n",
       "  'id3_11',\n",
       "  'id3_12',\n",
       "  'id3_13',\n",
       "  'id3_14',\n",
       "  'id13_11',\n",
       "  'id6_12',\n",
       "  'id0_15',\n",
       "  'id0_16',\n",
       "  'id12_12',\n",
       "  'id12_13'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['## Introduction\\nDear friends,\\nCongratulations to Geoff Hinton and John Hopfield for winning the 2024 Physics Nobel Prize! It‚Äôs wonderful to see pioneering work in AI recognized, and this will be good for our whole field. Years ago, I was the first to call Geoff the ‚ÄúGodfather of Deep Learning,‚Äù which later became ‚ÄúGodfather of AI.‚Äù I‚Äôm thrilled at the recognition he‚Äôs receiving via this most prestigious of awards.\\nAs Geoff relayed in the ‚ÄúHeroes of Deep Learning‚Äù interview I did with him years ago, his early work developing the foundations of neural networks has been instrumental to the rise of deep learning and AI. It has been years since I implemented a Hopfield network , but John‚Äôs work, too, has been influential. Their recognition is well deserved!\\nBut the Nobel committee wasn‚Äôt done yet. One day after the physics prize was announced, Demis Hassabis, John Jumper, and David Baker won the Chemistry Nobel Prize for their work on AlphaFold and protein design. AlphaFold and AlphaFold 2, as well as the work of Baker‚Äôs lab, are compelling applications of AI that made significant steps forward in chemistry and biology, and this award, too, is well deserved!\\nIt‚Äôs remarkable that the Nobel committees for physics and chemistry, which are made up of scientists in those fields, chose to honor AI researchers with this year‚Äôs awards. This is a sign of our field‚Äôs growing impact on society.',\n",
       "  \"While it‚Äôs good that people from outside AI are recognizing AI researchers, I wonder if there‚Äôs room for the AI community to pick more award recipients ourselves. Best-known in computer science is the Turing Award, which is selected by a broad group of computer scientists, many of whom have deep AI knowledge. Many AI conferences give out best-paper awards. And applications of AI to other fields doubtless will continue to receive much-deserved recognition by leaders in those fields. I‚Äôm optimistic this will allow AI researchers to win more Nobel Prizes ‚Äî someday also in economics, literature, medicine, and peace, too.\\xa0Nonetheless, this seems like a good time to see how all of us in AI can do more to recognize the work of innovators in our field.\\nGeoff once thanked me for my role in getting him anointed ‚ÄúGodfather of AI,‚Äù which he said was good for his career. I didn‚Äôt realize before that I had the power to give out such titles üòâ but I would love for there to be numerous godfathers and godmothers ‚Äî and many other awards ‚Äî in AI!\\nAt Geoff's retirement party last October (pictured in the photo above), I spoke with affection and gratitude for all the work he has done to grow AI. Even as we cheer the new Nobel wins for AI, let‚Äôs continue to think about how we in AI can do more to celebrate the next generation of innovators.\\nKeep learning!\\nAndrew\\nTry the new capabilities of Llama 3.2 in our latest course with Meta. Learn how to compose multimodal prompts, call custom tools, and use the Llama Stack API to build applications with Meta‚Äôs family of open weights models. Enroll for free!\",\n",
       "  '## Familiar Faces, Synthetic Soundtracks\\nMeta upped the ante for text-to-video generation with new systems that produce consistent characters and matching soundtracks.\\nWhat‚Äôs new: Meta presented Movie Gen , a series of four systems that generate videos, include consistent characters, alter generated imagery, and add matching sound effects and music. Movie Gen will be available on Instagram in 2025. Meanwhile, you can view and listen to examples here . The team explains how the model was built an extensive 92-page paper.\\nGenerated videos: Movie Gen Video can output 256 frames (up to 16 seconds at 16 frames per second) at 1920x1080-pixel resolution. It includes a convolutional neural network autoencoder, transformer, and multiple embedding models.\\n- Movie Gen Video produces imagery by flow matching, a technique related to diffusion. It learned to remove noise from noisy versions of images and videos given matching text descriptions from 1 billion image-text pairs and 100 million video-text pairs. At inference, it starts with pure noise and generates detailed imagery according to a text prompt.\\n- The system concatenates multiple text embeddings to combine the strengths of different embedding models. UL2 was trained on text-only data, so its embeddings may provide ‚Äúreasoning abilities,‚Äù according to the authors. Long-prompt MetaCLIP was trained to produce similar text and image representations, so its embeddings might be useful for ‚Äúcross-modal generation.‚Äù ByT5 produces embeddings of individual text elements such as letters, numbers, and symbols; the system uses it when a prompt requests text within a clip.\\nConsistent characters: Given an image of a face, a fine-tuned version of Movie Gen Video generates a video that depicts a person with that face.',\n",
       "  'Consistent characters: Given an image of a face, a fine-tuned version of Movie Gen Video generates a video that depicts a person with that face.\\n- To gather a training dataset for this capability, the team filtered Movie Gen Video‚Äôs pretraining dataset for clips that show a single face and consecutive frames are similar to one another. They built video-face examples by pairing each clip with a frame selected from the clip at random. To train the system, the team fed it text, the clip with added noise, and the single-frame face. It learned to remove the noise.\\n- Trained on this data alone, the system generated videos in which the person always faces the camera. To expand the variety of poses, they further trained it on examples that substituted the faces in the previous step with generated versions with alternate poses and facial expressions.\\nAltered clips: The team modified Movie Gen Video‚Äôs autoencoder to accept an embedding of an alteration ‚Äî say, changing the background or adding an object. They trained the system to alter videos in three stages:\\n- First, they trained the system, given a starting image and an instruction to alter it, to produce an altered image.\\n- They further trained the system to produce altered clips. They generated two datasets of before-and-after clips based on instructions. (i) For instance, given a random frame and an instruction to, say, replace a person with a cat, the system altered the frame accordingly. Then the team subjected both frames to a series of augmentations selected at random, creating matching clips, one featuring a person, the other featuring a cat. Given the initial clip and the instruction, the system learned to generate the altered clip. (ii) The team used DINO and SAM 2 to segment clips. Given an unsegmented clip and an instruction such as ‚Äúmark <object> with <color>,‚Äù the system learned to generate the segmented clip.',\n",
       "  '- Finally, they trained the system to restore altered clips to their original content. They built a dataset by taking a ground-truth clip and using their system to generate an altered version according to an instruction. Then Llama 3 rewrote the instruction to modify the altered clip to match the original. Given the altered clip and the instruction, the system learned to generate the original clip.\\nSynthetic soundtracks: Given a text description, a system called Movie Gen Audio generates sound effects and instrumental music for video clips up to 30 seconds long. It includes a DACVAE audio encoder (which encodes sounds that comes before and/or after the target audio), Long-prompt MetaCLIP video encoder, T5 text encoder, vanilla neural network that encodes the current time step, and transformer.\\n- Movie Gen Audio learned to remove noise from noisy versions of audio associated with 1 million videos with text captions.\\xa0 At inference, it starts with pure noise and generates up to 30 seconds of audio at once.\\n- At inference, it can extend audio. Given the last n seconds of audio, the associated portion of a video, and a text description, it can generate the next 30 - n seconds.\\nResults: Overall, Movie Gen achieved performance roughly equal to or better than competitors in qualitative evaluations of overall quality and a number of specific qualities (such as ‚Äúrealness‚Äù). Human evaluators rated their preferences for Movie Gen or a competitor. The team reported the results in terms of net win rate (win percentage minus loss percentage) between -100 percent and 100 percent, where a score above zero means that a system won more than it lost.\\n- For overall video quality, Movie Gen achieved a net win rate of 35.02 percent versus Runway Gen3, 8.23 percent versus Sora (based on the prompts and generated clips available on OpenAI‚Äôs website), and 3.87 percent versus Kling 1.5.',\n",
       "  '- Generating clips of specific characters, Movie Gen achieved a net win rate of 64.74 percent versus ID-Animator, the state of the art for this capability.\\n- Generating soundtracks for videos from the SReal SFX dataset, Movie Gen Audio achieved a net win rate between 32 percent and 85 percent compared to various video-to-audio models.\\n- Altering videos in the TGVE+ dataset, Movie Gen beat all competitors more than 70 percent of the time.\\nWhy it matters: With Movie Gen, table stakes for video generation rises to include consistent characters, soundtracks, and various video-to-video alterations. The 92-page paper is a valuable resource for builders of video generation systems, explaining in detail how the team filtered data, structured models, and trained them to achieve good results.\\nWe‚Äôre thinking: Meta has a great track record of publishing both model weights and papers that describe how the models were built. Kudos to the Movie Gen team for publishing the details of this work!',\n",
       "  \"## Voice-to-Voice and More for GPT-4o API\\nOpenAI launched a suite of new and updated tools to help AI developers build applications and reduce costs.\\nWhat‚Äôs new: At its annual DevDay conference, OpenAI\\xa0introduced an API for speech processing using GPT-4o, distillation tools , vision fine-tuning capabilities , and the ability to cache prompts for later re-use. These tools are designed to make it easier to build fast applications using audio inputs and outputs, customize models, and cut costs for common tasks.\\nDevelopment simplified: The new offerings aim to make it easier to build applications using OpenAI models, with an emphasis on voice input/output and image input, customizing models, and resolving common pain points.\\n- The Realtime API enables speech-to-speech interactions with GPT-4o using six preset voices, like ChatGPT's Advanced Voice Mode but with lower latency. The API costs $100/$200 per 1 million input/output tokens (about $0.06/$0.24 per minute of input/output). (The API processes text at $5/$20 per million input/output tokens.\\n- The Chat Completions API now accepts voice input and generates voice outputs for GPT-4o‚Äôs usual price ($3.75/$15 per million input/output tokens). However, it generates outputs less quickly than the Realtime API. (OpenAI didn‚Äôt disclose specific latency measurements.)\\n- The distillation tools simplify the process of using larger models like o1-preview as teachers whose output is used to fine-tune smaller, more cost-efficient students like GPT-4o mini. Developers can generate datasets, fine-tune models, and evaluate performance within OpenAI's platform. For example, you can use GPT-4o to create responses to customer-service questions, then use the resulting dataset to fine-tune GPT-4o mini.\",\n",
       "  \"- Vision fine-tuning allows developers to enhance GPT-4o's image understanding by fine-tuning the model on a custom image dataset. For instance, developers can improve visual search, object detection, or image analysis for a particular application by fine-tuning the model on domain-specific images. Vision fine-tuning costs $25 per million training tokens for GPT-4o, but OpenAI will give developers 1 million free training tokens per day through October 31.\\n- Prompt caching automatically reuses input tokens that were entered in recent interactions with GPT-4o, GPT-4o mini, and their fine-tuned variants. Repeated prompts cost half as much and get processed faster. The discount and speed especially benefit applications like chatbots and code editors, which frequently reuse input context.\\nBehind the news: OpenAI is undertaking a major corporate transformation. A recent funding round values OpenAI at $157 billion, making it among the world‚Äôs most valuable private companies, and the company is transferring more control from its nonprofit board to its for-profit subsidiary. Meanwhile, it has seen an exodus of executives that include CTO Mira Murati, Sora co-lead Tim Brooks, chief research officer Bob McGrew, research VP Barret Zoph, and other key researchers .\\nWhy it matters: The Realtime API enables speech input and output without converting speech to text, allowing for more natural voice interactions. Such interactions open a wide range of applications, and they‚Äôre crucial for real-time systems like customer service bots and virtual assistants. Although Amazon Web Service and Labelbox provide services to distill knowledge from OpenAI models into open architectures, OpenAI‚Äôs tools ease the process of distilling from OpenAI models into other OpenAI models. Image fine-tuning and prompt caching, like similar capabilities for Anthropic Claude and Google Gemini, are welcome additions.\",\n",
       "  'We‚Äôre thinking: OpenAI‚Äôs offerings have come a long way since DevDay 2023 , when speech recognition was ‚Äúcoming soon.‚Äù We‚Äôre eager to see what developers do with voice-driven applications!',\n",
       "  '## German Court: LAION Didn‚Äôt Violate Copyrights\\nA German court dismissed a copyright lawsuit against LAION, the nonprofit responsible for large-scale image datasets used to train Midjourney, Stable Diffusion, and other image generators.\\nWhat‚Äôs new: The court rejected a lawsuit claiming that cataloging images on the web to train machine learning models violates the image owners‚Äô copyrights. It ruled that LAION‚Äôs activities fall under protections for scientific research.\\nHow it works: LAION doesn‚Äôt distribute images. Instead, it compiles links to images and related text that are published on publicly available websites. Model builders who wish to use the images and/or text must download them from those sources. In 2023, photographer Robert Kneschke sued LAION for including his photos. The court‚Äôs decision emphasized several key points.\\n- LAION, while compiling links to images, had indeed made unauthorized copies of images protected by copyright, as defined by German law. However, Germany‚Äôs Copyright Act allows unauthorized use of copyrighted works for scientific research. The court ruled that LAION had collected the material for this purpose, so it did not violate copyrights.\\n- Moreover, the court found that downloading images and text in order to correlate them likely fell under a further exemption to copyright for data mining. This finding wasn‚Äôt definitive because the exemption for research made it irrelevant, but the court mentioned it to help guide future rulings.',\n",
       "  '- The dataset‚Äôs noncommercial status was a key factor in the ruling. LAION distributed the dataset for free, and no commercial entity controlled its operations. Although a LAION dataset may be used to train a machine learning model that‚Äôs intended to be sold commercially, this is not sufficient to classify creating such datasets as commercial activity. The plaintiff contended that, because some LAION members have paid roles in commercial companies, LAION could be considered a commercial entity. However, the court rejected that argument.\\nBehind the news: Several other artists have sued LAION , which stands for Large-scale AI Open Network, claiming that the organization used their works without their consent. They have also sued AI companies, including a class action suit against Stability AI, Midjourney, and DeviantArt for using materials under copyright, including images in LAION‚Äôs datasets, to train their models. Similar cases have been brought against makers of music generators and coding assistants . All these lawsuits, which are in progress, rest on the plaintiff‚Äôs claim that assembling a training dataset of copyrighted works infringes copyrights.\\nWhy it matters: The German ruling is the first AI-related decision in Europe since the adoption of the AI Act, and the court took that law‚Äôs intent into account when making its decision. It affirms that creating text-image pairs of publicly available material for the purpose of training machine learning models does not violate copyrights, even if commercial organizations later use the data. However, the court did not address whether training AI models on such datasets, or using the trained models in a commercial setting, violates copyrights.\\nWe‚Äôre thinking: This decision is encouraging news for AI researchers. We hope jurisdictions worldwide establish that training models on media that‚Äôs available on the open web is fair and legal.',\n",
       "  '## AI‚Äôs Criminal Underground Revealed\\nResearchers probed the black market for AI services that are designed to facilitate cybercrime.\\nWhat‚Äôs new : Zilong Lin and colleagues at Indiana University Bloomington studied how large language models (LLMs) are used to provide harmful services, specifically generating malicious code, phishing emails, and phishing websites. They weren‚Äôt very effective, by and large (though a high success rate may not be necessary to support a thriving market in automated criminal activity).\\nRisky business: Providers base such services on either uncensored LLMs ‚Äî that is, those that weren‚Äôt fine-tuned to reflect human preferences or don‚Äôt employ input/output filters ‚Äî or publicly available models that they prompt using jailbreak techniques that circumvent built-in guardrails. They sell their services in hacker‚Äôs marketplaces and forums, charging far less than typical traditional malware vendors, but services based on models that have been fine-tuned to deliver malicious output command a premium. The authors found that one\\xa0service generated revenue of more than $28,000 in two months.\\nSprawling market: The authors identified 212 harmful services. Of those, 125 were hosted on the Poe AI platform, 73 were on FlowGPT, and the remaining 14 resided on unique servers. Of those, the authors were unable to access five because either the provider blocked them, or the service was fraudulent. They identified 11 LLMs used by these services including Claude-2-100k, GPT-4, and Pygmalion-13B (a variant of LLaMA-13B).\\nTesting output quality: The authors prompted more than 200 services using over 30 prompts to generate malicious code, phishing emails, or phishing websites. They evaluated the responses according to:\\n- Format: How often they followed the expected format (as defined by regular expressions)\\n- Compilability: How often generated Python, C, or C++ code was able to compile',\n",
       "  \"## Introduction\\nDear friends,\\nLast week I spoke at Coursera Connect, the company‚Äôs annual conference in Las Vegas, where a major topic was AI and education. There has been a lot of hype about generative AI‚Äôs ability to transform industries overnight. Certainly many industries ‚Äî including education ‚Äî will be transformed. But we‚Äôre about 15 years into the deep learning revolution, and we‚Äôre not yet done identifying and building useful deep learning applications. Despite the exciting progress to date with generative AI, I expect that a decade from now we will still be far from finished identifying and building generative AI applications for education and numerous other sectors.\\nThis was the first time since 2019 that Coursera‚Äôs conference was held in person. It was great to see so many people dedicated to the educational mission coming together to discuss innovations, including generative AI innovations, that serve learners.\\nCoursera‚Äôs CEO Jeff Maggioncalda and the company‚Äôs executive team demonstrated multiple generative AI products, such as:\\n- Coursera Coach, a chatbot that understands the context of a learner's journey and answers their questions (without giving away exact answers to quiz questions!)\\n- Course Builder, which businesses are using to customize long courses or specializations quickly, for example, by selecting the parts most relevant to their business\\n- Coach for Interactive Instruction, which lets learners have a Socratic dialog and learn or practice new concepts in conversation\\nBecause AI is a general-purpose technology, there are many opportunities to apply it to different tasks in education. I was thrilled at the volume of experimentation happening across Coursera, DeepLearning.AI, and the broader ecosystem of partners and customers. I was also proud to present awards to many partners and customers who are doing great work to serve learners.\",\n",
       "  'I was particularly gratified by the number of people coming together in service of the education mission. Even before the recent rise of AI, education was already urgently in need of improvement. With AI transforming jobs, the need has become even more acute. My heart was warmed by the conversations I had with many people from universities, high schools, businesses, and the Coursera team who have a deep desire to help others through education.\\nCoursera held its first conference in 2013, when the online education movement was in its early days, and we all had high hopes for where it could go. Today, there are over 155 million learners on Coursera. Despite that, given society‚Äôs heightened need for education and AI‚Äôs potential to transform the field, I feel the opportunities for edtech at this moment are greater than at any moment over the past decade.\\nKeep learning!\\nAndrew\\nP.S. I‚Äôm excited to announce our new specialization, Generative AI for Software Development , taught by Laurence Moroney! Using chatbots to generate code is not the only way AI can help developers. This three-course series shows you how to use AI throughout the software development lifecycle ‚Äì from design and architecture to coding, testing, deployment, and maintenance. Everyone who writes software can benefit from these skills. Please sign up here !\\nGenerative AI for Software Development , our new skill certificate, gives you practical experience applying AI to coding, debugging, optimization, and documentation as it explores AI‚Äôs role across the entire development lifecycle‚Äîdesign, architecture, coding, testing, deployment, and maintenance. Equip yourself with the tools to enhance every step of your dev workflow. Enroll now',\n",
       "  '## California Restricts Deepfakes\\nCalifornia, a jurisdiction that often influences legislators worldwide, passed a slew of new laws that regulate deepfakes.\\nWhat‚Äôs new: California Governor Gavin Newsom signed into law eight bills that aim to curb the use of generative AI in politics and entertainment .\\nHow it works: The legislation prohibits deceptive AI-generated media in political campaigns; requires permission for using digital stand-ins for actors, musicians, and other entertainers; and criminalizes generation of sexually explicit imagery without the subject‚Äôs consent.\\n- One law prohibits knowingly distributing deceptive AI-generated information about candidates, elections officials, or voting processes between 120 days before and 60 days after elections. The bill defines ‚Äúmaterially deceptive content‚Äù as images, audio, or video that were intentionally created or modified but would appear to a reasonable person to be authentic.\\n- Two related laws mandate disclosure when AI is used to produce political advertisements. The first requires that AI-generated campaign ads include the statement, ‚Äúad generated or substantially altered using artificial intelligence.‚Äù The other calls for large online platforms to label or remove AI-generated media related to elections.\\n- Two further laws protect performers by controlling ‚Äúdigital replicas,‚Äù defined as ‚Äúcomputer-generated, highly realistic electronic representation[s] of an individual‚Äôs voice or likeness.‚Äù One voids contracts for the use of digital replicas if performers didn‚Äôt have legal or union representation when they made the agreements. The other prohibits commercial use of deceased performers‚Äô digital replicas without permission of their estates.',\n",
       "  '- Two laws regulate sexually explicit synthetic content. One establishes the creation and distribution of non-consensual, AI-generated sexually explicit content as a disorderly conduct misdemeanor. The other requires social media platforms to report sexually explicit deepfakes.\\n- An additional law requires that AI-generated media include a disclosure of its provenance.\\nBehind the news: Newsom has not yet acted on Senate Bill 1047, a controversial law that would impose significant burdens on AI model developers. He has expressed that the bill could interfere with innovation, especially with respect to open source projects.\\nWhy it matters: Laws passed in California often point the way for legislators in other U.S. states, the federal government, and consequently other countries. The new laws that regulate deepfakes in political campaigns fill a gap left by the Federal Election Commission (FEC), which has said it lacks authority to regulate the use of AI in political ads. Meanwhile, the Federal Communications Commission (FCC) proposed rules that would mandate disclosure of uses of AI in political ads but has yet to implement them.\\nWe‚Äôre thinking: We‚Äôre glad to see California target undesirable applications rather than AI models. Regulating applications rather than general-purpose technology that has a wide variety of uses ‚Äî many of which are beneficial ‚Äî avoids the dangers of California SB-1047, which is still awaiting the governor‚Äôs signature or veto. That law, which seeks to restrict AI models, would endanger innovation and especially open source.',\n",
       "  '## More, Better Open Source Options\\nThe parade of ever more capable LLMs continues with Qwen 2.5.\\nWhat‚Äôs new: Alibaba released Qwen 2.5 in several sizes, the API variants Qwen Plus and Qwen Turbo, and the specialized models Qwen 2.5-Coder and Qwen 2.5-Coder-Instruct and Qwen 2.5-Math and Qwen 2.5-Math-Instruct . Many are freely available for commercial use under the Apache 2.0 license here . The 3B and 72B models are also free, but their license requires special arrangements for commercial use.\\nHow it works: The Qwen 2.5 family ranges from 500 million parameters to 72 billion parameters.\\n- Qwen 2.5 models were pretrained on 18 trillion tokens. Sizes up to 3 billion parameters can process up to 32,000 input tokens; the larger models can process up to 128,000 input tokens. All versions can have an output length of 8,000 tokens.\\n- Qwen 2.5-Coder was further pretrained on 5.5 trillion tokens of code. It can process up to 128,000 input tokens and generate up to 2,000 output tokens. It comes in 1.5B and 7B versions.\\n- Qwen 2.5-Math further pretrained on 1 trillion tokens of math problems, including Chinese math problems scraped from the web and generated by the earlier Qwen 2-Math-72B-Instruct. Qwen 2.5-Math can process 4,000 input tokens and generate up to 2,000 output tokens. It comes in 1.5B, 7B, and 72B versions. In addition to solving math problems, Qwen 2.5-Math can generate code to help solve a given math problem.',\n",
       "  'Results: Compared to other models with open weights, Qwen 2.5-72B-Instruct beats LLama 3.1 405B Instruct and Mistral Large 2 Instruct (123 billion parameters) on seven of 14 benchmarks including LiveCodeBench , MATH (solving math word problems), and MMLU (answering questions on a variety of topics). Compared to other models that respond to API calls, Qwen-Plus beats LLama 3.1 405B, Claude 3.5 Sonnet, and GPT-4o on MATH, LiveCodeBench, and ArenaHard . Smaller versions also deliver outstanding performance. For instance, Qwen 2.5-14B-Instruct outperforms Gemma 2 27B Instruct and GPT-4o mini on seven benchmarks.\\nBehind the news: Qwen 2.5 extends a parade of ever more capable LLMs that include Claude 3.5 Sonnet, GPT-4o, and LLama 3.1 as well as the earlier Qwen 2 family .\\nWhy it matters: The new models raise the bar for open weights models of similar sizes. They also rival some proprietary models, offering options to users who seek to balance performance and cost.\\nWe‚Äôre thinking: Some companies encourage developers to use their paid APIs by locking their LLMs behind non-commercial licenses or blocking commercial applications beyond a certain threshold of revenue. We applaud Qwen‚Äôs approach, which keeps most models in the family open.',\n",
       "  '## Hollywood Embraces Video Generation\\nThe AI startup Runway is helping to retool Lionsgate, the producer of blockbuster movie franchises like The Hunger Games and John Wick , for the era of generated video.\\nWhat‚Äôs new: Runway will build a custom video generator to help Lionsgate streamline its production processes. It also launched an API for its Gen-3 Alpha Turbo model.\\nRunway + Lionsgate: Runway will fine-tune its proprietary models on Lionsgate productions to enable the filmmaker to generate new imagery based on its previous work. The companies didn‚Äôt disclose financial terms of the arrangement.\\n- Lionsgate plans to use the custom model for pre-production tasks like visualization and storyboarding, and for post-production processes like editing and special effects.\\n- The custom model could save Lionsgate ‚Äúmillions and millions of dollars,‚Äù a Lionsgate executive told The Wall Street Journal .\\n- Other studios, too, are looking into building video generation models that are fine-tuned on their own productions, Variety reported . Runway is in talks with some of them, the startup‚Äôs CEO Crist√≥bal Valenzuela told Axios.\\nGen-3 API: Concurrently with announcing the Lionsgate deal, Runway unveiled an API that drives its Gen-3 Alpha and Gen-3 Alpha Turbo models as well as updates to Gen-3 Alpha.\\n- The company charges around $0.60 to $1.20, depending on the service tier, to generate outputs up to 5 seconds long and twice that for up to 10 seconds long.\\n- Third-party user interfaces that connect to the API must include a ‚ÄúPowered by Runway‚Äù banner that links to Runway‚Äôs website.\\n- Gen-3 Alpha now allows users to transform existing videos into new styles using text prompts and steer its output using video input in addition to a prompt. The model‚Äôs output will follow the input video‚Äôs shapes and motions.',\n",
       "  'Why it matters: Although the plan is to use Runway‚Äôs technology for pre- and post-production, this deal puts state-of-the-art video generation at the heart of Lionsgate‚Äôs operations and encourages professional cinematographers, editors, special effects artists, and other cinematic specialists to see what they can do with it. For Lionsgate, it‚Äôs a bid to stay ahead of competitors. For AI, it could be a major move into the Hollywood spotlight.\\nWe‚Äôre thinking: While upstart competitors are using pretrained models, Lionsgate will be using a model that has internalized its own style and capabilities.',\n",
       "  '## Robot Server\\nA robot that plays table tennis beats human beginners and entertains experts.\\nWhat‚Äôs new: David B. D‚ÄôAmbrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Pannag R. Sanketi and colleagues at Google showed off a robot arm that challenges human players at table tennis. You can see it in action here .\\nKey insight: A table tennis match can be broken into individual volleys that start when an opponent hits the ball and end when the robot returns the ball to the opponent‚Äôs side of the table or the ball goes out of play. This simple scheme enables a robotic control system to learn how to return a ball without attending to strategy.\\nThe robot: The authors mounted a robotic arm atop two linear gantries that enabled the arm to move to the left and right, and forward and backward. Two cameras captured images of the ball and fed them to a perception system that estimated ball positions. A 20-camera motion-capture system tracked the position of the opponent‚Äôs paddle.\\nHow it works: Instead of training an end-to-end system or using a robotics foundation model, the authors broke down the gameplay into subtasks, delegated them to separate modules, and orchestrated them to work together. The robot was controlled by a high-level controller: a custom algorithm including a convolutional neural network (CNN) that classified whether to return the ball using a forehand or backhand stroke and a vanilla neural network that classified spin. The high-level controller selected among 17 low-level controllers (all CNNs). Each low-level controller executed a different skill, enabling the system to return serves or rallies, adjust for ball spin, target different spots on the table, and so on.',\n",
       "  '- The authors collected a dataset of ball positions from human-to-human play. Using the perception system, they derived the ball‚Äôs initial positions, velocities, and angular velocities. After training the system the first time, they collected similar data for human-robot play and trained their system further using those examples.\\n- Training took place in a simulation (except the high-level controller‚Äôs vanilla neural network, which learned to classify spin via supervision).The high-level controller‚Äôs CNN learned to choose forehand or backhand to maximize the rate at which the robot successfully returned the ball. The low-level controllers learned using blackbox gradient sensing , an evolutionary algorithm, based on several rewards, such as rewarding the controller if it successfully returned the ball and punishing it if the robot collided with itself or the table.\\n- Each time the opponent hit the ball, the high-level controller decided which low-level controller to use. The decision was based on factors such as whether the ball had topspin or underspin and estimated statistics such as return rate, opponent‚Äôs paddle velocity, and estimated position where the ball would land on the opponent‚Äôs side.\\n- Given the last 0.14 seconds of the ball‚Äôs position and velocity, as well as the robot‚Äôs joint positions and its position on the gantries, the selected low-level controller determined how fast to move the robot to return the ball.\\nResults: The robot played 29 three-game matches against 29 players of varying skill (beginner, intermediate, advanced, and advanced+ as rated by a professional coach).\\n- It won all 7 (100 percent) of its matches against beginners, 6 (55 percent) of its matches against intermediate players, and zero matches against advanced or advanced+ players.\\n- On a point-by-point basis, it won 72 percent of points against beginners, 50 percent against intermediate players, and 34 percent of points against advanced and advanced+ players.',\n",
       "  '- On a point-by-point basis, it won 72 percent of points against beginners, 50 percent against intermediate players, and 34 percent of points against advanced and advanced+ players.\\n- When asked if they would like to play against the robot again on a scale of 1 (definitely not) to 5 (definitely yes), the average response was 4.87.\\nWhy it matters: Roboticists have been programming robot arms to play table tennis for at least a decade . Earlier projects enabled robots to perform various aspects of the game, like aiming at a specific target or smashing, but none tackled complete gameplay against competitive human opponents. Breaking the problem into two parts ‚Äî a library of individual skills (low-level controllers) and an algorithm that chooses which to use ‚Äî simplifies the task. Weaknesses in the robot‚Äôs performance (for example, difficulty returning underspin) can be addressed by adding a skill that compensates.\\nWe‚Äôre thinking: Even expert players had enough fun playing against this robot to want to play more. That‚Äôs a successful gaming system!',\n",
       "  '- For the typical training process, the authors trained TinyLLaMa-1.1B for one epoch on a subset of RedPajama , a de-duplicated dataset of text scraped from the web. To provide duplicate text, they added 2,000 sequences from Wikipedia, each repeated 50 times.\\n- To promote memorization, they fine-tuned a pretrained Llama 2 7B for 100 epochs on 100 Wikipedia articles.\\nResults: The authors assessed the results using two metrics: (i) ROUGE-L , which falls between 0 and 100 percent and reflects the longest subsequence in common between ground-truth and generated data, and (ii) the percentage of tokens that exactly matched the original text in proper order. Both measure memorization, so lower scores are better.\\n- In the typical setting, the model trained using the next-token-prediction loss memorized heavily, while the model trained with the goldfish loss memorized just a little bit.\\n- In the setting that promoted memorization, the model trained using the next-token-prediction loss exactly matched 85 percent of the tokens in the Wikipedia articles and achieved 96 percent ROUGE-L. The model using the goldfish loss exactly matched 0 percent of the Wikipedia tokens and achieved 51 percent ROUGE-L.\\n- Both models achieved similar performance on six common-sense reasoning and question answering tasks, indicating that the goldfish loss didn‚Äôt hinder the accuracy on those tasks.\\nWhy it matters: Businesses are worried about whether using LLMs poses risks to intellectual property rights and privacy. Techniques that address this concern without significantly impacting performance are welcome.\\nWe‚Äôre thinking: Memorization also happens in models generating images. We look forward to research into using similar techniques in that domain.',\n",
       "  \"## Introduction\\nDear friends,\\nThere‚Äôs a lingering misconception that building with generative AI is expensive. It is indeed expensive to train cutting-edge foundation models, and a number of companies have spent billions of dollars doing this (and even released some of their models as open weights). But as a result, it‚Äôs now very inexpensive to build a wide range of AI applications.\\nThe AI stack has several layers, shown in the diagram below. Here are the lower layers, from the bottom up:\\n- Semiconductors. Nvidia has been a huge benefactor in this space. AMD‚Äôs MI300 and forthcoming MI350 are also strong alternatives to the Nvidia H100 and the delayed Blackwell chips.\\n- Cloud. AWS (disclosure: I serve on Amazon‚Äôs board of directors), Google Cloud, and Microsoft Azure make it easy for developers to build.\\n- Foundation models. This includes both proprietary models such as OpenAI‚Äôs and Anthropic‚Äôs, and open weights models such as Meta‚Äôs Llama.\\nThe foundation model layer frequently appears in headlines because foundation models cost so much to build. Some companies have made massive investments in training these models, and a few of those have added to the hype by pointing out that paying lots for compute and data would lead (probably) to predictably better performance following scaling laws .\\nThis layer is also currently hyper-competitive, and switching costs for application developers to move from one model to another are fairly low (for example, requiring changes to just a few lines of code). Sequoia Capital‚Äôs thoughtful article on ‚Äú AI's $600B Question ‚Äù points out that, to justify massive capital investments in AI infrastructure (particularly GPU purchases and data center buildouts), generative AI needs to get around $600B of revenue. This has made investing at the foundation model layer challenging. It‚Äôs expensive, and this sector still needs to figure out how to deliver returns. (I‚Äôm cautiously optimistic it will work out!)\",\n",
       "  'On top of this layer is an emerging orchestration layer, which provides software that helps coordinate multiple calls to LLMs and perhaps to other APIs. This layer is becoming increasingly agentic. For example, Langchain has helped many developers build LLM applications, and its evolution into LangGraph for building agents has been a great development. Other platforms such as Autogen , MemGPT , and CrewAI (disclosure: I made a personal investment in CrewAI) are also making it easier to build agentic workflows. Switching costs for this layer are much higher than for the foundation model layer, since, if you‚Äôve built an agent on one of these frameworks, it‚Äôs a lot of work to switch to a different one. Still, competition in the orchestration layer, as in the foundation model layer, seems intense.\\nFinally, there‚Äôs the application layer. Almost by definition, this layer has to do better financially than all the layers below. In fact, for investments at the lower layers to make financial sense, the applications had better generate even more revenue, so the application vendors can afford to pay providers of infrastructure, cloud computing, foundation models, and orchestration. (This is why my team AI Fund focuses primarily on AI application companies, as I discussed in a talk .)\\nFortunately, because of the massive investments in foundation models, it‚Äôs now incredibly inexpensive to experiment and build prototypes in the applications layer! Over Thanksgiving holiday, I spent about one and a half days prototyping different generative AI applications, and my bill for OpenAI API calls came out to about $3. On my personal AWS account, which I use for prototyping and experimentation, my most recent monthly bill was $35.30. I find it amazing how much fun you can have on these platforms for a small number of dollars!',\n",
       "  'By building on widely available AI tools, AI Fund now budgets $55,000 to get to a working prototype. And while that is quite a lot of money, it‚Äôs far less than the billions companies are raising to develop foundation models. Individuals and businesses can experiment and test important ideas at reasonable cost.\\nKeep learning!\\nAndrew\\nStarting your career in AI has never been easier with Machine Learning Specialization , a foundational program for beginners in machine learning. Get started!',\n",
       "  '## Agents Open the Wallet\\nOne of the world‚Äôs biggest payment processors is enabling large language models to spend real money.\\nWhat‚Äôs new: Stripe announced Stripe Agent Toolkit, a library for Python and Typescript that supports agentic workflows that use API calls to execute monetary transactions. You can download it here .\\nHow it works: An agentic purchasing workflow may look like this: A user asks the agent to find a flight to a certain destination, on a certain schedule, with a certain price limit; and an LLM queries a flight database, chooses a flight, obtains authorization from the user, and purchases the flight. Stripe Agent Toolkit supports agentic workflow frameworks from CrewAI , LangChain , and Vercel . It doesn‚Äôt yet implement all of Stripe‚Äôs API, but Stripe expects to extend it in the future.\\n- The library can issue virtual debit cards for one-time use, so applications based on LLMs can spend money only when you want them to.\\n- It also authorizes transactions in real time, so you can present intended purchases to an end user for approval before an agent executes them.\\n- It can track the LLM‚Äôs use of tokens per customer, so you can bill clients for costs they incur while using agents you‚Äôve built.\\n- Stripe provides restricted API keys, so you can limit the range of API calls an LLM is allowed to request.\\nWhy it matters: Agents that can spend money securely open a wide variety of applications. Stripe‚Äôs API previously made it possible to enable an LLM-based application to make purchases online, but doing so required trusting the LLM to generate the right API calls and not to make inappropriate ones. The new library makes it easier to enforce spending limits and API constraints, and thus to build agents that engage in ecommerce safely.\\nWe‚Äôre thinking: Stripe‚Äôs offering helps developers build agents that are cents-ible!',\n",
       "  '## Mistral‚Äôs Vision-Language Contender\\nMistral AI unveiled Pixtral Large, which rivals top models at processing combinations of text and images.\\nWhat‚Äôs new: Pixtral Large outperforms a number of leading vision-language models on some tasks. The weights are free for academic and non-commercial use and can be licensed for business use. Access is available via Mistral AI‚Äôs website or API for $2/$6 per million tokens for input/output. In addition, Pixtal Large now underpins le Chat, Mistral AI‚Äôs chatbot, which also gained several new features.\\nHow it works: Pixtral Large generates text in response to text and images in dozens of languages. It processes 131,072 tokens of context, which is sufficient to track relationships among 30 high-resolution images at a time. Based on Mistral Large 2 (a 123 billion-parameter large language model) and a 1 billion-parameter vision encoder, it demonstrates strong performance across several benchmarks (as reported by Mistral).\\n- Mistral compared Pixtral Large to the open weights Llama 3.2 90B and the closed models Gemini-1.5 Pro, GPT-4o, and Claude-3.5 Sonnet. In Mistral‚Äôs tests (as opposed to the other model providers‚Äô reported results, which differ in some cases), Pixtral Large achieved the best performance on four of eight benchmarks that involved analyzing text and accompanying visual elements.\\n- For instance, on MathVista (math problems that involve visual elements, using chain-of-thought prompting), it achieved 69.4 percent accuracy, while Gemini 1.5 Pro, the next-best model in Mistral AI‚Äôs report, achieved 67.8 percent accuracy. (Claude 3.5 Sonnet outperforms Pixtral-Large on this benchmark according to Anthropic‚Äôs results. So do OpenAI o1 and Claude-3.5 Sonnet, according to their developers‚Äô results, which Mistral did not include in its presentation.)',\n",
       "  '- Pixtral Large powers new features of le Chat including PDF analysis for complex documents and a real-time interface for creating documents, presentations, and code, similar to Anthropic‚Äôs Artifacts and OpenAI‚Äôs Canvas. Le Chat also gained beta-test features including image generation (via Black Forest Labs‚Äô Flux.1 ), web search with source citations (using Mistral‚Äôs proprietary search engine), and customizable agents that can perform tasks like scanning receipts, summarizing meetings, and processing invoices. These new features are available for free.\\nBehind the news: Pixtral Large arrives as competition intensifies among vision-language models. Meta recently entered the field with Llama 3.2 vision models in 11B and 90B variants. Both Pixtral Large and Llama 3.2 90B offer open weights, making them smaller and more widely available than Anthropic‚Äôs, Google‚Äôs, or OpenAI‚Äôs leading vision-language models. However, like those models, Pixtral Large falls short of the reported benchmark scores of the smaller, more permissively licensed Qwen2-VL 72B .\\nWhy it matters: Pixtral Large and updates to le Chat signal that vision-language capabilities ‚Äî combining text generation, image recognition, and visual reasoning ‚Äî are essential to compete with the AI leaders. In addition, context windows of 129,000 tokens and above have become more widely available, making it possible to analyze lengthy (or multiple) documents that include text, images, and graphs as well as video clips.\\nWe‚Äôre thinking: Mistral is helping to internationalize development of foundation models. We‚Äôre glad to see major developers emerging in Europe!',\n",
       "  '## Garbage Out\\nRapid progress in generative AI comes with a hidden environmental cost: mountains of obsolete hardware.\\nWhat‚Äôs new: A study projects that servers used to process generative AI could produce millions of metric tons of electronic waste by 2030. Extending server lifespans could reduce the burden substantially, according to author Peng Weng and colleagues at the Chinese Academy of Sciences and Reichman University.\\nHow it works: The study extrapolated from publicly available data to model accumulation of electronic waste, or e-waste, between 2023 and 2030. The authors examined four scenarios: One scenario assumed linear growth in which hardware manufacturing expands at the current rate of 41 percent annually. The other three assumed exponential growth of demand for computing: conservative (85 percent annually), moderate (115 percent annually), and aggressive (136 percent annually). The study evaluated each scenario with and without measures taken to reduce waste.\\n- In the linear-growth scenario, e-waste could add up to 1.2 million metric tons between 2023 and 2030. In the aggressive scenario, the total could reach 5 million metric tons, or roughly 1 percent of total electronic waste during that period. (These figures don‚Äôt account for mitigations, which would improve the numbers, or ongoing manufacturing of earlier, less efficient technology, which would exacerbate them.)\\n- The study assumed that servers typically would be discarded after three years. Upgrading servers more frequently, when improved hardware becomes available, would reduce overall server numbers because fewer servers would deliver greater processing power. However, because servers would be discarded more quickly, it could add a cumulative 1.2 million metric tons in the linear scenario or 2.3 million metric tons in the aggressive scenario, assuming no mitigation measures are taken.',\n",
       "  '- U.S. trade restrictions on advanced chips are also likely to exacerbate the problem. They could push affected countries to rely on less-efficient hardware designs and thus require more new servers to reach a competitive processing capacity. This could increase total waste by up to 14 percent.\\n- The authors explored several approaches to reducing e-waste. Repurposing equipment for non-AI applications and reusing critical components like GPUs and CPUs could cut e-waste by 42 percent. Improving the power efficiency of chips and optimizing AI models could reduce e-waste by 16 percent.\\n- The most promising approach to reducing e-waste is to extend server lifespans. Adding one year to a server‚Äôs operational life could reduce e-waste by 62 percent.\\nWhy it matters: E-waste is a problem not only due to its sheer quantity. Server hardware contains materials that are both hazardous and valuable. Discarded servers contain toxic substances like lead and chromium that can find their way into food water supplies. They also contain valuable metals, such as gold, silver, and platinum, that could save the environmental and financial costs of producing more of them. Proper recycling of these components could yield $14 billion to $28 billion, highlighting both the economic potential and the urgent need to develop and deploy advanced recycling technologies.\\nWe‚Äôre thinking: Humanity dumps over 2 billion metric tons of waste annually, so even comprehensive recycling and repurposing of AI hardware and other electronic devices would make only a small dent in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy.',\n",
       "  '## Breaking Jailbreaks\\nJailbreak prompts can prod a large language model (LLM) to overstep built-in boundaries, leading it to do things like respond to queries it was trained to refuse to answer. Researchers devised a way to further boost the probability that LLMs will respond in ways that respect such limits.\\nWhat‚Äôs new: Jingtong Su, Julia Kempe, and Karen Ullrich at New York University and MetaAI improved model behavior via E-DPO . Their method modifies Direct Preference Optimization (DPO), a popular way to align models with human preferences.\\nKey insight: DPO fine-tunes a model to encourage a developer‚Äôs notion of good behavior and suppress bad behavior, but it must also ensure that the model doesn‚Äôt forget knowledge it learned during pretraining. To this end, DPO‚Äôs loss function includes a regularization constraint that encourages the model to produce token probabilities similar to those it produced prior to fine-tuning. However, this causes the model to retain not only desired knowledge but also undesired knowledge that may lead it to produce an unwanted response. We can reduce the probability that it will draw on such undesired knowledge by changing the regularization constraint. The idea is to ensure similar token probabilities between (a) a model prior to fine-tuning, asked to behave harmlessly prior to receiving the harmful prompt and (b) the fine-tuned model, given a harmful prompt. This adjustment helps the fine-tuned model deliver outputs based on benign knowledge, along with the usual benefits of DPO.\\nHow it works: The authors used E-DPO to further fine-tune Mistral-7b-sft-constitutional-ai (which is aligned using the technique known as constitutional AI ) on two datasets in which each example consists of a prompt, a preferred response, and an objectionable response.\\n- The authors prompted GPT-3.5 Turbo to classify harmful prompts in the datasets.',\n",
       "  '- The authors prompted GPT-3.5 Turbo to classify harmful prompts in the datasets.\\n- They fine-tuned the model according to DPO but, when the input was classified as harmful, they computed the regularization constraint differently. The updated regularization constraint encouraged the fine-tuned model‚Äôs token probabilities to be similar to those assigned by the original model after prompting it to ‚Äúadhere to community guidelines and ethical standards.‚Äù\\nResults: E-DPO reduced Mistral-7b-SFT-constitutional-ai‚Äôs average attack success rate (ASR, the percentage of times a jailbreak prompt successfully elicited an objectionable responses) across 11 jailbreak datasets and methods (two sets of human-proposed jailbreak prompts and a variety of automatic jailbreak prompt-finding methods) from the HarmBench benchmark. The fine-tuned model achieved 36.95 ASR, while prior to fine-tuning it achieved 44.47 ASR. Typical DPO reduced the average ASR to 42.00.\\nWhy it matters: We can‚Äôt train a model to respond in a desirable way to all jailbreaks, no matter how big the training dataset. The space of potential jailbreaks is practically unlimited. Instead, it‚Äôs necessary to alter training methods, as this work does.\\nWe‚Äôre thinking: Humans, like learning algorithms, can circumvent social norms when they encounter a harmful request (attack your neighbors) cloaked in a manipulative scenario (to uphold religious or nationalistic values). While we work on aligning models with human preferences, let‚Äôs make sure we ourselves are aligned, too.',\n",
       "  '## Object Detection for Small Devices\\nAn open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells.\\nWhat‚Äôs new: Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced Grounding DINO 1.5 , a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights here .\\nKey insight: The original Grounding DINO follows many of its predecessors by using image embeddings of different levels (from lower-level embeddings produced by an image encoder‚Äôs earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to better detect objects at different scales . However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process.\\nHow it works: Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples.\\n- Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.\\n- Given the corresponding text, BERT produced a text embedding composed of tokens.',\n",
       "  '- Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.\\n- Given the corresponding text, BERT produced a text embedding composed of tokens.\\n- Given the highest-level image embedding and the text embedding, a cross-attention model updated each one to incorporate information from the other (fusing text and image modalities, in effect). After the update, a CNN-based model combined the updated highest-level image embedding with the lower-level image embeddings to create a single image embedding.\\n- Grounding DINO 1.5 calculated which 900 tokens in the image embedding were most similar to the tokens in the text embedding.\\n- A cross-attention model detected objects using both the image and text embeddings. For each token in the updated image embedding, it determined: (i) which text token(s), if any, matched the image token, thereby giving each image token a classification including ‚Äúnot an object‚Äù and (ii) a bounding box that enclosed the corresponding object (except for tokens that were labeled ‚Äúnot an object‚Äù).\\n- The system learned to (i) maximize the similarity between matching tokens from the text and image embeddings and minimize the similarity between tokens that didn‚Äôt match and (ii) minimize the difference between its own bounding boxes and those in the training dataset.\\nResults: Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an Nvidia Jetson Orin NX computer. Tested on a dataset of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and YOLO-Worldv2-L (a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent.',\n",
       "  'Why it matters: The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results.\\nWe‚Äôre thinking: Lately model builders have been building better, smaller, faster large language models for edge devices. We‚Äôre glad to see object detection get similar treatment.',\n",
       "  \"## Introduction\\nDear friends,\\nI‚Äôm thrilled that former students and postdocs of mine won both of this year‚Äôs NeurIPS Test of Time Paper Awards. This award recognizes papers published 10 years ago that have significantly shaped the research field. The recipients included Ian Goodfellow (who, as an undergraduate, built my first GPU server for deep learning in his dorm room) and his collaborators for their work on generative adversarial networks, and my former postdoc Ilya Sutskever and PhD student Quoc Le (with Oriol Vinyals) for their work on sequence-to-sequence learning. Congratulations to all these winners!\\nBy nature, I tend to focus on the future rather than the past. Steve Jobs famously declined to build a corporate museum, instead donating Apple's archives to Stanford University, because he wanted to keep the company forward-looking. Jeff Bezos encourages teams to approach every day as if it were ‚ÄúDay 1,‚Äù a mindset that emphasizes staying in the early, innovative stage of a company or industry. These philosophies resonate with me.\\nBut taking a brief look at the past can help us reflect on lessons for the future. One takeaway from looking at what worked 10 to 15 years ago is that many of the teams I led bet heavily on scaling to drive AI progress ‚Äî a bet that laid a foundation to build larger and larger AI systems. At the time, the idea of scaling up neural networks was controversial, and I was on the fringe. I recall distinctly that, around \\xa02008, Yoshua Bengio advised me not to bet on scaling and to focus on inventing algorithms instead!\",\n",
       "  'A lesson I carry from that time is to not worry about what others think, but follow your convictions, especially if you have data to support your beliefs. Small-scale experiments performed by my Stanford group convinced me that scaling up neural networks would drive significant progress, and that‚Äôs why I was willing to ignore the skeptics. The diagram below, generated by Adam Coates and Honglak Lee, is the one that most firmed up my beliefs at that time. It shows that, for a range of models, the larger we scaled them, the better they perform. I remember presenting it at CIFAR 2010 , and if I had to pick a single reason why I pushed through to start Google Brain and set as the team‚Äôs #1 goal to scale up deep learning algorithms, it is this diagram!\\nI also remember presenting at NeurIPS in 2008 our work on using GPUs to scale up training neural networks. (By the way, one measure of success in academia is when your work becomes sufficiently widely accepted that no one cites it anymore. I‚Äôm quite pleased the idea that GPUs should be used for AI ‚Äî which was controversial back then ‚Äî is now such a widely accepted ‚Äúfact‚Äù that no one bothers to cite early papers that pushed for it.üòÉ)\\nWhen I started Google Brain, the thesis was simple: I wanted to use the company‚Äôs \\xa0huge computing capability to scale up deep learning. Shortly afterward, I built Stanford‚Äôs first supercomputer for deep learning using GPUs, since I could move faster at Stanford than within a large company. A few years later, my team at Baidu showed that as you scale up a model, its performance improves linearly on a log-log scale, which was a precursor to OpenAI‚Äôs scaling laws.\\nAs I look to the future, I‚Äôm sure there are ideas that many people are skeptical about today, but will prove to be accurate. Scaling up AI models turned out to be useful for many teams, and it continues to be exciting, but now I‚Äôm even more excited by upcoming ideas that will prove to be even more valuable in the future.',\n",
       "  'This past year, I spent a lot of time encouraging teams to build applications with agentic AI and worked to share best practices. I have a few hypotheses for additional technologies that will be important next year. I plan to spend the winter holiday playing with a few of them, and I will have more to share next year. But if you have an idea that you have conviction on, so long as you can do so responsibly, I encourage you to pursue it!\\nKeep learning,\\nAndrew\\nIn our latest short course, you‚Äôll learn how to use OpenAI o1 for advanced reasoning in tasks like coding, planning, and image analysis. Explore tradeoffs between intelligence gains and cost as well as techniques, such as meta prompting, to optimize performance. Enroll now!',\n",
       "  '## Phi-4 Beats Models Five Times Its Size\\nMicrosoft updated its smallest model family with a single, surprisingly high-performance model.\\nWhat‚Äôs new: Marah Abdin and a team at Microsoft released Phi-4 , a large language model of 14 billion parameters that outperforms Llama 3.3 70B and Qwen 2.5 (72 billion parameters) on math and reasoning benchmarks. The model is available at Azure AI Foundry under a license that permits non-commercial uses, and the weights will be released via Hugging Face next week.\\nHow it works: Phi-4 is a transformer that processes up to 16,000 tokens of input context. The ways the authors constructed the pretraining and fine-tuning datasets accounts for most of its performance advantage over other models.\\n- Much of the pretraining set was high-quality data from the web or existing datasets. The authors used known high-quality datasets and repositories of high-quality web data (like books and research papers). They also filtered websites using classifiers they trained to recognize high-quality text.\\n- The rest of the pretraining data was generated or rewritten by GPT-4o. Given snippets of text from web pages, code, scientific papers, and books, GPT-4o rewrote them as exercises, discussions, question-and-answer pairs, and structured reasoning tasks. GPT-4o then followed a feedback loop to improve its accuracy by critiquing its own outputs and generating new ones.\\n- The authors fine-tuned Phi-4 on existing and newly generated data they acquired in similar ways.',\n",
       "  '- The authors fine-tuned Phi-4 on existing and newly generated data they acquired in similar ways.\\n- They further fine-tuned it on two rounds of generated data using Direct Preference Optimization (DPO) , which trains models to be more likely to generate a preferred example and less likely to generate a not-preferred example. In the first round, the authors generated preferred/not-preferred pairs by identifying important tokens in generated responses: They considered a token to be important if, after the model generated it (as part of a partial response), the probability that it ultimately would produce a correct output significantly improved (or declined). They measured this probability by generating multiple completions of a given prompt and determining the percentage of times the model produced the correct answer after generating a given token. The preferred/not-preferred pairs (in which one element of the pair is composed of an input, token(s) to generate, and preferred or not-preferred label) took tokens generated prior to the important token as the input, the important token as the preferred token, and the important token that decreased the probability as the not-preferred token.\\n- In the second round of generating preferred/not-preferred pairs and fine-tuning via DPO, the authors generated responses from GPT-4o, GPT-4 Turbo, and Phi-4, and then used GPT-4o to rate them. Highly rated responses were preferred, and lower-rated responses were not preferred.\\nResults: Of 13 benchmarks, Phi-4 outperforms Llama 3.3 70B (its most recent open weights competitor) on six and Qwen 2.5 on five.\\n- Phi-4 outperforms Llama 3.3 70B, Qwen 2.5, and GPT-4o on GPQA (graduate level questions and answers) and MATH (competition-level math problems).\\n- However, Llama 3.3 70B wins DROP (reading comprehension) and SimpleQA (answering questions about basic facts). Llama 3.3 70B also performs significantly better on IFEval (instruction-following).',\n",
       "  '- However, Llama 3.3 70B wins DROP (reading comprehension) and SimpleQA (answering questions about basic facts). Llama 3.3 70B also performs significantly better on IFEval (instruction-following).\\nWhy it matters: Phi-4 shows that there‚Äôs still room to improve the performance of small models by curating training data, following the age-old adage that better data makes a better model.\\nWe‚Äôre thinking: Some researchers found that earlier versions of Phi showed signs of overfitting to certain benchmarks. In their paper, the Microsoft team stressed that they had improved the data decontamination process for Phi-4 and added an appendix on their method. We trust that independent tests will show that Phi-4 is as impressive as its benchmark scores suggest.',\n",
       "  '## Open Video Gen Closes the Gap\\nThe gap is narrowing between closed and open models for video generation.\\nWhat‚Äôs new: Tencent released HunyuanVideo , a video generator that delivers performance competitive with commercial models. The model is available as open code and open weights for developers who have less than a 100 million monthly users and live outside the EU, UK, and South Korea.\\nHow it works: HunyuanVideo comprises a convolutional video encoder-decoder, two text encoders, a time-step encoder, and a transformer. The team trained the model in stages (first the encoder-decoder, then the system as a whole) using undisclosed datasets before fine-tuning the system.\\n- The team trained the encoder-decoder to reconstruct images and videos.\\n- They trained the system to remove noise from noisy embeddings of videos. They started with low-resolution images; then higher-resolution images; then low-resolution, shorter videos; and\\xa0 progressively increased to higher-resolution, longer videos.\\n- Given a video, the encoder embedded it. Given a text description of the video, a pretrained Hunyuan-Large produced a detailed embedding of the text and a pretrained CLIP produced a general embedding. A vanilla neural network embedded the current timestep. Given the video embedding with added noise, the two text embeddings, and the time-step embedding, the transformer learned to generate a noise-free embedding.\\n- The team fine-tuned the system to remove noise from roughly 1 million video examples that had been curated and annotated by humans to select those with the most aesthetically pleasing and compelling motions.\\n- At inference, given pure noise, a text description, and the current time step, the text encoders embed the text and the vanilla neural network embeds the time step. Given the noise, text embeddings, and the time-step embedding, the transformer generates a noise-free embedding, and the decoder turns it back into video.',\n",
       "  'Results: 60 people judged responses to 1,533 text prompts by HunyuanVideo, Gen-3 and Luma 1.6 . The judges preferred HunyuanVideo‚Äôs output overall. Examining the systems‚Äô output in more detail, they preferred HunyuanVideo‚Äôs quality of motion but Gen-3‚Äôs visual quality.\\nBehind the news: In February, OpenAI‚Äôs announcement of Sora (which was released as this article was in production) marked a new wave of video generators that quickly came to include Google Veo , Meta Movie Gen , Runway Gen-3 Alpha , and Stability AI Stable Video Diffusion . Open source alternatives like Mochi continue to fall short of publicly available commercial video generators.\\nWhy it matters: Research in image generation has advanced at a rapid pace, while progress in video generation has been slower. One reason may be the cost of processing, which is especially intensive when it comes to video. The growing availability of pretrained, open source video generators could accelerate the pace by relieving researchers of the need to pretrain models and enabling them to experiment with fine-tuning and other post-training for specific tasks and applications.\\nWe‚Äôre thinking: Tencent‚Äôs open source models are great contributions to research and development in video generation. It‚Äôs exciting to see labs in China contributing high-performance models to the open source community!',\n",
       "  '## Multimodal Modeling on the Double\\nGoogle‚Äôs Gemini 2.0 Flash, the first member of its updated Gemini family of large multimodal models, combines speed with performance that exceeds that of its earlier flagship model, Gemini 1.5 Pro, on several measures.\\nWhat‚Äôs new: Gemini 2.0 Flash processes an immense 2 million tokens of input context including text, images, video, and speech, and generates text, images, and speech. Text input/output is available in English, Spanish, Japanese, Chinese, and Hindi, while speech input/output is available in English only for now. It can use tools, generate function calls, and respond to a real-time API ‚Äî capabilities that underpin a set of pre-built agents that perform tasks like research and coding. Gemini 2.0 Flash is available for free in an experimental preview version via Google AI Studio, Google Developer API, and Gemini Chat.\\nHow it works: Gemini 2.0 Flash (parameter count undisclosed) matches or outperforms several competing models on key benchmarks, according to Google‚Äôs report.\\n- Gemini 2.0 Flash is faster than Gemini 1.5 Flash. It offers relatively low average latency (0.53 seconds to receive the first token, just ahead of Mistral Large 2 and GPT-4o mini) and relatively high output speed (169.5 tokens per second, just ahead of AWS Nova Lite and OpenAI o1 Preview but behind Llama), according to Artificial Analysis.\\n- It beats Gemini 1.5 Pro on multiple key benchmarks, including measures of language understanding ( MMLU-Pro ) and visual and multimedia understanding ( MMMU ). It also excels at competition-level math problems, achieving state-of-the-art results on MATH and HiddenMath . It outperforms Gemini 1.5 Pro when generating Python, Java, and SQL code ( Natural2Code ) and ( LiveCodeBench ).',\n",
       "  '- Compared to competing models, Gemini 2.0 Flash does well on language and multimedia understanding. On MMLU-Pro, Gemini 2.0 Flash outperforms GPT-4o and is just behind Claude 3.5 Sonnet, according to TIGER-Lab . Google reports a score of 70.7 percent on MMMU, which would put it ahead of GPT-4o and Claude 3.5 Sonnet, but behind o1‚Äôs, on the MMMU leaderboard as of this publication date. It does less well on tests of coding ability, in which it underperforms Claude 3.5 Sonnet, GPT-4o, o1-preview, and o1-mini.\\n- The Multimodal Live API feeds live-streamed inputs from cameras or screens to Gemini 2.0 Flash, enabling real-time applications like live translation and video recognition.\\n- The model‚Äôs multimodal input/output capabilities enable it to identify and locate objects in images and reason about them. For instance, it can locate a spilled drink and suggest ways to clean it up. It can alter images according to natural-language commands, such as turning a picture of a car into a convertible, and explain the changes step by step.\\nAgents at your service: Google also introduced four agents that take advantage of Gemini 2.0 Flash‚Äôs ability to use tools, call functions, and respond to the API in real time. Most are available via a waitlist.\\n- Astra , which was previewed in May, is an AI assistant for smartphones (and for prototype alternative-reality glasses that are in beta test with US and UK users). Astra recognizes video, text, images, and audio in real time and integrates with Google services to help manage calendars, send emails, and answer search queries.\\n- Mariner automatically compares product prices, buys tickets, and organizes schedules on a user‚Äôs behalf using a Chrome browser extension.\\n- Deep Research is a multimodal research assistant that analyzes datasets, summarized text, and compiles reports. It‚Äôs designed for academic and professional research and is available to Gemini Advanced subscribers.',\n",
       "  '- Jules is a coding agent for Python and JavaScript. Given text instructions, Jules creates plans, identifies bugs, writes and completes code, issues GitHub pull requests, and otherwise streamlines development. Jules is slated for general availability in early 2025.\\nBehind the news: OpenAI showed off GPT-4o‚Äôs capability for real-time video understanding in May, but Gemini 2.0 Flash beat it to the punch: Google launched the new model and its multimodal API one day ahead of ChatGPT‚Äôs Advanced Voice with Vision.\\nWhy it matters: Speed and multimodal input/output are valuable characteristics for any AI model, and they‚Äôre especially useful in agentic applications. Google CEO Sundar Pichai said he wants Gemini to be a ‚Äúuniversal assistant.‚Äù The new Gemini-based applications for coding, research, and video analysis are steps in that direction.\\nWe‚Äôre thinking: While other large language models can take advantage of search, Gemini 2.0 Flash generates calls to Google Search and uses that capability in agentic tools ‚Äî a demonstration of how Google‚Äôs dominance in search strengthens its efforts in AI.',\n",
       "  '## When LLMs Propose Research Ideas\\nHow do agents based on large language models compare to human experts when it comes to proposing machine learning research? Pretty well, according to one study.\\nWhat‚Äôs new: Chenglei Si, Diyi Yang, and Tatsunori Hashimoto at Stanford produced ideas for research in machine learning using Anthropic‚Äôs Claude 3.5 Sonnet and human researchers, and also evaluated them using both manual and automated methods. Claude 3.5 Sonnet generated competitive proposals, but its evaluations of proposals were less compelling.\\nHow it works: Each proposal included a problem statement, motivation, step-by-step plan, backup plan, and examples of baseline outcomes versus expected experimental outcomes.\\n- Automated proposal generation: Given one of seven topics (bias, coding, safety, multilinguality, factuality, math, or uncertainty) and 10 related papers found by the Semantic Scholar search engine, Claude 3.5 Sonnet generated 4,000 research ideas. The authors embedded the ideas using all-MiniLM-L6-v2 and removed duplicate ideas based on the cosine similarity of their embeddings. This left roughly 200 AI-generated ideas for each topic. For each remaining idea, the model generated a proposal.\\n- Automated ranking: Claude Sonnet 3.5 ranked the proposals in a five-round tournament that awarded points for superior quality and pitted highest-scoring proposals against one another. In addition, one of the authors manually ranked the generated proposals.\\n- Human proposal generation: The authors paid 49 machine learning engineers to propose their own ideas. They obscured authorship by prompting an unidentified large language model to edit them according to a style guide. Then they manually checked the rewritten proposals to ensure that the model‚Äôs editing didn‚Äôt change their content significantly.',\n",
       "  '- Human ranking: A group of 79 machine learning engineers reviewed the 49 human-written proposals, the top 49 AI-generated proposals ranked by humans, and the top 49 AI-generated proposals ranked by AI (resulting in two to four reviews per proposal). They scored the proposals between 1 and 10 on five factors: novelty, feasibility, expected effectiveness, how exciting they were, and overall quality.\\nResults: Human judges deemed proposals generated by Claude 3.5 Sonnet as good as or better than those produced by humans. However, large language models proved less effective at judging the proposals‚Äô quality.\\n- On average, humans scored the AI-generated and human-written proposals roughly equally in feasibility, expected effectiveness, how exciting they were, and overall quality. They deemed the AI-generated proposals significantly more novel. The top AI-generated proposals as ranked by humans achieved an average 5.78 novelty. The top AI-generated proposal as ranked by AI achieved an average 5.62 novelty. Human-written proposals achieved an average 4.86 novelty.\\n- The authors found that LLMs don‚Äôt yet match human performance when it comes to judging scientific papers. They compared the rates of agreement among five LLMs that evaluated proposals in their experiment, human judgements of the proposals, and human reviews of papers submitted to the NeurIPS and\\xa0 ICLR conferences. The most consistent LLM, Claude 3.5 Sonnet, was 53.3 percent consistent with average human judgment. The human judges were 56.1 percent consistent. Reviewers for NeurIPS and ICLR were 66 and 71.9 percent consistent respectively. Random chance was 50 percent.\\nWhy it matters: AI models play a growing role in scientific discovery . This work shows they can set directions for research ‚Äî in machine learning, at least ‚Äî\\xa0 that rival those set by humans. However, human evaluation remains the gold standard for comparing performance on complex problems like generating text.',\n",
       "  'We‚Äôre thinking: Coming up with good research ideas is hard! That a large language model can do it with some competency has exciting implications for the future of both AI and science.',\n",
       "  '- At inference, given a query, the model used cross-attention to select a subset of LoRA adapters and responded accordingly.\\nResults: The authors tested their LoRA-enhanced model‚Äôs ability to answer questions about a database via SQL queries. The model, which was outfitted for retrieval-augmented generation (RAG), achieved 94.7 percent accuracy. An unnamed model with RAG achieved 50 percent accuracy.\\nYes, but: It stands to reason that the authors‚Äô approach saves processing, but it‚Äôs unclear how much. The authors didn‚Äôt mention the cost of fine-tuning Llama-3-8B in the usual way on their training dataset for the same number of epochs.\\nWhy it matters: The authors argue that eliminating hallucinations is possible in typical training, it‚Äôs just computationally very expensive (not to mention the risk of overfitting). An architecture designed to store and retrieve facts, via LoRA adapters in this case, makes the process more feasible.\\nWe‚Äôre thinking: While some researchers want large language models to memorize facts, others want them to avoid memorizing their training data . These aims address very different problems. Preventing LLMs from memorizing training data would make them less likely to regurgitate it verbatim and thus violate copyrights. On the other hand, this work memorizes facts so the model can deliver consistent, truthful responses that might be stated in a variety of ways.',\n",
       "  '## Introduction\\nDear friends,\\nStartups live or die by their ability to execute at speed . For large companies, too, the speed with which an innovation team is able to iterate has a huge impact on its odds of success. Generative AI makes it possible to quickly prototype AI capabilities. AI capabilities that used to take months can sometimes be built in days or hours by simply prompting a large language model. I find this speed exciting and have been thinking about how to help startups and large companies alike go faster.\\nI‚Äôve been obsessed with speedy execution for a long time. When working on a project, I am loath to take two weeks to do something that I could do in one week. The price of moving at that pace is not that we take one week longer (which might be okay) but that we‚Äôre 2x slower (which is not)!\\nWhen building an AI-powered product, there are many steps in designing, building, shipping, and scaling the product that are distinct from building the AI capability, and our ability to execute these other steps has not sped up as much as the AI part. But the speed with which we can prototype AI creates significant pressure to speed up these other steps, too. If it took 6 months to collect data, train a supervised learning algorithm, and deploy the model to the cloud, it might be okay to take 2 months to get user feedback. But if it takes a week to build a prototype, waiting 2 months for feedback seems intolerably slow!',\n",
       "  'I‚Äôd like to focus on one key step of building applications: getting user feedback. A core part of the iterative workflow of designing and building a product (popularized by Eric Ries in his book The Lean Startup ) is to build a prototype (or MVP, minimum viable product), get user feedback on it, and to use that feedback to drive improvements. The faster you can move through this loop ‚Äî which may require many iterations ‚Äî the faster you can design a product that fits the market. This is why AI Fund, a venture studio that I lead, uses many fast, scrappy tactics to get feedback.\\nFor B2C (business to consumer) offerings, here is a menu of some options for getting customer feedback:\\nAs we go down this list, we get (probably) more accurate feedback, but the time needed to get that feedback increases significantly. Also, the tactics at the top of the list create basically no risk, and thus it‚Äôs safe to repeatedly call on them, even with preliminary ideas and prototypes. Another advantage of the tactics further up the list is that we get more qualitative feedback (for example, do users seem confused? Are they telling us they really need one additional feature?), which sparks better ideas for how to change our product than an A/B test, which tells us with rigor whether a particular implementation works but is less likely to point us in new directions to try. I recommend using the fast feedback tactics first. As we exhaust the options for learning quickly, we can try the slower tactics.\\nWith these tactics, scrappy startup leaders and innovation-team leaders in large companies can go faster and have a much higher chance of success.',\n",
       "  'With these tactics, scrappy startup leaders and innovation-team leaders in large companies can go faster and have a much higher chance of success.\\nThe mantra ‚Äúmove fast and break things‚Äù got a bad reputation because, well, it broke things. Unfortunately, some have interpreted this to mean we should not move fast, but I disagree. A better mantra is ‚Äúmove fast and be responsible.‚Äù There are many ways to prototype and test quickly without shipping a product that can cause significant harm. In fact, prototyping and testing/auditing quickly before launching to a large audience is a good way to identify and mitigate potential problems.\\nThere are numerous AI opportunities ahead, and our tools are getting better and better to pursue them at speed, which I find exhilarating!\\nKeep learning!\\nAndrew\\nBuild advanced, multi-agent systems for project planning, sales pipelines, customer support analysis, and content creation in our new course with crewAI! Gain hands-on skills in performance testing, multi-model setups, and using human feedback to optimize AI agents. Enroll for free',\n",
       "  '## AI Giants Go Nuclear\\nMajor AI companies plan to meet the growing demand with nuclear energy.\\nWhat‚Äôs new: Amazon, Google, and Microsoft announced substantial investments in nuclear power projects. Amazon and Google forged partnerships to build a new generation of small reactors, while Microsoft cut a deal to revive a shuttered nuclear plant. (Andrew Ng is a member of Amazon‚Äôs board of directors.)\\nHow it works: Nuclear power provides around 18 percent of electricity in the United States and more in France and several other European countries. Its steady generating capacity and zero carbon emissions (after plant construction) make it an attractive way to power AI infrastructure. However, new nuclear plants have been difficult to build in the U.S. since a string of high-profile accidents at Three Mile Island in the U.S. (1979), Chernobyl in Ukraine (1986), and Fukishima in Japan (2011). Since then, pressure to reduce carbon emissions has driven calls to build new plants. In March, President Biden signed legislation that streamlines construction and regulation of nuclear plants.\\n- Amazon is taking part in a number of nuclear projects. It led a $500 million investment in X-energy, a designer of small modular reactors, an emerging class of lower-cost reactor designs. X-energy‚Äôs reactors use advanced fuel that surrounds nuclear particles with carbon and ceramic to resist corrosion, rust, melting, or other dangers of high-temperature reactors. (The International Atomic Energy Agency regards small modular reactors as safer than earlier reactors. The Union of Concerned Scientists expresses doubts.) In addition, Amazon announced a partnership with the utility consortium Energy Northwest to deploy a 320-megawatt X-energy reactor in the state of Washington, which may expand to 960 megawatts. Separately, Amazon agreed with Dominion Energy to build a small modular reactor in Virginia, which would give Amazon‚Äôs data centers an additional 300 megawatts.',\n",
       "  '- Google partnered with Kairos Power to develop small modular reactors. Terms of the deal have not been disclosed. Kairos expects the new plants to begin operation in 2030, with more planned by 2035, providing up to 500 megawatts of electricity. This summer, Kairos broke ground on a demonstration unit in Tennessee, the first small modular reactor project permitted by the U.S. Nuclear Regulatory Commission, which is expected to open in 2027.\\n- In September, Microsoft signed a 20-year power purchase agreement with Constellation Energy, which intends to restart Unit 1 of Pennsylvania‚Äôs Three Mile Island nuclear plant (which was not damaged in the 1979 partial meltdown) by 2028.\\nBehind the news: The tech industry‚Äôs growing interest in nuclear power is driven by surging demand for AI and corporate commitments to reduce carbon emissions. Data centers that train and run AI models consume vast amounts of electricity, and nuclear energy offers a reliable, carbon-free source. Microsoft, Nvidia, and OpenAI have urged the White House to deliver a so-called ‚Äúenergy New Deal‚Äù that would allocate hundreds of billions of dollars to subsidize new power plants.\\nWhy it matters: The fact that tech giants are investing directly in nuclear power plants indicates the high stakes of competition in AI. Economists estimate that data centers that process AI, among other workloads, will consume more than 1,000 terawatt-hours of electricity by 2026, more than double the amount they consumed in 2022. Nuclear power could give them bountiful, carbon-free energy for decades to come.',\n",
       "  'We‚Äôre thinking: Fossil fuels like coal do tremendous damage to the environment, while renewables like solar and wind energy can‚Äôt fully meet the always-on demands of AI infrastructure. Next-generation reactor designs that improve safety and reduce costs are worth exploring. However, a significant obstacle remains: Few countries have a certifiably safe repository for long-term disposal of highly radioactive spent fuel. U.S. efforts toward this goal are stalled .',\n",
       "  '## AI Bromance Turns Turbulent\\nOnce hailed by OpenAI chief Sam Altman as the ‚Äúbest bromance in tech,‚Äù the partnership between Microsoft and OpenAI is facing challenges as both companies seek greater independence.\\nWhat‚Äôs new: Sources inside Microsoft and OpenAI revealed that both companies are working to reduce their reliance on the other, according to The New York Times . Their collaboration, which brought both companies great rewards, is now complicated by demands for resources, friction between leaders, and partnerships with other companies.\\nHow it works: In a series of deals that started in 2019, Microsoft invested a total of $13 billion in OpenAI, giving the startup access to Microsoft‚Äôs processing infrastructure and Microsoft special access to OpenAI‚Äôs models (which it integrated into its own applications), a large cut of its revenue, and potential equity. Microsoft built a 10,000-GPU system on Azure for training OpenAI models. But OpenAI sought to renegotiate its agreements, while Microsoft continued to develop its own AI capabilities.\\n- Last year, OpenAI CEO Sam Altman negotiated for further investment from Microsoft. But Microsoft reconsidered its commitment after OpenAI briefly ousted Altman in November. The tech giant‚Äôs hesitation strained relations as OpenAI continued to seek more funding and computing power.\\n- In April, Microsoft hired former Inflection AI CEO Mustafa Suleyman to head up its AI efforts. Suleyman‚Äôs aggressive leadership, including his frustration over what he perceived as OpenAI‚Äôs slow progress delivering new technologies, raised tensions between the parties.\\n- Microsoft engineers reportedly downloaded critical OpenAI software without following protocols the two companies had agreed upon, further straining the relationship.',\n",
       "  '- Microsoft engineers reportedly downloaded critical OpenAI software without following protocols the two companies had agreed upon, further straining the relationship.\\n- In June, Microsoft agreed to an exception in the partnership that allowed OpenAI to cut a $10 billion deal with Oracle for additional computing power. More recently, it cut the price it charged the startup for cloud computing.\\n- Under the original agreement, Microsoft would lose access to OpenAI‚Äôs technologies if the startup were to develop artificial general intelligence (AGI). This clause was intended to prevent commercial exploitation or abuse of emergent AI capabilities. However, it allows OpenAI‚Äôs board of directors to declare that the company has achieved AGI, which could enable OpenAI to exit the contract or give it leverage in renegotiations.\\nBehind the news: OpenAI‚Äôs valuation soared to $157 billion with new funding from Nvidia and other investors following a period of mounting financial pressure . The increased valuation gives OpenAI new power in its relationship with Microsoft. Moreover Microsoft holds no seats on its nonprofit board of directors, which limits its influence over strategic decisions at OpenAI despite its significant financial stake in the startup‚Äôs for-profit wing.\\nWhy it matters: The Microsoft-OpenAI partnership has reshaped the AI landscape, and shifts in their partnership have an outsized impact on a wide range of research and product development. Their evolving relationship illustrates the challenge of sustaining a close collaboration amid rapidly changing technology. Microsoft provided vital resources that helped OpenAI scale up, while OpenAI‚Äôs models enabled Microsoft to keep rivals off-balance as it reinvented products including Bing, Windows, Office, Azure, and its expanding line of Copilots. However, facing fierce competition, both companies need ample flexibility to innovate and adapt.',\n",
       "  'We‚Äôre thinking: Together and separately, Microsoft and OpenAI have done tremendous work to advance the field from research to applications. We hope they can strike a balance that maintains their partnership and fuels their growth.',\n",
       "  '## Mistral AI Sharpens the Edge\\nMistral AI launched two models that raise the bar for language models with 8 billion or fewer parameters, small enough to run on many edge devices.\\nWhat‚Äôs new: Ministral 3B and Ministral 8B , which come in base and instruction-tuned versions, outperform Google‚Äôs and Meta‚Äôs similar-sized models on several measures of knowledge retrieval, common-sense reasoning, and multilingual understanding. Ministral 8B-Instruct is free to download and use for noncommercial purposes, and commercial licenses are negotiable for this model and the others in the family. Accessed via Mistral‚Äôs APIs, Ministral 3B costs $0.04 per million tokens of input and output, and Ministral 8B costs $0.10 per million tokens of input and output.\\nHow it works: The Ministral family can process 131,072 tokens of input context. The models are built to support function calling natively to interact, for example, with external APIs that fetch real-time weather data or control smart-home devices.\\n- Ministral 3B is sized for smaller devices like smartphones. In Mistral‚Äôs tests, it surpassed Gemma 2 2B and Llama 3.2 3B on MMLU, AGIEval, and TriviaQA (question answering and common-sense reasoning), GSM8K (math), HumanEval (coding), and multilingual tasks in French, German, and Spanish. Independent tests by Artificial Analysis show Ministral 3B behind Llama 3.2 3B on MMLU and MATH.\\n- In Mistral‚Äôs tests, the instruction-tuned Ministral 3B-Instruct outperformed Gemma 2 2B and Llama 3.2 3B across several benchmarks including GSM8K, HumanEval, and three arena-style competitions judged by GPT-4o.',\n",
       "  '- Ministral 8B targets more powerful devices like laptops and requires 24GB of GPU memory to run on a single GPU. In Mistral‚Äôs tests, it outperformed its predecessor Mistral 7B and Meta‚Äôs Llama 3.1 8B on most benchmarks reported except HumanEval one-shot, where it was slightly behind Llama 3.1 8B. Independent tests by Artificial Analysis show Ministral 8B behind Llama 3.1 8B and Gemma 2 9B on MMLU and MATH.\\n- In Mistral‚Äôs tests, Ministral 8B-Instruct outperformed its peers on all benchmarks reported except WildBench , on which Gemma 2 9B Instruct achieved a higher score. WildBench tests responses to real-world requests that include digressions, vague language, idiosyncratic requirements, and the like.\\nBehind the news: Headquartered in France, Mistral AI competes head-to-head in AI with U.S. tech giants. It released its first model, Mistral 7B, a year ago under an Apache open source license. Since then, it has released model weights under a range of licenses while exploring alternative architectures such as mixture-of-experts and mamba. It also offers closed models that are larger and/or built for specialized tasks like code generation and image processing.\\nWhy it matters: Edge devices can play a crucial role in applications that require fast response, high privacy and security, and/or operation in the absence of internet connectivity. This is particularly important for autonomous and smart home devices where uninterrupted, rapid processing is critical. In addition, smaller models like Ministral 8B-Instruct enable developers and hobbyists to run advanced AI on consumer-grade hardware, lowering costs and broadening access to the technology.\\nWe‚Äôre thinking: Mistral‚Äôs new models underscore the growing relevance of edge computing to AI‚Äôs future. They could prove to be affordable and adaptable alternatives to Apple and Google‚Äôs built-in models on smartphones and laptops.',\n",
       "  '## Faster, Cheaper Video Generation\\nResearchers devised a way to cut the cost of training video generators. They used it to build a competitive open source text-to-video model and promised to release the training code.\\nWhat‚Äôs new: Yang Jin and colleagues at Peking University, Kuaishou Technology, and Beijing University of Posts and Telecommunications proposed Pyramidal Flow Matching , a method that reduced the amount of processing required to train video generators. They offer the code and a pretrained model that‚Äôs free for noncommercial uses and for commercial uses by developers who make less than $1 million in annual revenue.\\nKey insight: Models that generate output by starting with noise and removing it over several steps, such as diffusion and flow matching models, typically learn by removing noise from an embedding to which noise was added. Starting with a downsampled (smaller) version of the embedding and then upsampling (enlarging) it gradually throughout the process, hitting the full size near the end, saves processing during training and inference.\\nHow it works: The authors‚Äô system comprises a pretrained SD3 Medium image generator, an image autoencoder, and two pretrained text encoders: T5 and CLIP . They pretrained the autoencoder to reconstruct images and sequences of video frames, and trained SD3 Medium to remove noise from an embedding of eight video frames given both text embeddings and embeddings of previous sequences of frames. The training sets included WebVid-10M , OpenVid-1M , and Open-Sora Plan . The authors modified the typical process of removing noise from image embeddings in two ways: spatially and temporally.',\n",
       "  '- Spatially: Given an embedding of eight video frames, SD3 Medium starts by removing noise on a heavily downsampled (very small) version of the embedding. After a number of noise-removal steps, the system increases the embedding size and adds further noise. It repeats these steps until SD3 is finished removing noise from the full-size embedding.\\n- Temporally: When it‚Äôs removing noise from an embedding of eight frames, SD3 Medium receives downsampled versions of the previous embeddings it has generated. These embeddings start at the size of the current embedding and get progressively smaller for earlier frames. (They‚Äôre progressively smaller because the further they are from the current embedding, the less closely related they are to the current embedding.)\\n- At inference: Given a prompt, T5 and CLIP produce text embeddings. Given the text embeddings, an embedding of pure noise, and previously denoised embeddings, SD3 Medium removes noise. Given the denoised embeddings from SD3 Medium, the autoencoder‚Äôs decoder turns them into a video.\\nResults: The authors compared their model to other open and closed models using VBench, a suite of benchmarks for comparing the quality of generated video. They also conducted a survey of human preferences. On VBench, their model outperformed other open models but slightly underperformed the best proprietary models, such as Kling. Human evaluators rated their model as superior to Open-Sora 1.2 for esthetics, motion, and adherence to prompts, and better than Kling for esthetics and adherence to prompts (but not motion). Furthermore, running on an Nvidia A100 GPU, their model took 20,700 hours to learn to generate videos up to 241 frames long. Running on a faster Nvidia H100 GPU, Open-Sora 1.2 took 37,800 hours to learn to generate 97 frames.\\nWhy it matters: Video generation is a burgeoning field that consumes enormous amounts of processing. A simple way to reduce processing could help it scale to more users.',\n",
       "  '## Introduction\\nDear friends,\\nIs AI progressing rapidly? Yes! But while the progress of underlying AI technology has indeed sped up over the past 2 years, the fastest acceleration is in applications.\\nConsider this: GPT-4 was released March 2023. Since then, models have become much faster, cheaper, sometimes smaller, more multimodal, and better at reasoning, and many more open weight versions are available ‚Äî so progress has been fantastic! (Claims that AI is ‚Äúhitting a wall‚Äù seem extremely ill-informed.) But more significantly, many applications that\\xa0 already were theoretically possible using the March 2023 version of GPT-4 ‚Äî in areas such as customer service, question answering, and process automation ‚Äî now have significant early momentum.\\nI‚Äôm confident 2025 will see even faster and more exciting advances than 2024 in both AI technology and applications. Looking back, the one thing that could have stopped AI was bad, anti-competitive regulation that would have put onerous burdens on developers, particularly of open models. So long as we remain vigilant and hold off these anti-innovation forces, we‚Äôll keep up or even further accelerate progress.\\nI‚Äôm also seeing a widening gap between those at the cutting edge (which includes many readers of The Batch !) and those who have not yet tried out ChatGPT even once (yes, a lot of people are still in this group!). As technology changes around us, we all have to keep up to remain relevant and be able to make significant contributions. I‚Äôm committed to making sure DeepLearning.AI continues to help you learn the most useful and important AI technologies. If you‚Äôre making New Year‚Äôs resolutions, I hope you‚Äôll include us in your learning plan!\\nAI is the most important technological change happening in the world right now. I‚Äôm thrilled to be working in this exciting sector alongside you, and I‚Äôm grateful for your efforts to learn about and apply it to better the lives of yourself and others.\\nHappy holidays!\\nAndrew',\n",
       "  '## A Blizzard of Progress\\nWhat a year! AI made dramatic advances in 2024. Agentic systems improved their abilities to reason, use tools, and control desktop applications. Smaller models proliferated, many of them more capable and less expensive than their larger forbears. While some developments raised worries , far more sparked wonder and optimism. As in the waning days of earlier years , we invite you to pour a cup of hot cocoa and consider the high points of the last 12 months.',\n",
       "  '## Agents Ascendant\\nThe AI community laid the foundation for systems that can act by prompting large language models iteratively, leading to much higher performance across a range of applications.\\nWhat happened: AI gained a new buzzword ‚Äî agentic ‚Äî as researchers, tool vendors, and model builders equipped large language models (LLMs) to make choices and take actions to achieve goals. These developments set the stage for an upswell of agentic activity in the coming year and beyond.\\nDriving the story: Several tools emerged to help developers build agentic workflows.\\n- Microsoft primed the pump for agentic development tools in late 2023 with Autogen, an open source conversational framework that orchestrates collaboration among multiple agents. (Learn how to take advantage of it in our short course ‚Äú AI Agentic Design Patterns with Autogen .‚Äù) In late 2024, part of the Autogen team split off to build AG2 based on a fork of the code base.\\n- In October 2023, CrewAI released its open source Python framework for building and managing multi-agent systems. Agents can be assigned roles and goals, gain access to tools like web search, and collaborate with each other. (DeepLearning.AI‚Äôs short courses ‚Äú Multi-Agent Systems with crewAI ‚Äù and ‚Äú Practical Multi AI-Agents and Advanced Use Cases with crewAI ‚Äù can give you a fast start.)\\n- In January, LangChain, a provider of development tools, introduced LangGraph, which orchestrates agent behaviors using cyclical graphs. The framework enables LLM-driven agents to receive inputs, reason over them, decide on actions, use tools, evaluate the results, and repeat these steps to improve results. (Our short course ‚Äú AI Agents in LangGraph ‚Äù offers an introduction.)\\n- In September, Meta introduced Llama Stack for building agentic applications based on Llama models. Llama Stack provides memory, conversational skills, orchestration services, and ethical guardrails.',\n",
       "  '- Throughout the year, integrated development environments implemented agentic workflows to generate code. For instance, Devin and OpenHands accept natural-language instructions to generate prototype programs. Replit Agent, Vercel‚Äôs V0, and Bolt streamline projects by automatically writing code, fixing bugs, and managing dependencies.\\n- Meanwhile, a number of LLM makers supported agentic workflows by implementing tool use and function calling. Anthropic added computer use , enabling Claude 3.5 Sonnet to control users‚Äô computers directly.\\n- Late in the year, OpenAI rolled out its o1 models and the processing-intensive o1 pro mode, which use agentic loops to work through prompts step by step. DeepSeek-R1 and Google Gemini 2.0 Flash Thinking Mode followed with similar agentic reasoning. In the final days of 2024, OpenAI announced o3 and o3-preview, which further extend o1‚Äôs agentic reasoning capabilities with impressive reported results.\\nBehind the news: Techniques for prompting LLMs in more sophisticated ways began to take off in 2022. They coalesced in moves toward agentic AI early this year. Foundational examples of this body of work include:\\n- Chain of Thought prompting, which asks LLMs to think step by step\\n- Self-consistency , which prompts a model to generate several responses and pick the one that‚Äôs most consistent with the others\\n- ReAc t, which interleaves reasoning and action steps to accomplish a goal\\n- Self-Refine , which enables an agent to reflect on its own output\\n- Reflexion , which enables a model to act, evaluate, reflect, and repeat.\\n- Test-time compute , which increases the amount of processing power allotted to inference\\nWhere things stand: The agentic era is upon us! Regardless of how well scaling laws continue to drive improved performance of foundation models, agentic workflows are making AI systems increasingly helpful, efficient, and personalized.',\n",
       "  '## Prices Tumble\\nFierce competition among model makers and cloud providers drove down the price of access to state-of-the-art models.\\nWhat happened: AI providers waged a price war to attract paying customers. A leading indicator: From March 2023 to November 2024, OpenAI cut the per-token prices of cloud access to its models by nearly 90 percent even as performance improved, input context windows expanded, and the models became capable of processing images as well as text.\\nDriving the story: Factors that pushed down prices include open source, more compute-efficient models, and excitement around agentic workflows that consume more tokens at inference. OpenAI‚Äôs GPT-4 Turbo set a baseline when it debuted in late 2023 at $10.00/$30.00 per million tokens of input/output. Top model makers slashed prices in turn: Google and OpenAI at the higher end of the market, companies in China at the lower end, and Amazon at both. Meanwhile, startups with specialized hardware offered open models at prices that dramatically undercut the giants.\\n- Competitive models with open weights helped drive prices down by enabling cloud providers to offer high-performance models without bearing the cost of developing or licensing them. Meta released Llama 3 70B in April, and various cloud providers offered it at an average price of $0.78/$0.95 per million input/output tokens. Llama 3.1 405B followed in July 2024; Microsoft Azure priced it at almost half the price of GPT-4 Turbo ($5.33/$16.00).\\n- Per-token prices for open weights models tumbled in China. In May, DeepSeek released DeepSeek V2 and soon dropped the price to $0.14/$0.28 per million tokens of input/output. Alibaba, Baidu, and Bytedance slashed prices for Qwen-Long ($0.06/$0.06), Ernie-Speed and Ernie-Lite (free), and Doubau ($0.11/$0.11) respectively.',\n",
       "  '- Makers of closed models outdid one another with lower and lower prices. In May, OpenAI introduced GPT-4o at $5.00/$15.00 per million tokens of input/output, half as much as GPT-4 Turbo. By August, GPT-4o cost $2.50/$10.00 and the newer GPT-4o mini cost $0.15/$0.60 (half as much for jobs with slower turnaround times).\\n- Google ultimately cut the price of Gemini 1.5 Pro to $1.25/$5.00 per million input/output tokens (twice as much for prompts longer than 128,000 tokens) and slashed Gemini 1.5 Flash to $0.075/$0.30 per million input/output tokens (twice as much for prompts longer than 128,000 tokens). As of this writing, Gemini 2.0 Flash is free to use as an experimental preview, and API prices have not been announced.\\n- In December, Amazon introduced the Nova family of LLMs. At launch, Nova Pro ($0.80/$3.20 per million tokens of input/output) cost much less than top models from OpenAI or Google, while Nova Lite ($0.06/$0.24) and Nova Micro ($0.035/$0.14 respectively) cost much less than GPT-4o mini. (Disclosure: Andrew Ng serves on Amazon‚Äôs board of directors.)\\n- Even as model providers cut their prices, startups including Cerebrus, Groq, and SambaNova designed specialized chips that enabled them to serve open weights models faster and more cheaply. For example, SambaNova offered Llama 3.1 405B for $5.00/$10.00 per million tokens of input/output, processing a blazing 132 tokens per second. DeepInfra offered the same model at a slower speed for as little as $2.70/$2.70.\\nYes, but: The trend toward more processing-intensive models is challenged but not dead. In September, OpenAI introduced token-hungry models with relatively hefty price tags: o1-preview ($15.00/$60.00 per million tokens input/output) and o1-mini ($3.00/$12.00). In December, o1 arrived with a more accurate pro mode that‚Äôs available only to subscribers who are willing to pay $200 per month.',\n",
       "  'Behind the news: Prominent members of the AI community pushed against regulations that threatened to restrict open source models, which played an important role in bringing down prices. Opposition by developers helped to block California SB 1047, a proposed law that would have held developers of models above certain size limits liable for unintended harms caused by their models and required a ‚Äúkill switch‚Äù that would enable developers to disable them ‚Äî a problematic requirement for open weights models that anyone could modify and deploy. California Governor Gavin Newsom vetoed the bill in October.\\nWhere things stand: Falling prices are a sign of a healthy tech ecosystem. It‚Äôs likely that in-demand models will always fetch relatively high prices, but the market is increasingly priced in pennies, not dollars, per million tokens.',\n",
       "  '## Generative Video Takes Off\\nVideo generation exploded in an abundance of powerful models.\\nWhat happened: Companies big and small introduced new or updated text-to-video generators. Some added image-to-video and/or video-to-video capabilities. While most models focus on generating cinematic clips, some specialize in videos for social media.\\nDriving the story: Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed. Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while ramping up image resolution, speed, output length, and users‚Äô ability to control their outputs.\\n- OpenAI Sora set a high bar early in the year. Introduced in February and shown privately to Hollywood creators, it built a formidable buzz despite being available to only selected users. Unauthorized users gained access in November, and OpenAI made the model available the following month. Built on a diffusion transformer , Sora generates consistent (if somewhat dreamlike) scenes of up to 1 minute long.\\n- Runway Gen 3 Alpha and Gen 3 Alpha Turbo improved on their predecessors, generating higher-resolution videos (up to 1,280x768-pixel resolution) and introducing an API. Runway struck a deal with the film studio Lionsgate, which will use a custom version fine-tuned on its archive for visual effects and pre-visualizations.\\n- Adobe took a different approach with its Firefly Video model. In addition to offering a web application, the company incorporated the model directly into its best-selling Adobe Premiere Pro video editing suite. The integration enables video artists to generate clips, extend or enhance existing ones, and add effects within the program.',\n",
       "  '- Meta introduced Movie Gen , a suite of four systems. While its video output rivals that of competitors, it stands out especially for its ability to generate soundtracks. One system produces sound effects and music that match video. Another specializes in producing videos in which characters‚Äô faces remain consistent, and another performs video-to-video alterations. Movie Gen will be available on Instagram in 2025.\\n- Model builders in China tailored their models for producing social media. Kling AI emphasized making TikTok and Instagram Reels. PixVerse and Jimeng AI likewise introduced video generators designed for social media users. In October, TikTok‚Äôs parent ByteDance added two video generation models, PixelDance and Seaweed, that produce 10-second and 30-second clips respectively.\\nBehind the news: Video generation is already reshaping the movie industry. In February, after seeing a preview of Sora, American filmmaker Tyler Perry halted a planned expansion of his production studio, arguing that within a few years, AI video could put traditional studios out of business. Members of the video graphics team at The Late Show with Stephen Colbert use Runway‚Äôs technology to add special effects to conventional digital video, cutting editing time from hours to minutes.\\nWhere things stand: Video generation came a long way in 2024, but there‚Äôs still plenty of room for improvement. Because most models only generate a small number of frames at a time, they can struggle to track physics and geometry and to generate consistent characters and scenery over time. The computational demands of maintaining consistency across frames means that generated clips are brief. And even short outputs take substantial time and resources to generate: Sora can take 10 to 20 minutes to render clips as short as 3 seconds. OpenAI and Runway released faster versions ‚Äî Sora Turbo and Gen-3 Alpha Turbo ‚Äî to address the challenge.',\n",
       "  '## Smaller Is Beautiful\\nFor years, the best AI models got bigger and bigger. But in 2024, some popular large language models were small enough to run on a smartphone.\\nWhat happened : Instead of putting all their resources into building big models, top AI companies promoted families of large language models that offer a choice of small, medium, and large. Model families such as Microsoft Phi-3 (in versions of roughly 3.8 billion, 7 billion, and 14 billion parameters), Google Gemma 2 (2 billion, 9 billion, and 27 billion), and Hugging Face SmolLM (135 million, 360 million, and 1.7 billion) specialize in small.\\nDriving the story: Smaller models have become more capable thanks to techniques like knowledge distillation (in which a larger teacher model is used to train a smaller student model to match its output), parameter pruning (which removes less-influential parameters), quantization (which reduces neural network sizes by representing each parameter with fewer bits), and greater attention to curating training sets for data quality. Beyond performance, speed, and price, the ability to run on relatively low-powered hardware is a competitive advantage for a variety of uses.\\n- Model builders have offered model families that include members of various sizes since at least 2019, when Google introduced the T5 family (five models between roughly 77 million parameters and 11 billion parameters). The success of OpenAI‚Äôs GPT series, which over time grew from 117 million parameters to a hypothesized 1.76 trillion parameters, demonstrated the power of bigger models. OpenAI researchers formulated scaling laws that appeared to guarantee that bigger models, training sets, and compute budgets would lead to predictable improvements in performance. This finding spurred rivals to build larger and larger models.\\n- The tide started to turn in early 2023. Meta‚Äôs Llama 2 came in parameter counts of roughly 7 billion, 13 billion, and 70 billion with open weights.',\n",
       "  '- The tide started to turn in early 2023. Meta‚Äôs Llama 2 came in parameter counts of roughly 7 billion, 13 billion, and 70 billion with open weights.\\n- In December 2023, Google launched the Gemini family, including Gemini Nano (1.8 billion parameters). In February, it released the small, open weights family Gemma 1 (2 billion and 7 billion parameters), followed by Gemma 2 (9 billion and 27 billion).\\n- Microsoft introduced Phi-2 (2.7 billion parameters) in December 2023 and Phi-3 (3.8 billion, 7 billion, and 14 billion) in April.\\n- In August, Nvidia released its Minitron models. It used a combination of distillation and pruning to shrink Llama 3.1 from 8 billion to 4 billion parameters and Mistral NeMo from 12 billion to 8 billion parameters, boosting speed and lowering computing costs while maintaining nearly the same level of accuracy.\\nBehind the news: Distillation, pruning, quantization, and data curation are longstanding practices. But these techniques have not resulted in models quite this ratio of size and capability before, arguably because the larger models that are distilled, pruned, or quantized have never been so capable.\\n- In 1989, Yann LeCun and colleagues at Bell Labs published ‚Äú Optimal Brain Damage ,‚Äù which showed that\\xa0 deleting weights selectively could reduce a model‚Äôs size and, in some cases, improve its ability to generalize.\\n- Quantization dates to 1990, when E. Fiesler and colleagues at the University of Alabama demonstrated various ways to represent the parameters of a neural network in ‚Äú A Weight Discretization Paradigm for Optical Neural Networks .‚Äù It made a resurgence 2010‚Äôs with the growth in popularity and sizes of neural networks, which spurred the refinements quantization-aware training and post-training quantization .',\n",
       "  '- In 2006, Rich Caruana and colleagues at Cornell published ‚Äú Model Compression ,‚Äù showing how to train a single model to mimic the performance of multiple models. Geoffrey Hinton and colleagues at Google Brain followed in 2015 with ‚Äú Distilling the Knowledge in a Neural Network ,‚Äù which improved the work of Caruana et al. and introduced the term distillation to describe a more general way to compress models.\\n- Most of the current crop of smaller models were trained on datasets that were carefully curated and cleaned. Higher-quality data makes it possible to get more performance out of fewer parameters. This is an example of data-centric AI , the practice of improving model performance by improving the quality of their training data.\\nWhere things stand: Smaller models dramatically widen the options for cost, speed, and deployment. As researchers find ways to shrink models without sacrificing performance, developers are gaining new ways to build profitable applications, deliver timely services, and distribute processing to the edges of the internet.',\n",
       "  '## Alternatives to Acquisitions\\nBig AI companies found creative ways to gain cutting-edge technology and talent without buying startups.\\nWhat happened: In 2024, some tech giants entered into novel partnership arrangements with AI startups, hiring top executives and securing access to technology without acquiring the companies outright. These agreements enabled the giants to take on elite talent and proven technology quickly with less risk that regulators might hinder such actions. The startups lost their leadership teams and control over key technical developments. In return, they received cash (in some cases, at least), rewarded investors, and were able to step back from the expense of building cutting-edge models.\\nDriving the story: Microsoft, Amazon, and Google used their deep pockets and cloud infrastructure to strike deals with Inflection AI, Adept AI and Covariant, and Character.ai respectively. (Disclosure: Andrew Ng is a member of Amazon‚Äôs board of directors.)\\n- Microsoft blazed the trail in March. The tech giant invested $650 million in Inflection AI, licensed the startup‚Äôs models, integrated its conversational AI technologies, and hired much of its staff, including co-founders Mustafa Suleyman and Kar√©n Simonyan. Microsoft named Suleyman CEO of a new AI division, putting him in charge of Microsoft‚Äôs own model building efforts and consumer-facing products like Bing and the Copilot product line. The remainder of Inflection focuses on customizing AI models for commercial clients.\\n- In July, Amazon inked a similar agreement with Adept, a startup that built agents for tasks such as automating data entry and managing customer support tickets, under undisclosed terms. Amazon hired most of Adept AI‚Äôs staff, including CEO David Luan and other co-founders who were alumni from Google and OpenAI, and licensed Adept‚Äôs models, datasets, and other technology non-exclusively. Adept stopped developing in-house models to concentrate on building agents.',\n",
       "  '- In October, Amazon further bolstered its logistics capabilities by forging an agreement with Covariant, a maker of AI-driven warehouse robots, also under undisclosed terms. Amazon hired most of the startup‚Äôs staff, including CEO/co-founder Peter Chen and chief scientist/co-founder Pieter Abbeel, and licensed its robotics models. In December, Amazon paired Abbeel and former Adept CEO Luan to run a new lab devoted to developing agents and artificial general intelligence. Covariant continues to serve customers in fulfillment centers and other industries.\\n- In August, Google and conversational AI startup Character.ai cut a similar deal. Google hired Character.ai‚Äôs co-founders, Noam Shazeer and Daniel De Freitas, along with key team members, and inked a non-exclusive license to its technology. Shazeer joined Google‚Äôs Deep Learning research team, and other new hires set to work on Google‚Äôs chat services. Google gave Character.ai an undisclosed sum to buy out its investors and continue developing personalized AI products.\\nBehind the news: Tech giants have long relied on traditional acquisitions to gain new talent and capabilities, often acquiring startups specifically for their skilled teams (known as an acquihire) and/or their products or underlying technology, which can be expensive and time-consuming to develop and test in the market. But traditional acquisitions increasingly face scrutiny from antitrust regulators who are concerned about big companies reducing competition by buying out smaller ones. For example, the United States Federal Trade Commission sought to block Amazon‚Äôs acquisition of iRobot, prompting the companies to abandon the transaction in January 2024.',\n",
       "  '## Introduction\\nDear friends,\\nTrump and the Republican party chalked up huge wins this week. Did manipulation of social media by generative AI play any role in this election? While many have worried about AI creating fake or misleading content that influences people, generative AI has probably not been the primary method of manipulation in this election cycle. Instead, I think a bigger impact might have been the ‚Äúamplification effect‚Äù where software bots ‚Äî which don‚Äôt have to rely heavily on generative AI ‚Äî create fake engagement (such as likes/retweets/reshares), leading social media companies‚Äô recommendation algorithms to amplify certain content to real users, some of whom promote it to their own followers. This is how fake engagement leads to real engagement.\\nThis amplification effect is well known to computer security researchers. It is an interesting sign of our global anxiety about AI that people ascribe social media manipulation to AI becoming more powerful. But the problem here is not that AI is too powerful; rather, it is that AI is not powerful enough. Specifically, the issue is not that generative AI is so powerful that hostile foreign powers or unethical political operatives are successfully using it to create fake media that influences us; the problem is that some social media companies‚Äô AI algorithms are not powerful enough to screen out fake engagement by software bots, and mistake it for real engagement by users. These bots (which don‚Äôt need to be very smart) fool the recommender algorithms into amplifying certain content.\\nThe Washington Post reported that tweets on X/Twitter posted by Republicans were more viral than tweets from Democrats . Did this reflect the audience‚Äôs deeper engagement with Republican messages than Democratic ones, or have bots influenced this by boosting messages on either side? It is hard to know without access to Twitter‚Äôs internal data.',\n",
       "  'The bottleneck to disinformation is not creating it but disseminating it . It is easy to write text that proposes a certain view, but hard to get many people to read it. Rather than generating a novel message (or using deepfakes to generate a misleading image) and hoping it will go viral, it might be easier to find a message written by a real human that supports a point of view you want to spread, and use bots to amplify that.\\nI don‚Äôt know of any easy technical or legislative approach to combating bots. But it would be a good step to require transparency of social media platforms so we can better spot problems, if any. Everyone has a role to play in protecting democracy, and in tech, part of our duty will be to make sure social media platforms are fair and defend them against manipulation by those who seek to undermine democracy.\\nDemocracy is one of humanity‚Äôs best inventions. Elections are an important mechanism for protecting human rights and supporting human flourishing. Following this election, we must continue to strenuously nourish democracy and make sure this gem of human civilization continues to thrive.\\nKeep learning!\\nAndrew\\nLearn the principles of effective data engineering in this four-course professional certificate taught by Joe Reis. Develop your skills in the data engineering lifecycle and gain hands-on experience building data systems on Amazon Web Services. Earn a certificate upon completion! Enroll today',\n",
       "  '## Claude Controls Computers\\nAPI commands for Claude Sonnet 3.5 enable Anthropic‚Äôs large language model to operate desktop apps much like humans do. Be cautious, though: It‚Äôs a work in progress.\\nWhat‚Äôs new: Anthropic launched API commands for computer use. The new commands prompt Claude Sonnet 3.5 to translate natural language instructions into commands that tell a computer to open applications, fetch data from local files, complete forms, and the like. (In addition, Anthropic improved Claude Sonnet 3.5 to achieve a state-of-the-art score on the SWE-bench Verified coding benchmark and released the faster, cheaper Claude Haiku 3.5, which likewise shows exceptional performance on coding tasks.)\\nHow it works: The commands for computer use don‚Äôt cost extra on a per-token basis, but they may require up to 1,200 additional tokens and run repeatedly until the task at hand is accomplished, consuming more input tokens. They‚Äôre available via Anthropic, Amazon Bedrock, and Google Vertex.\\n- Claude Sonnet 3.5 can call three new tools: Computer (which defines a computer‚Äôs screen resolution and offers access to its keyboard, mouse, and applications), Text Editor, and Bash (a terminal that runs command-line programs in various languages). The model can compose Python scripts in the text editor, run them in Bash, and store outputs in a spreadsheet.\\n- The model tracks a computer‚Äôs state by taking screenshots. This enables it to see, for example, the contents of a spreadsheet and respond to changes such as the arrival of an email. It examines pixel locations to move the cursor, click, and enter text accordingly. An agentic loop prompts it to execute actions, observe results, and change or correct its own behavior until it completes the task at hand.',\n",
       "  \"- On OSWorld , a benchmark that evaluates AI models' abilities to use computers, Claude Sonnet 3.5 succeeded at about 15 percent of tasks when given 15 attempts. Cradle, the next-best system, achieved about 8 percent, and GPT-4V achieved about 7.5 percent.\\xa0 Human users typically complete about 72 percent.\\nYes, but: The current version of computer use is experimental, and Anthropic acknowledges various limitations. The company strongly recommends using these commands only in a sandboxed environment, such as a Docker container, with limited access to the computer‚Äôs hard drive and the web to protect sensitive data and core system files. Anthropic restricts the ability to create online accounts or post to social media or other sites (but says it may lift this restriction in the future).\\nBehind the news: Several companies have been racing to build models that can control desktop applications. Microsoft researchers recently released OmniParser , a tool based on GPT-4V that identifies user-interface elements like windows and buttons within screenshots, potentially making it easier for agentic workflows to navigate computers. In July, Amazon hired staff and leaders from Adept, a startup that trained models to operate computer applications. (Disclosure: Andrew Ng sits on Amazon‚Äôs board of directors.) Open Interpreter is an open-source project that likewise uses a large language model to control local applications like image editors and web browsers.\\nWhy it matters: Large multimodal models already use external tools like search engines, web browsers, calculators, calendars, databases, and email. Giving them control over a computer‚Äôs visual user interface may enable them to automate a wider range of tasks we use computers to perform, such as creating lesson plans and ‚Äî more worrisome ‚Äî taking academic tests .\",\n",
       "  'We‚Äôre thinking: Controlling computers remains hard. For instance, using AI to read a screenshot and pick the right action to take next is very challenging. However, we‚Äôre confident that this capability will be a growth area for agentic workflows in coming years.',\n",
       "  '## Robots On the Loading Dock\\nShipping ports are the latest front in the rising tension between labor unions and AI-powered automation.\\nWhat‚Äôs new: Autonomous vehicles, robotic cranes, and computer vision systems increasingly manage the flow of goods in and out of ports worldwide. Dockworkers in the United States are worried that such technology threatens their livelihoods, The Wall Street Journal reported .\\nHow it works: Automation boosts the number of containers a port can move per hour from vessel to dock. For instance, Shanghai‚Äôs Yangshan Deep Water Port, one of the world‚Äôs most automated ports, moves more than 113 containers per hour, while Oakland, California‚Äôs less-automated port moves around 25 containers per hour, according to a report by S&P Global Market Intelligence for the World Bank.\\n- Self-driving vehicles transport containers between docks and stacking yards, navigating by techniques such as following lines painted on the floor. In ports like Yangshan and Rotterdam , zero-emission automated vehicles work continuously without human intervention.\\n- Automated stacking cranes work in tandem with self-driving vehicles to manage containers in port yards. They reposition containers when they‚Äôre not needed for efficient use of available space. Rotterdam‚Äôs automated cranes boost productivity by 40 percent compared to conventional terminals.\\n- Remote-controlled ship-to-shore cranes load and unload vessels, improving safety and efficiency. In Rotterdam, such cranes can move up to 30 containers per hour, while manual cranes move 25 to 28 containers per hour.\\n- AI-powered systems monitor container movements and read identification codes to streamline the flow of cargo. These systems check containers into and out of the port automatically and track their locations in real time.\\n- Data management systems coordinate all automated equipment to predict schedules and reduce bottlenecks.',\n",
       "  '- Data management systems coordinate all automated equipment to predict schedules and reduce bottlenecks.\\nDockworkers disagree: Harold Daggett, leader of the International Longshoremen‚Äôs Association, a union that negotiates on behalf of dockworkers, vowed to fight port automation, which he sees as a pretext to eliminate jobs. He has proposed that members of unions internationally refuse work for shipping companies that use automated equipment. Fresh from a three-day strike in early October, longshoremen will return to negotiations with shipping companies in mid-January.\\nWhy it matters: Ports are one of many work environments where AI is bringing down costs while improving throughput. In many such situations, humans can continue to perform tasks that machines don‚Äôt do well. But where human jobs are at risk, society must determine the most productive path. Dockworkers, through their unions, have significant power in this equation. A protracted U.S. dockworker strike risks economic losses of up to $7.5 billion a week . On the other hand, automation could bring tremendous gains in safety, speed, and economic efficiency.\\nWe‚Äôre thinking: We are very sympathetic to workers‚Äô rights. Yet we also believe that more-efficient ports will boost commerce, creating many new jobs. As traditional roles change, workers need opportunities to learn new skills and adapt to the evolving job market. Society has a responsibility to provide a safety net as well as training and education for those whose jobs are threatened by automation.',\n",
       "  '## Does Your Model Comply With the AI Act?\\nA new study suggests that leading AI models may meet the requirements of the European Union‚Äôs AI Act in some areas, but probably not in others.\\nWhat‚Äôs new: The Zurich-based startup LatticeFlow, working with research institutions in Bulgaria and Switzerland, developed COMPL-AI , an unofficial framework designed to evaluate large language models‚Äô likely compliance with the AI Act. A leaderboard ranks an initial selection of models. (LatticeFlow does not work for the European Commission or have legal standing to interpret the AI Act.)\\nHow it works: A paper explains how COMPL-AI maps the AI Act‚Äôs requirements to specific\\xa0benchmarks. It evaluates each requirement using new or established tests and renders an aggregate score. These scores are relative measures, and the authors don‚Äôt propose thresholds for compliance. The assessment covers five primary categories:\\n- Technical robustness and safety. The AI Act requires that models return consistent responses despite minor variations in input prompts and resist adversarial attacks. The framework uses metrics like MMLU and BoolQ to assess the impact of small changes in a prompt‚Äôs wording. It measures monotonicity (consistency in the relationship between specific inputs and outputs) to see how well a model maintains its internal logic across prompts. It uses Tensor Trust and LLM RuLES to gauge resistance to cyberattacks. This category also examines whether a model can identify and correct its own errors.\\n- Privacy and data protection. Model output must be free of errors, bias, and violations of laws governing privacy and copyright. The framework looks for problematic examples in a model‚Äôs training dataset and assesses whether a model repeats erroneous, personally identifying, or copyrighted material that was included in its training set. Many developers don‚Äôt provide their models‚Äô training datasets, so the authors use open datasets such as the Pile as a proxy.',\n",
       "  '- Transparency and interpretability. Developers must explain the capabilities of their models, and the models themselves must enable those who deploy them to interpret the relationships between inputs and outputs. Measures of interpretability include TriviaQA and Expected Calibration Error , which test a model‚Äôs ability to gauge its own accuracy. The framework also assesses such requirements by, for instance, testing whether a model will tell users they‚Äôre interacting with a machine rather than a person, and whether it watermarks its output.\\n- Fairness and non-discrimination. The law requires that model providers document potentially discriminatory outputs of their systems and that high-risk systems reduce the risk of biased outputs. The framework uses tests like RedditBias , BBQ , and BOLD to gauge biased language, and FaiRLLM to assess equitable outputs. It uses DecodingTrust to measure fairness across a variety of use cases.\\n- Social and environmental wellbeing. Developers of high-risk systems must minimize harmful and undesirable behavior, and all AI developers must document consumption of energy and other resources used to build their models as well as their efforts to reduce it.\\xa0The framework uses RealToxicityPrompts and AdvBench to measure a model‚Äôs propensity to generate objectionable or otherwise toxic output. It calculates a model‚Äôs carbon footprint to measure environmental wellbeing.\\nResults: The authors evaluated nine open models and three proprietary ones on a scale between 0 and 1. Their reports on each model reveal considerable variability. (Note: The aggregate scores cited in the reports don‚Äôt match those in the paper.)\\n- All models tested performed well on benchmarks for privacy and data governance (achieving scores of 0.99 or 1) and social and environmental well-being (0.96 or above). However, several achieved relatively low scores in fairness and security, suggesting that bias and vulnerability to adversarial attacks are significant issues.',\n",
       "  '- GPT-4 Turbo and Claude 3 Opus achieved the highest aggregate score, 0.89. However, their scores were diminished by low ratings for transparency, since neither model‚Äôs training data is disclosed.\\n- Gemma-2-9B ranked lowest with an aggregate score of 0.72. It also scored lowest on tests of general reasoning (MMLU), common-sense reasoning (HellaSwag), and self-assessment (a model‚Äôs certainty in its answers to TriviaQA).\\n- Some models performed well on typical benchmark tasks but less well in areas that are less well studied or easily measured. For instance, Qwen1.5-72B struggled with interpretability (0.61). Mixtral-8x7B performed poorly in resistance to cyberattacks (0.32).\\nYes, but: The authors note that some provisions of the AI Act, including explainability, oversight (deference to human control), and corrigibility (whether an AI system can be altered to change harmful outputs, which bears on a model‚Äôs risk classification under the AI Act), are defined ambiguously under the law and can‚Äôt be measured reliably at present. These areas are under-explored in the research literature and lack benchmarks to assess them.\\nWhy it matters: With the advent of laws that regulate AI technology, developers are responsible for assessing a model‚Äôs compliance before they release it or use it in ways that affect the public. COMPL-AI takes a first step toward assuring model builders that their work is legally defensible or else alerting them to flaws that could lead to legal risk if they‚Äôre not addressed prior to release.\\nWe‚Äôre thinking: Thoughtful regulation of AI is necessary, but it should be done in ways that don‚Äôt impose an undue burden on developers. While the AI Act itself is overly burdensome, we‚Äôre glad to see a largely automated path to demonstrating compliance of large language models.',\n",
       "  '## When Agents Train Algorithms\\nCoding agents are improving, but can they tackle machine learning tasks?\\nWhat‚Äôs new: Chan Jun Shern and colleagues at OpenAI introduced MLE-bench , a benchmark designed to test how well AI coding agents do in competitions hosted by the Kaggle machine learning contest platform. The benchmark is available here .\\nAgentic framework basics: An agentic framework or scaffold consists of a large language model (LLM) and code to prompt the model to follow a certain procedure. It may also contain tools the LLM can use, such as a Python console or web browser. For example, given a problem to solve, a framework might prompt the model to generate code, run the code in the Python console, generate evaluation code, run evaluation code, change the solution based on the console‚Äôs output, and repeat until the problem is solved.\\nHow it works: MLE-bench is an offline competition environment that contains 75 Kaggle competitions selected manually by the authors, such as contests to identify toxic comments and predict volcanic eruptions . Each competition includes a description, training and testing datasets, code to grade submissions, a leaderboard of human contestants for comparison with an agent‚Äôs performance, and a ‚Äúcomplexity‚Äù rating (produced by OpenAI): low (takes an experienced human less than two hours to code a solution, not including training time), medium (between two and 10 hours), or high (more than 10 hours). Given a competition, an agent must produce a submission by (i) generating code to train a machine learning model and (ii) running the model on the test set. Users grade the submission to evaluate the agent‚Äôs performance.\\n- The authors ran their benchmark on three open source agentic frameworks using GPT-4o as the LLM. The frameworks were AIDE , ResearchAgent , and CodeActAgent . AIDE earned the highest score.',\n",
       "  '- The authors ran their benchmark on three open source agentic frameworks using GPT-4o as the LLM. The frameworks were AIDE , ResearchAgent , and CodeActAgent . AIDE earned the highest score.\\n- They ran their benchmark again on AIDE, this time using four different LLMs: o1-preview, GPT-4o, Claude 3.5 Sonnet, and Llama 3.1 405B.\\n- To make sure the agents didn‚Äôt find the solution in a web search or use a successful solution that was included in the LLM‚Äôs training data, the authors performed two checks: (i) GPT-4o checked the agent‚Äôs logs for calls to an external API or downloads of restricted resources and (ii) the Dolos anti-plagiarism tool compared the agent‚Äôs submission with the top 50 human submissions.\\nResults: The authors evaluated agent performance according to Kaggle‚Äôs standards for awarding medals to human contestants (described in the final bullet below).\\n- The pairing of AIDE/o1-preview performed best, winning medals in 16.9 percent of competitions.\\n- AIDE/GPT-4o was a distant second place with medals in 8.7 percent of competitions.\\n- AIDE/Claude 3.5 Sonnet won medals in 7.6 percent of competitions.\\n- AIDE/Llama 3.1 won medals in 3 percent of competitions.\\n- Kaggle does not award medals for certain types of competition. However, for competitions in which it does award medals, it uses the following formula: For competitions in which less than 250 human teams participated, contestants win a medal if they score within the top 40 percent. For competitions in which 250 to 999 teams participated, they win a medal if they score in the top 100. For competitions that included 1,000 teams or more, they win a medal if they score within the top 10 percent.',\n",
       "  'Yes, but: The percentage of medals won by agents in this study is not comparable to percentages of medals won by humans on Kaggle. The authors awarded medals for excellent performance in all competitions included in the benchmark, but Kaggle does not. The authors didn‚Äôt tally the agents‚Äô win rate for only competitions in which Kaggle awarded medals.\\nWhy it matters: It‚Äôs important to evaluate the abilities of coding agents to solve all kinds of programming problems. Machine learning tasks are especially valuable as they bear on the ability of software to analyze unstructured data and adapt to changing conditions.\\nWe‚Äôre thinking: We‚Äôre glad to see machine learning catching on among humans and machines alike!',\n",
       "  'How scared should you be: Training on synthetic data is at the heart of some of today‚Äôs best-performing models, including the Llama 3.1, Phi 3, and Claude 3 model families. (Meta showed that using an agentic workflow with Llama 3.0 to generate data ‚Äî rather than generating data directly ‚Äî resulted in useful data to train Llama 3.1.) This approach is essential to the technique known as knowledge distillation, which makes smaller, more parameter-efficient models. Moreover, it‚Äôs valuable for building models that can perform tasks for which little real-world data is available, for instance machine translation models that can handle languages spoken by relatively small populations. Although the authors of ‚ÄúThe Curse of Recursion‚Äù found that training a series of models, each exclusively on the output of the previous one, leads to rapid degradation in performance, introducing even 10 percent real-world data significantly curbed this decline.\\nFacing the fear: Model collapse is not a near-term risk, and perhaps not any risk at all, given research progress on generating synthetic data. Still, it makes sense to track the presence of generated data in training datasets and include it carefully. The large-scale web dataset Common Crawl captures regular snapshots of the web. If generated data were to inundate the online environment, using an earlier snapshot would eliminate a huge amount of it. More broadly, model builders increasingly curate high quality data, and whether a given example appears to have been generated will become a factor. Datasets can be filtered using algorithms designed to identify generated content. Increasing use of watermarking would make the job still easier. These measures will help developers ensure a healthy balance of real and generated data in training sets for a long time to come.',\n",
       "  '## Introduction\\nDear friends,\\nLarge language models (LLMs) are typically optimized to answer peoples‚Äô questions. But there is a trend toward models also being optimized to fit into agentic workflows. This will give a huge boost to agentic performance!\\nFollowing ChatGPT‚Äôs breakaway success at answering questions, a lot of LLM development focused on providing a good consumer experience. So LLMs were tuned to answer questions (‚ÄúWhy did Shakespeare write Macbeth ?‚Äù) or follow human-provided instructions (‚ÄúExplain why Shakespeare wrote Macbeth ‚Äù). A large fraction of the datasets for instruction tuning guide models to provide more helpful responses to human-written questions and instructions of the sort one might ask a consumer-facing LLM like those offered by the web interfaces of ChatGPT, Claude, or Gemini.\\nBut agentic workloads call on different behaviors. Rather than directly generating responses for consumers, AI software may use a model in part of an iterative workflow to reflect on its own output, use tools, write plans, and collaborate in a multi-agent setting. Major model makers are increasingly optimizing models to be used in AI agents as well.\\nTake tool use (or function calling ). If an LLM is asked about the current weather, it won‚Äôt be able to derive the information needed from its training data. Instead, it might generate a request for an API call to get that information. Even before GPT-4 natively supported function calls, application developers were already using LLMs to generate function calls, but by writing more complex prompts (such as variations of ReAct prompts) that tell the LLM what functions are available and then have the LLM generate a string that a separate software routine parses (perhaps with regular expressions) to figure out if it wants to call a function.',\n",
       "  'Generating such calls became much more reliable after GPT-4 and then many other models natively supported function calling. Today, LLMs can decide to call functions to search for information for retrieval-augmented generation (RAG), execute code, \\xa0send emails, place orders online, and much more.\\nRecently, Anthropic released a version of its model that is capable of computer use, using mouse-clicks and keystrokes to operate a computer (usually a virtual machine). I‚Äôve enjoyed playing with the demo . While other teams have been prompting LLMs to use computers to build a new generation of RPA (robotic process automation) applications, native support for computer use by a major LLM provider is a great step forward. This will help many developers!\\nAs agentic workflows mature, here is what I am seeing:\\n- First, many developers are prompting LLMs to carry out the agentic behaviors they want. This allows for quick, rich exploration!\\n- In a much smaller number of cases, developers who are working on very valuable applications will fine-tune LLMs to carry out particular agentic functions more reliably. For example, even though many LLMs support function calling natively, they do so by taking as input a description of the functions available and then (hopefully) generating output tokens to request the right function call. For mission-critical applications where generating the right function call is important, fine-tuning a model for your application‚Äôs specific function calls significantly increases reliability. (But please avoid premature optimization! Today I still see too many teams fine-tuning when they should probably spend more time on prompting before they resort to this.)',\n",
       "  '- Finally, when a capability such as tool use or computer use appears valuable to many developers, major LLM providers are building these capabilities directly into their models. Even though OpenAI o1-preview‚Äôs advanced reasoning helps consumers, I expect that it will be even more useful for agentic reasoning and planning.\\nMost LLMs have been optimized for answering questions primarily to deliver a good consumer experience, and we‚Äôve been able to ‚Äúgraft‚Äù them into complex agentic workflows to build valuable applications. The trend of LLMs built to support particular operations in agents natively will create a lot of lift for agentic performance. I‚Äôm confident that large agentic performance gains in this direction will be realized in the next few years.\\nKeep learning!\\nAndrew\\nPrevent common issues in applications based on large language models such as hallucinations, data leaks, and off-topic responses. Build guardrails that protect against incorrect or sensitive responses in our new short course, made in collaboration with GuardrailsAI. Sign up now!',\n",
       "  '## Mixture of Experts Pulls Ahead\\nA new open source large language model outperforms competitors, including the open-weights Llama 3.1 405B, on a variety of benchmarks.\\nWhat‚Äôs new: Tencent released Hunyuan-Large , a mixture-of-experts model with open code and open weights . It comes in base and instruction-tuned versions, both of which can process a relatively large input context window of 256,000 tokens. It‚Äôs free for developers outside the European Union who have fewer than 100 million monthly users. You can experiment with it here .\\nMixture of experts (MoE) basics: The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\\nHow it works: Hunyuan-Large comprises 389 billion parameters but uses 52 billion parameters to process any given input. The team pretrained the model on 7 trillion tokens primarily of English and Chinese text, of which 5.5 trillion tokens came from unspecified sources and 1.5 trillion synthetic tokens were generated by unspecified large language models. The models used to generate training data were ‚Äúspecialized‚Äù to provide expert-level responses in various domains. The team fine-tuned Hunyuan-Large on unspecified datasets of instructions and human feedback.\\n- MoE models typically select which expert(s) to use based on the input. Hunyuan-Large chooses one of 16 experts, but it also uses a shared expert ‚Äî an expert that processes every input.',\n",
       "  '- MoE models typically select which expert(s) to use based on the input. Hunyuan-Large chooses one of 16 experts, but it also uses a shared expert ‚Äî an expert that processes every input.\\n- Recent research showed that there is a formula for the optimal learning rate based on the batch size (the number of examples a model sees during one training step). The shared expert and the chosen expert see a different amount of data in each training step, so the team modified the learning rate for the chosen expert based on that formula.\\nResults: The team compared the Hunyuan-Large models to four open source models and their instruction-tuned versions: Llama 3.1 70B, Llama 3.1 405B, and the MoE models Mixtral-8x22B and DeepSeek-V2.\\n- Hunyuan-Large achieved the best performance on 15 of 19 benchmarks that test English, Chinese, math, and coding proficiency. For example, on MMLU (answering multiple choice questions in topics including elementary mathematics, history, computer science, and law), Hunyuan-Large achieved 88.4 percent accuracy. The next-best competitor, Llama 3.1 405B, achieved 85.2 percent.\\n- The instruction-tuned version achieved the best performance on 10 of 13 benchmarks including measures of instruction-following ability and alignment with certain human preferences. For instance, Hunyuan-Large-Instruct maintained its dominance on MMLU (89.9 percent accuracy to Llama 3.1 405B Instruct‚Äôs 87.3 percent accuracy). On AlpacaEval 2, an instruction-following benchmark, Hunyuan-Large-Instruct achieved 51.8 percent, while the next-best competitor, DeepSeek 2.5 Chat, achieved 50.5 percent.\\nWhy it matters: Hunyuan-Large generally outperforms Llama 405B, achieving the performance of a 405 billion parameter model while computing only 52 billion parameters. That‚Äôs a significantly lower processing requirement, and the model is free for many purposes.',\n",
       "  'We‚Äôre thinking: Setting aside Switch Transformer ‚Äî a 1.6 trillion parameter behemoth that was built to test the limits of size rather than performance ‚Äî Hunyuan-Large is among the largest MoE models we‚Äôve come across. It‚Äôs an impressive demonstration of what larger MoE models can accomplish.',\n",
       "  '## Big AI Pursues Military Contracts\\nTwo top AI companies changed their stances on military and intelligence applications.\\nWhat‚Äôs new: Meta made its Llama family of large language models available to the U.S. government for national security purposes ‚Äî a major change in its policy on military applications. Similarly, Anthropic will offer its Claude models to U.S. intelligence and defense agencies.\\nHow it works: Meta and Anthropic are relying on partnerships with government contractors to navigate the security and procurement requirements for military and intelligence work.\\n- Meta‚Äôs partners in the defense and intelligence markets include Accenture, Amazon, Anduril, Booz Allen, Databricks, Deloitte, IBM, Leidos, Lockheed Martin, Microsoft, Oracle, Palantir, Scale AI, and Snowflake. These companies will integrate Llama models into U.S. government applications in areas like logistics, cybersecurity, intelligence analysis, and tracking terrorists‚Äô financial activities.\\n- Some Meta partners have built specialized versions of Llama. For example, Scale AI fine-tuned Llama 3 for national security applications. Called Defense Llama, the fine-tuned model can assist with tasks such as planning military operations and analyzing an adversary‚Äôs vulnerabilities.\\n- Anthropic will make its Claude 3 and 3.5 model families available to U.S. defense and intelligence agencies via a platform built by Palantir, which provides big-data analytics to governments, and hosted by Amazon Web Services. The government will use Claude to review documents, find patterns in large amounts of data, and help officials make decisions.',\n",
       "  'Behind the news: In 2018, Google faced backlash when it won a contract with the U.S. government to build Project Maven , an AI-assisted intelligence platform. Employees protested, resigned, and called on the company to eschew military AI work. Google withdrew from the project and Palantir took it over. Subsequently, many AI developers, including Meta and Anthropic, have forbidden use of their models for military applications. Llama‚Äôs new availability to U.S. military and intelligence agencies is a notable exception. In July, Anthropic, too, began to accommodate use of its models for intelligence work. Anthropic still prohibits using Claude to develop weapons or mount cyberattacks.\\nWhy it matters: The shift in Meta‚Äôs and Anthropic‚Äôs policies toward military uses of AI is momentous. Lately AI has become a battlefield staple in the form of weaponized drones , and AI companies must take care that their new policies are consistent with upholding human rights. Military uses for AI include not only weapons development and targeting but also potentially life-saving search and rescue, logistics, intelligence, and communications. Moreover, defense contracts represent major opportunities for AI companies that can fund widely beneficial research and applications.\\nWe‚Äôre thinking: Peace-loving nations face difficult security challenges, and AI can be\\xa0 helpful in meeting them. At the same time, the militarization of AI brings challenges to maintaining peace and stability, upholding human rights, and retaining human control over autonomous systems. We call on developers of military AI to observe the guidelines , proposed by Responsible Artificial Intelligence in the Military, which are endorsed by more than 60 countries and call for robust governance, oversight, accountability, and respect for human rights.',\n",
       "  '## Voter‚Äôs Helper\\nSome voters navigated last week‚Äôs United States elections with help from a large language model that generated output based on verified, nonpartisan information.\\nWhat‚Äôs new: Perplexity, an AI-powered search engine founded in 2022 by former OpenAI and Meta researchers, launched its Election Information Hub , an AI-enhanced website that combines AI-generated analysis with real-time data. The model provided live updates, summaries, and explanations of key issues in the recent national, state, and local elections in the U.S. (The hub remains live, but it no longer displays information about local contests or delivers detailed results for election-related searches.)\\nHow it works: Perplexity partnered with Associated Press for election news and Democracy Works , a nonprofit that develops technology and data related to democracy. Democracy Works provided an API for information about elections, issues, and polling locations.\\n- Users could search by candidate, issue, state, district, or postal code. For example, searching a\\xa0postal code returned AI-generated summaries of local races, measures, or other ballot issues drawn from vetted sources such as Ballotpedia, a nonpartisan clearinghouse for election information. A chatbot window enabled users to ask questions and drill down on citations of information sources.\\n- Initial testing by The Verge revealed problems with accuracy in AI-generated summaries. These included outdated information (for example, summaries failed to consistently note Robert F. Kennedy Jr.‚Äôs withdrawal from the presidential election), mistakes in candidate profiles, and mishandling of write-in candidates. Perplexity eventually fixed many of the errors.',\n",
       "  'Behind the news: While Perplexity courted demand for AI-generated information about the U.S. elections, other search-engine providers took more cautious approaches. You.com offered an election chatbot that focused on vote tallies provided by Decision Desk HQ, an election information broker, rather than information about issues or polling locations. Google and Microsoft Bing emphasized information from vetted sources. Microsoft Copilot and OpenAI (which had launched its SearchGPT service the week before the election) simply declined to answer election-related questions, referring users to other sources of information.\\nWhy it matters: Chatbots are maturing to the point where they can provide fairly trustworthy information in high-stakes decisions like elections. The combination of web search and retrieval-augmented generation contributes to decision support systems that are both personalized and accurate.\\nWe‚Äôre thinking: Perfect information is hard to come by in any election. Traditional media, social media, and your uncle‚Äôs strongly held opinions all have limitations. Chatbots aren‚Äôt perfect either, but when they‚Äôre properly designed to avoid biased output and outfitted with high-quality information sources, they can help strengthen users‚Äô choices and voices.',\n",
       "  '## Free Agents\\nAn open source package inspired by the commercial agentic code generator Devin aims to automate computer programming and more.\\nWhat‚Äôs new: OpenHands , previously known as OpenDevin, implements a variety of agents for coding and other tasks. It was built by Xingyao Wang and a team at University of Illinois Urbana-Champaign, Carnegie Mellon, Yale, University of California Berkeley, Contextual AI, King Abdullah University of Science and Technology, Australian National University, Ho Chi Minh City University of Technology, Alibaba, and All Hands AI. The code is free to download , use, and modify.\\nHow it works: OpenHands provides a set of agents, or workflows for the user‚Äôs choice of large language models. Users can command various agents to generate, edit, and run code; interact with the web; and perform auxiliary tasks related to coding and other work. The agents run in a secure Docker container with access to a server to execute code, a web browser, and tools that, say, copy text from pdfs or transcribe audio files.\\n- The CodeAct agent follows the CodeAct framework, which specifies an agentic workflow for code generation. Given a prompt or results of a code execution, it can ask for clarification, write code and execute it, and deliver the result. It can also retrieve relevant information from the web.\\n- The browsing agent controls a web browser. At every time step, it receives the user‚Äôs prompt and a text description of each element it sees on the resulting webpage. The description includes a numerical identifier, words like ‚Äúparagraph‚Äù or ‚Äúbutton‚Äù (and associated text), a list of possible actions (such as scroll, click, wait, drag and drop, and send a message to the user), an example chain of thought for selecting an action, and a list of previous actions taken. It executes actions iteratively until it has sent a message to the user.',\n",
       "  '- A set of ‚Äúmicro agents‚Äù perform auxiliary tasks such as writing commit messages, writing Postgres databases, summarizing codebases, solving math problems, delegating actions to other agents, and the like. Users can write their own prompts to define micro agents.\\nResults: Overall, OpenHands agents achieve similar performance to previous agents on software engineering problems, web browsing, and miscellaneous tasks like answering questions. For example, fixing issues in Github in SWE-Bench , the CodeAct agent using Claude 3.5 Sonnet solved 26 percent while Moatless Tools using the same model solved 26.7 percent. On GPQA Diamond , a set of graduate-level questions about physics, chemistry, and biology, the CodeAct agent using GPT-4-turbo with search wrote code to perform the necessary calculations and found relevant information to answer the questions, achieving 51.8 percent accuracy. GPT-4 with search achieved 38.8 percent accuracy.\\nWhy it matters: Agentic workflows are rapidly expanding the scope and capabilities of large language models. As open source software, this system gives developers an extensible toolkit for designing agentic systems. Although it‚Äôs oriented toward coding, it accommodates a variety of information-gathering, -processing, and -publishing tasks.\\nWe‚Äôre thinking: This system lets users tailor custom agents simply by rewriting prompts. We look forward to seeing what non-programmers do with it!\\nBuild AI applications that have long-term agentic memory! Our short course ‚ÄúLLMs as Operating Systems: Agent Memory‚Äù is based on insights from the MemGPT paper and taught by two of its coauthors. Learn how to implement persistent, efficient memory management for applications based on large language models. Enroll for free',\n",
       "  'Yes, but: The percentage of medals won by agents in this study is not comparable to percentages of medals won by humans on Kaggle. The authors awarded medals for excellent performance in all competitions included in the benchmark, but Kaggle does not. The authors didn‚Äôt tally the agents‚Äô win rate for only competitions in which Kaggle awarded medals.\\nWhy it matters: It‚Äôs important to evaluate the abilities of coding agents to solve all kinds of programming problems. Machine learning tasks are especially valuable as they bear on the ability of software to analyze unstructured data and adapt to changing conditions.\\nWe‚Äôre thinking: We‚Äôre glad to see machine learning catching on among humans and machines alike!',\n",
       "  \"## Introduction\\nDear friends,\\nIt‚Äôs high time to take geoengineering more seriously as a potential tool to mitigate climate change. 2023 was the hottest year on record, and 2024 is likely to top that. In the United States, Hurricane Helene caused over 200 deaths, and Hurricane Milton's death toll is at least two dozen. It‚Äôs well established that the hurricanes are growing stronger as global temperatures rise.\\nWhile stratospheric aerosol injection (SAI) ‚Äî which sprays particles (aerosols) in the atmosphere to provide a small amount of shade from the sun ‚Äî is far from a perfect solution, we should take it seriously as a possible tool for saving lives. A few months ago, my collaborators and I had released a climate emulator, Planet Parasol , that you can play with to simulate different SAI scenarios to understand its possible impact. By using AI to model its impact and thereby advance our understanding of SAI, we‚Äôll be better prepared to decide if this is a good step.\\nThe key idea of SAI, which is a form of climate geoengineering, is to spray reflective particles into the stratosphere to reflect a little more, say 1%, of the sunlight that otherwise would fall on Earth back into space. This small increase in reflected sunlight would be sufficient to mitigate much of the impact of human-induced warming. For example, in 1991, Mount Pinatubo ejected almost 20 tons of aerosols (sulfur dioxide) into the atmosphere and cooled down the planet by around 0.5 degrees Celsius over the following year. We should be able to induce cooling equivalent to, say, a fraction of Mount Pinatubo, via a fair, international process that‚Äôs backed by science.\\nThere are many criticisms of SAI, such as:\\n- It could have unintended climate consequences, for example, disrupting\\xa0local weather patterns and creating droughts or floods.\\n- If it were started and then stopped suddenly, it could lead to sudden warming, known as ‚Äútermination shock.‚Äù\",\n",
       "  '- If it were started and then stopped suddenly, it could lead to sudden warming, known as ‚Äútermination shock.‚Äù\\n- Depending on the aerosol used (sulfur dioxide is a leading candidate), it could contribute to pollution and/or ozone depletion.\\n- It might reduce urgency to decarbonize (an example of a ‚Äúmoral hazard‚Äù).\\nIn addition, many people have a visceral emotional reaction, as I once did before I understood the science more deeply, against ‚Äúplaying god‚Äù by daring to engineer the planet.\\nAll these downsides should be balanced against the reality that people are dying.\\nI‚Äôm moved by meteorologist John Morales‚Äô emotional account of the havoc caused by Hurricane Milton. The New York Times quoted him as saying, ‚ÄúIt claims lives. It also wrecks lives.‚Äù\\nSkyfire AI, a drone company led by CEO Don Mathis that my team AI Fund helped to co-build, was recently on the ground in the aftermath of Helene and Milton, deploying drones to help emergency responders survey remote areas and find survivors. Mathis reports that Skyfire was credited with saving at least 13 lives. On Monday, I also spoke about AI applied to renewable energy with AES‚Äô CEO Andres Gluski and CPO Chris Shelton. You can view our conversation here .\\nWhile I‚Äôm glad that AI can help mitigate these disasters, it saddens me that so many lives have already been lost due to climate-influenced causes. My mind frequently returns to SAI as one of the few untapped tools in our arsenal that can help. We need to be investing in SAI research now.',\n",
       "  'I‚Äôm grateful to my collaborators on the Planet Parasol emulator (a group that includes many climate scientists) including Jeremy Irvin, Daniele Visioni, Ben Kravitz, Dakota Gruener, Chris Smith, and Duncan Watson-Parris. MIT Technology Review ‚Äôs James Temple wrote about his experience playing with our emulator and also outlines fair criticisms. Much work remains to be done, and making sure our actions are based on science ‚Äî a task that AI can help with (witness the recent Chemistry and Physics Nobel Prizes going to innovators in AI!) ‚Äì will help us make better decisions.\\nIf you‚Äôre interested in learning more about SAI, check out this recent panel discussion where I spoke alongside climate scientists Chris Field, David Keith, Douglas MacMartin, and Simone Tilmes about the science and possible roadmaps ahead.\\nKeep learning!\\nAndrew\\nIn this course, you‚Äôll learn to build scalable agents without managing infrastructure. Explore agentic workflows, tool integration, and setting up guardrails for secure and responsible operations. Sign up today',\n",
       "  '## Malaysia‚Äôs Data Center Boom\\nMalaysia‚Äôs location, natural resources, and investor-friendly government are perfect for data centers, turning part of the country into an AI-fueled boomtown.\\nWhat‚Äôs new: Data center construction is flourishing in the southern Malaysian state of Johor, where companies including ByteDance and Microsoft are spending billions of dollars on facilities, The Wall Street Journal reported . These data centers will provide processing power for AI, cloud computing, and telecommunications.\\nHow it works: Data center construction has slowed in established areas like Ireland and Northern Virginia as space and resources have become scarce. All regions face shortages of electrical power, analysts say , and some U.S. locations face public resistance to new projects. Johor has emerged as an attractive alternative.\\n- Johor has space, energy (mostly coal), water for cooling, and proximity to Singapore, a global communications hub that lacks the land and power to host many new data centers. The Malaysian government and local politicians streamlined the permitting process and advocated for additional infrastructure, such as water desalination plants, to support such projects. Moreover, Malaysia‚Äôs strong relationships with both the U.S. and China reduce political risks for companies that operate in the region.\\n- Data center investments in Johor will reach $3.8 billion this year, according to regional bank Maybank. ByteDance allocated $350 million for data center construction in the region. Microsoft purchased land nearby for $95 million and announced a plan to spend $2.2 billion. Oracle expects to invest $6.5 billion in Malaysia.\\n- While some tech giants are building their own data centers, independent operators are building facilities to serve companies like Amazon, Alphabet, and Meta.',\n",
       "  '- While some tech giants are building their own data centers, independent operators are building facilities to serve companies like Amazon, Alphabet, and Meta.\\nBehind the news: The Asia-Pacific region is second to North America in data center construction, according to one recent report , ahead of Europe, South America, and the Middle East and Africa. As Johor builds out its data-center inventory, it will compete with established Asia-Pacific markets in Hong Kong, Mumbai, Seoul, Singapore, Sydney, and Tokyo.\\nWhy it matters: AI is poised to transform virtually every industry, but doing so requires ample processing power. The data-center buildout will help fuel improvements in AI as well as spread the technology to new industries and bring its benefits to people throughout the world. Malaysia‚Äôs role as a data center hub is also bound to bring huge economic benefits to the country itself.\\nWe‚Äôre thinking: Many data centers have been built near users to reduce latency. But the cost of processing compute-intensive AI workloads is so high relative to the cost of transmitting data that it makes sense to transmit AI-related data long distances for processing. (As Andrew wrote, the gravity of data is decreasing .) We hope the increasing flexibility in siting data centers will enable more nations that aren‚Äôt traditional tech hubs to participate in the tech economy and reap significant benefits from doing so.',\n",
       "  '## U.S. Cracks Down on AI Apps That Overpromise, Underdeliver\\nThe United States government launched Operation AI Comply, targeting businesses whose uses of AI allegedly misled customers.\\nWhat‚Äôs new: The Federal Trade Commission (FTC) took action against five businesses for allegedly using or selling AI technology in deceptive ways. Two companies settled with the agency, while three face ongoing lawsuits.\\nHow it works: The FTC filed complaints against the companies based on existing laws and rules against unfair or deceptive commercial practices. The FTC alleges:\\n- DoNotPay claimed its AI service was a ‚Äúrobot lawyer‚Äù that could substitute for human legal expertise. The FTC said the company misled consumers about its system‚Äôs ability to handle legal matters and provide successful outcomes. DoNotPay settled the case, paying $193,000 in consumer redress and notifying customers about the limitations of its services.\\n- Rytr, a writing tool, generated fake reviews of companies. According to the FTC, Rytr offered to create and post fake reviews on major platforms like Google and Trustpilot, which helped it to bring in $3.8 million in revenue from June 2022 to May 2023. Rytr agreed to settle and is barred from offering services that generate consumer reviews or testimonials. The settlement amount was not disclosed.\\n- Ascend Ecommerce claimed that its ‚Äúcutting-edge‚Äù AI-powered tools would help consumers quickly earn thousands of dollars monthly through online storefronts. The company allegedly charged thousands of dollars for its services, but the promised returns failed to materialize, defrauding customers of at least $25 million. The government temporarily halted the company‚Äôs operations and froze its assets.',\n",
       "  '- Ecommerce Empire Builders promised to help consumers build an ‚ÄúAI-powered Ecommerce Empire‚Äù through training programs that cost customers nearly $2,000 each, or readymade online storefronts that cost tens of thousands of dollars. A federal court temporarily halted the scheme.\\n- FBA Machine said its AI-powered tools could automate the building and management of online stores on platforms like Amazon and Walmart. The company promoted its software with guarantees that customers‚Äô monthly earnings would exceed $100,000. Consumers paid nearly $16 million but didn‚Äôt earn the promised profits. A federal court temporarily halted FBA‚Äôs operations.\\nBehind the news: The FTC has a broad mandate to protect consumers, including both deceptive and anticompetitive business practices. In June, it agreed to focus on Microsoft‚Äôs investment in OpenAI and Google‚Äôs and Amazon‚Äôs investments in Anthropic, while the U.S. Department of Justice would examine Nvidia‚Äôs dominant market share in chips designed to process AI workloads. The FTC previously brought cases against Rite Aid for misuse of AI-enabled facial recognition, Everalbum for deceptive use of facial recognition, and CRI Genetics , which misled consumers while using AI to conduct DNA tests.\\nWhy it matters: The FTC‚Äôs enforcement actions send a message to businesses that aim to take advantage of the latest AI models: making exaggerated claims about AI will bring legal consequences. The complaints point to a set of issues: falsely claiming to use AI to provide a particular service, exaggerating AI‚Äôs ability to replace human expertise, generating fake reviews of businesses, promising unrealistic financial returns, and failing to disclose crucial information about AI-based services.\\nWe‚Äôre thinking: These particular actions crack down not on AI per se but on companies that allegedly deceived consumers. By taking scams off the market while leaving legitimate businesses to operate freely, they may actually increase customer trust in AI.',\n",
       "  '## A Year of Contending Forces\\nA new report documents the interplay of powerful forces that drove AI over the past year: open versus proprietary technology, public versus private financing, innovation versus caution.\\nWhat‚Äôs new: Drawn from research papers, news articles, earnings reports, and the like, the seventh annual State of AI Report recaps the highlights of 2024.\\nLooking back: AI‚Äôs rapid advance in 2024 was marked by groundbreaking research, a surge of investment, international regulations, and a shift in safety concerns from hypothetical risks to real-world issues, according to investors Nathan Benaich and Ian Hogarth.\\n- Top models: Anthropic‚Äôs Claude, Google‚Äôs Gemini, and Meta‚Äôs Llama largely closed the gap with OpenAI‚Äôs top multimodal model, GPT-4o, before its successor o1 raised the bar for reasoning. Meanwhile, models built in China such as DeepSeek, Qwen, and Kling challenged the top models despite the United States‚Äô restrictions on exports of the most powerful AI chips. The year saw a proliferation of models small enough to run on local devices, such as Gemini Nano (3.25 billion parameters) and the smaller of Apple‚Äôs AFM family (3 billion parameters).\\n- Research: Model builders settled on mixtures of curated natural and synthetic data for training larger models (Microsoft‚Äôs Phi family, Anthropic Claude 3.5 Sonnet, Meta Llama 3.1) and knowledge distillation for training smaller ones (Flux.1, Gemini 1.5 Flash, Mistral-NeMo-Minitron, and numerous others). Meanwhile, researchers established benchmarks to measure new capabilities like video understanding and agentic problem-solving. Another motivation for new benchmarks is to replace older tests in which new models consistently achieve high scores, possibly because the test data had contaminated their training data.',\n",
       "  '- Finance: Investment boomed. The chip designer Nvidia contributed nearly one-third of the AI industry‚Äôs $9 trillion total value, including public and private companies, and the combined value of public AI companies alone exceeded the entire industry‚Äôs value last year. The most dramatic single trend in AI finance was the shift by major public companies from acquisitions to acquisition-like transactions, in which tech giants took on talent from top startups, sometimes in exchange for licensing fees, without buying them outright: notably Amazon-Covariant, Google-Character.AI, and Microsoft-Inflection. In venture investment, robotics now accounts for nearly 30 percent of all funding. Standouts included the humanoid startup Figure with a $675 million round at a $2.6 billion valuation and its competitor 1X with a $125 million round.\\n- Regulation: Regulation of AI remains fragmented globally. The U.S. issued executive orders that mainly relied on new interpretations or implementations of existing laws. Europe‚Äôs AI Act sought to balance innovation and caution by declaring that large models pose a special risk and banning applications such as predictive policing, but some observers have deemed it heavy-handed. China focused on enforcement of its more restrictive laws, requiring companies to submit models for government review. Widespread fears that AI would disrupt 2024‚Äôs many democratic elections proved unfounded.\\n- Safety: While anxieties in 2023 focused on abstract threats such as the risk that AI would take over the world, practical concerns came to the fore. Model makers worked to increase transparency, interpretability, and security against external attacks. Actual security incidents occurred on a more personal scale: Bad actors used widely available tools to harass and impersonate private citizens, notably generating fake pornographic images of them, which remains an unsolved problem.',\n",
       "  'Looking forward: The authors reviewed predictions they made in last year‚Äôs report ‚Äî among them, regulators would investigate the Microsoft/OpenAI Partnership (accurate), and a model builder would spend over $1 billion on training (not yet) ‚Äî and forecast key developments in 2025:\\n- An open source model will outperform OpenAI‚Äôs proprietary o1 on reasoning benchmarks.\\n- European lawmakers, fearing that the AI Act overreaches, will refrain from strict enforcement.\\n- Generative AI will hit big. A viral app or website built by a noncoder or a video game with interactive generative AI elements will achieve breakout success. An AI-generated research paper will be accepted at a major machine learning conference.\\nWhy it matters: The authors examined AI from the point of view of investors, keen to spot shifts and trends that will play out in significant ways. Their report dives deep into the year‚Äôs research findings as well as business deals and political currents, making for a well rounded snapshot of AI at the dawn of a new year.\\nWe‚Äôre thinking: The authors are bold enough to make clear predictions and self-critical enough to evaluate their own accuracy one year later. We appreciate their principled approach!',\n",
       "  '## Better Text Embeddings\\nText embedding models are often used to retrieve text, cluster text, determine similarity between texts, and generate initial embeddings for text classifiers. A new embedding model comes with adapters that specialize it to each of these use cases.\\nWhat‚Äôs new: Saba Sturua and colleagues at Jina AI released jina-embeddings-v3 , a text-embedding system with open weights that can process 8,192 input tokens and output embeddings of 1,024 values. It‚Äôs free for noncommercial use and competes with closed weight models from Cohere and OpenAI.\\nHow it works: Jina-embeddings-v3 comprises a transformer (559 million parameters) and five LoRA adapters that plug into the model and adjust its weights for retrieval, clustering, determining similarity, and classification. Two adapters adjust the model for retrieval: one for documents and one for queries.\\n- The authors started with a pretrained XLM-RoBERTa . They further pretrained it to predict masked words in data from text in 89 languages .\\n- They add a mean pooling layer to average output vectors into one embedding. They fine-tuned the model, using an unspecified dataset of 1 billion text pairs in various languages, to produce similar embeddings for matching text pairs and dissimilar embeddings for non-matching text pairs.',\n",
       "  '- They fine-tuned the five adapters on the four tasks. For retrieval, they trained the two adapters to produce similar embeddings of matching queries and documents and dissimilar embeddings for queries and documents that didn‚Äôt match. For clustering, the authors fine-tuned the adapter to produce more-similar embeddings of examples from the same class and less-similar embeddings of examples from different classes. Text similarity worked in a related manner: they fine-tuned the adapter to produce more-similar embeddings of similar examples than dissimilar examples. For classification, they fine-tuned the adapter to produce similar embeddings of examples of the same class and different embedding of different classes.\\n- They modified the loss function during training using matryoshka representation learning . This method encourages the loss function to solve the problem at hand using the first 32, 64, 128, 256, 512, and 768 values of the embedding as effectively as it would if it used all 1,024 values.\\nResults: The authors compared jina-embeddings-v3 to Cohere‚Äôs multilingual embed v3 , OpenAI‚Äôs text-embedding-3-large , and Microsoft‚Äôs open-weights Multilingual-E5-large-instruct . They tested their system on the Massive Text Embedding Benchmark (MTEB) for embedding tasks.\\n- On English-language tasks, Jina-embeddings-v3 achieved an average score of 65.52 percent, while OpenAI achieved 64.6 percent, Microsoft 64.41 percent, and Cohere 64.01 percent. For example, when they trained logistic classifiers on embeddings produced by the various models, jina-embeddings-v3 performed best as classification, achieving an average accuracy of 82.58 percent, while OpenAI achieved 75.45 percent, Microsoft 77.56 percent, and Cohere 76.01 percent.*',\n",
       "  '- The team also tested how well smaller versions of the embedding performed on retrieval. Medium sizes reduced performance only slightly. For instance, using all 1,024 values for retrieval, the model achieved 63.35 percent normalized discounted cumulative gain (nDCG), a measure of how well the model ranks the retrieved documents (higher is better). When it used the first 32 values, the model achieved 52.54 percent nDCG; and when it used 128 values, it achieved 61.64 percent nDCG.\\nWhy it matters: Training a set of LoRA adapters is becoming the go-to method for adapting a pretrained model for a variety of tasks. Jina extends the list to computing embeddings for different language tasks and gives developers a further option for generating high-quality embeddings.\\nWe‚Äôre thinking: The authors‚Äô results show that using embeddings that are one-eighth the typical size degrades performance by only 2 percent. That tradeoff may be worthwhile if your computational budget is constrained or your task is especially data-intensive.',\n",
       "  \"Why it matters : Previous work showed that LLMs-based services could generate misinformation and other malicious output, but little research has probed their actual use in cybercrime. This work evaluates their quality and effectiveness. In addition, the authors released the prompts they used to circumvent guardrails and generate malicious output ‚Äî a resource for further research that aims to fix such issues in future models.\\nWe‚Äôre thinking: It‚Äôs encouraging to see that harmful services didn‚Äôt get far in real-world tests, and the authors' findings should put a damper on alarmist scenarios of AI-enabled cybercrime. That doesn‚Äôt mean we don‚Äôt need to worry about harmful applications of AI technology. The AI community has a responsibility to design its products to be beneficial and evaluate them thoroughly for safety.\",\n",
       "  '## Introduction\\nDear friends,\\nHappy Thanksgiving! In the United States, this is a week when many reflect on their blessings and give thanks. Even as I reflect on how lucky I am to have food, shelter, family, and friends, I think about those who have much less and what we can do to help them.\\nLast week, I spoke with a woman who had been severely physically abused by her husband. She showed me pictures of her face from a few years ago, which had a bloodied sequence of tears down the middle. She also showed me scars left by cigarette burns inflicted by her husband, who told her these burns made her ugly so no other man would ever want her. She is no longer with her husband but continues to struggle. Her phone is badly cracked and barely holds a charge. Without a high-school degree, she has struggled to find a job and is surviving by staying on the couch of a friend. As winter approaches, they keep their place chilly to save the cost of electricity.\\nWorking in AI, I am fortunate to interact with many of the smartest and most capable technology and business leaders in the world. But both at home and when I travel, I try to meet with people of a broad range of backgrounds, because ultimately I want to do work that helps people broadly, and this requires that I understand people broadly. When you go to a grocery store and see someone put down a $5 carton of eggs because it is too expensive, and hear them think through how to explain to their kids why they‚Äôre skipping eggs that week, it gives you a deeper appreciation for why a $1.50/hour raise can be life-changing for many people.\\nWhile I can try to help out individuals here and there, technology is advancing rapidly, and this gives me a lot of optimism for the future. Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving.',\n",
       "  'I am optimistic about the future because I see so many ways life can be so much better for so many people. I feel blessed that, when my kids or I are cold, we have warm clothing, and when we are hungry, we have a working car to drive to the grocery and buy fresh food. I feel blessed that, rather than using a badly cracked cellphone, I have a modern laptop and a fast internet connection to do my work on.\\nAs a child, my father taught me the aphorism ‚Äú there but for the grace of God go I ‚Äù to recognize that, in even slightly different circumstances, I might have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life, and I hope many others in AI will do so, too.\\nI see so many people in the AI community building things to make the world better. I am thankful for what the AI community has already done, and I look forward to continuing to build and serve others together.\\nKeep building!\\nAndrew\\nGet started coding in Python with AI Python for Beginners , a four-part course led by Andrew Ng. Build projects from the very first lesson with real-time support from an AI assistant. Complete the course and bring your ideas to life! Start today',\n",
       "  \"## Reasoning Revealed\\nAn up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. Unlike o1, it displays its reasoning steps.\\nWhat‚Äôs new: DeepSeek announced DeepSeek-R1, a model family that processes prompts by breaking them down into steps. A free preview version is available on the web, limited to 50 messages daily; API pricing is not yet announced. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. DeepSeek said it would release R1 as open source but didn't announce licensing terms or a release date.\\nHow it works: DeepSeek-R1-lite-preview uses a smaller base model than DeepSeek 2.5, which comprises 236 billion parameters. Like o1-preview, most of its performance gains come from an approach known as test-time compute , which trains an LLM to think at length in response to prompts, using more compute to generate deeper answers. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview‚Äôs reasoning steps are visible. This makes the model more transparent, but it may also make it more vulnerable to jailbreaks and other manipulation.\\n- According to DeepSeek, R1-lite-preview, using an unspecified number of reasoning tokens, outperforms OpenAI o1-preview, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, Alibaba Qwen 2.5 72B, and DeepSeek-V2.5 on three out of six reasoning-intensive benchmarks.\\n- It substantially outperforms o1-preview on AIME (advanced high school math problems, 52.5 percent accuracy versus 44.6 percent accuracy), MATH (high school competition-level math, 91.6 percent accuracy versus 85.5 percent accuracy), and Codeforces (competitive programming challenges, 1,450 versus 1,428). It falls behind o1 on GPQA Diamond (graduate-level science problems), LiveCodeBench (real-world coding tasks), and ZebraLogic (logical reasoning problems).\",\n",
       "  '- DeepSeek reports that the model‚Äôs accuracy improves dramatically when it uses more tokens at inference to reason about a prompt (though the web user interface doesn‚Äôt allow users to control this). On AIME math problems, performance rises from 21 percent accuracy when it uses less than 1,000 tokens to 66.7 percent accuracy when it uses more than 100,000, surpassing o1-preview‚Äôs performance. The additional performance comes at the cost of slower and more expensive output.\\nBehind the news: DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are being questioned .\\nWhy it matters: DeepSeek is challenging OpenAI with a competitive large language model. It‚Äôs part of an important movement, after years of scaling models by raising parameter counts and amassing larger datasets, toward achieving high performance by spending more energy on generating output.\\nWe‚Äôre thinking: Models that do and don‚Äôt take advantage of additional test-time compute are complementary. Those that do increase test-time compute perform well on math and science problems, but they‚Äôre slow and costly. Those that don‚Äôt use additional test-time compute do well on language tasks at higher speed and lower cost. Applications that require facility in both math and language may benefit by switching between the two.',\n",
       "  '## Household Help\\nA new generation of robots can handle some household chores with unusual skill.\\nWhat‚Äôs new: Physical Intelligence, a startup based in San Francisco, unveiled œÄ0 (pronounced ‚Äúpi-zero‚Äù), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company also announced $400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms.\\nHow it works: œÄ0 is a version of the pretrained PaliGemma vision-language model that has been modified for flow matching . (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action.\\n- PaliGemma comprises SigLIP , a vision transformer that turns images into embeddings; a linear layer that adapts the image embeddings to serve as input for the pretrained large language model Gemma; and Gemma , which estimates the noise to be removed from a robot action embedding to which noise has been added.\\n- The authors modified PaliGemma as follows: (i) They adapted it to accept embeddings that represent the robots‚Äô state and previous actions, and to generate embeddings that represent the noise to be removed from noisy robot actions. (ii) They added a vanilla neural network to the input to turn the current timestep into an embedding. (iii) They modified\\xa0Gemma to be a mixture-of-experts model:\\xa0One expert, or subset of weights, is the pretrained weights, which process image and text embeddings. The other is a new set of weights that process robot action embeddings.',\n",
       "  '- They pretrained œÄ0 to remove noise from action embeddings. (Since œÄ0 produces embeddings of the noise to be removed, removing that noise is as simple as adding the two embeddings.)\\n- Training data included the Open X-Embodiment Dataset and a proprietary dataset of 10,000 hours of robotic states (for instance, current positions of a robot‚Äôs joints), actions (for instance, motions of the robot‚Äôs joints), and an associated language command. The proprietary dataset included data collected from seven different robots (such as a single stationary robot arm to two robot arms mounted on a mobile base) and 68 tasks (for example, folding laundry, making coffee, or bussing a table).\\n- After pretraining, the authors fine-tuned œÄ0 to remove noise from action tokens in 15 further tasks, some of which were not represented in the pretraining set. These tasks improved the model‚Äôs ability to follow more detailed instructions and perform multi-stage tasks such as packing food into a to-go box.\\n- At inference, given the robot‚Äôs camera view of the surrounding scene, SigLip embeds the images. A linear layer projects the resulting embeddings to fit Gemma‚Äôs expected input size and data distribution. Given the images, text command, robot‚Äôs state, current timestep, and 50 noisy action tokens (starting with pure noise), Gemma iteratively removes noise. To complete longer tasks, the process repeats: The robot takes more images of the surrounding scene and retrieves the robot‚Äôs state, which œÄ0 uses to generate further actions.',\n",
       "  \"Results: œÄ0 outperformed the open robotics models OpenVLA , Octo , ACT , and Diffusion Policy , all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot‚Äôs success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, œÄ0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, œÄ0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average.\\nYes, but: The robot occasionally makes mistakes . In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items.\\nBehind the news: Commercial robotics appears to be undergoing a renaissance. Skild raised $300 million to develop a ‚Äúgeneral-purpose brain for robots.‚Äù Figure AI secured $675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics, licensed its technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAI renewed its robotics effort after dismantling its robotics department in 2020.\\nWhy it matters: Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but œÄ0 shows that they can perform tasks that people need done.\",\n",
       "  'We‚Äôre thinking: One of the team members compared œÄ0 to GPT-1 for robotics ‚Äî an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.',\n",
       "  '## AI Power Couple Recommits\\nAmazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services‚Äô AI infrastructure and lengthening the high-flying startup‚Äôs runway.\\nWhat‚Äôs new: Amazon, already a significant investor in Anthropic, put another $4 billion into the AI company. In exchange, Anthropic will train and run its AI models on Amazon‚Äôs custom-designed chips. (Disclosure: Andrew Ng serves on Amazon‚Äôs board of directors.)\\nHow it works: The new round brings Amazon‚Äôs investment in Anthropic to $8 billion (though it remains a minority stake without a seat on the startup‚Äôs board). The deal extended the partnership in several ways:\\n- AWS becomes Anthropic‚Äôs primary partner for training AI models. Anthropic will train its models using Amazon‚Äôs Trainium chips, which are designed for training neural networks of 100 billion parameters and up. Amazon executives previously claimed that these chips could cut training costs by as much as 50 percent compared to Nvidia graphics processing units (GPUs).\\n- Previously Anthropic ran its Claude models on Nvidia hardware; going forward, Anthropic will run them on Amazon‚Äôs Inferentia chips, according to The Information . Customers of Amazon Web Services will be able to fine-tune Claude on Bedrock, Amazon Web Services‚Äô AI model platform.\\n- Anthropic will contribute to developing Amazon‚Äôs Neuron toolkit, software that accelerates deep learning workloads on Trainium and Inferentia chips.\\nBehind the news: In November, Anthropic agreed to use Google‚Äôs cloud-computing infrastructure in return for a $2 billion investment. The previous month, Amazon had committed to invest as much as $4 billion in Anthropic, and Anthropic had made Amazon Web Services the primary provider of its models.',\n",
       "  \"Yes, but: The UK‚Äôs Competition and Markets Authority recently cleared both Amazon‚Äôs and Google‚Äôs investments in Anthropic, but regulators continue to monitor such arrangements for violations of antitrust laws. Microsoft and OpenAI face a similar investigation by the European Commission and U.S. Federal Trade Commission.\\nWhy it matters: The speed and skill required to build state-of-the-art AI models is driving tech giants to collaborate with startups, while the high cost is driving startups to partner with tech giants. If the partnership between Amazon and Anthropic lives up to its promise, Claude users and developers could see gains in performance and efficiency. This could validate Amazon's hardware as a competitor with Nvidia and strengthen Amazon Web Services‚Äô position in the cloud market. On the other hand, if Claude faces any challenges in scaling while using Trainium and Inferentia, that could affect both companies' ambitions.\\nWe‚Äôre thinking: Does the agreement between Amazon and Anthropic give the tech giant special access to the startup‚Äôs models for distillation, research, or integration, as the partnership between Microsoft and OpenAI does? The companies‚Äô announcements don‚Äôt say.\",\n",
       "  '## Object Detection for Small Devices\\nAn open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells.\\nWhat‚Äôs new: Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced Grounding DINO 1.5 , a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights here .\\nKey insight: The original Grounding DINO follows many of its predecessors by using image embeddings of different levels (from lower-level embeddings produced by an image encoder‚Äôs earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to better detect objects at different scales . However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process.\\nHow it works: Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples.\\n- Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.\\n- Given the corresponding text, BERT produced a text embedding composed of tokens.',\n",
       "  '- Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.\\n- Given the corresponding text, BERT produced a text embedding composed of tokens.\\n- Given the highest-level image embedding and the text embedding, a cross-attention model updated each one to incorporate information from the other (fusing text and image modalities, in effect). After the update, a CNN-based model combined the updated highest-level image embedding with the lower-level image embeddings to create a single image embedding.\\n- Grounding DINO 1.5 calculated which 900 tokens in the image embedding were most similar to the tokens in the text embedding.\\n- A cross-attention model detected objects using both the image and text embeddings. For each token in the updated image embedding, it determined: (i) which text token(s), if any, matched the image token, thereby giving each image token a classification including ‚Äúnot an object‚Äù and (ii) a bounding box that enclosed the corresponding object (except for tokens that were labeled ‚Äúnot an object‚Äù).\\n- The system learned to (i) maximize the similarity between matching tokens from the text and image embeddings and minimize the similarity between tokens that didn‚Äôt match and (ii) minimize the difference between its own bounding boxes and those in the training dataset.\\nResults: Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an Nvidia Jetson Orin NX computer. Tested on a dataset of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and YOLO-Worldv2-L (a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent.',\n",
       "  'Why it matters: The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results.\\nWe‚Äôre thinking: Lately model builders have been building better, smaller, faster large language models for edge devices. We‚Äôre glad to see object detection get similar treatment.',\n",
       "  '## Introduction\\nDear friends,\\nAI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications. This is making it possible to build new kinds of things, which in turn is driving shifts in best practices in product management ‚Äî the discipline of defining what to build to serve users ‚Äî because what is possible to build has shifted. In this letter, I‚Äôll share some best practices I have noticed.\\nUse concrete examples to specify AI products. Starting with a concrete idea helps teams gain speed. If a product manager (PM) proposes to build ‚Äúa chatbot to answer banking inquiries that relate to user accounts,‚Äù this is a vague specification that leaves much to the imagination. For instance, should the chatbot answer questions only about account balances or also about interest rates, processes for initiating a wire transfer, and so on? But if the PM writes out a number (say, between 10 and 50) of concrete examples of conversations they‚Äôd like a chatbot to execute, the scope of their proposal becomes much clearer. Just as a machine learning algorithm needs training examples to learn from, an AI product development team needs concrete examples of what we want an AI system to do. In other words, the data is your PRD (product requirements document)!',\n",
       "  'In a similar vein, if someone requests ‚Äúa vision system to detect pedestrians outside our store,‚Äù it‚Äôs hard for a developer to understand the boundary conditions. Is the system expected to work at night? What is the range of permissible camera angles? Is it expected to detect pedestrians who appear in the image even though they‚Äôre 100m away? But if the PM collects a handful of pictures and annotates them with the desired output, the meaning of ‚Äúdetect pedestrians‚Äù becomes concrete. An engineer can assess if the specification is technically feasible and if so, build toward it. Initially, the data might be obtained via a one-off, scrappy process, such as the PM walking around taking pictures and annotating them. Eventually, the data mix will shift to real-word data collected by a system running in production.\\nUsing examples (such as inputs and desired outputs) to specify a product has been helpful for many years, but the explosion of possible AI applications is creating a need for more product managers to learn this practice.\\nAssess technical feasibility of LLM-based applications by prompting. When a PM scopes out a potential AI application, whether the application can actually be built ‚Äî that is, its technical feasibility ‚Äî is a key criterion in deciding what to do next. For many ideas for LLM-based applications, it‚Äôs increasingly possible for a PM, who might not be a software engineer, to try prompting ‚Äî or write just small amounts of code ‚Äî to get an initial sense of feasibility.',\n",
       "  'For example, a PM may envision a new internal tool for routing emails from customers to the right department (such as customer service, sales, etc.). They can prompt an LLM to see if they can get it to select the right department based on an input email, and see if they can achieve high accuracy. If so, this gives engineering a great starting point from which to implement the tool. If not, the PM can falsify the idea themselves and perhaps improve the product idea much faster than if they had to rely on an engineer to build a prototype.\\nOften, testing feasibility requires a little more than prompting. For example, perhaps the LLM-based email system needs basic RAG capability to help it make decisions. Fortunately, the barrier to writing small amounts of code is now quite low, since AI can help by acting as a coding companion, as I describe in the course, ‚Äú AI Python for Beginners .‚Äù This means that PMs can do much more technical feasibility testing, at least at a basic level, than was possible before.\\nPrototype and test without engineers. User feedback to initial prototypes is also instrumental to shaping products. Fortunately, barriers to building prototypes rapidly are falling, and PMs themselves can move prototypes forward without needing software developers.\\nIn addition to using LLMs to help write code for prototyping, tools like Replit, Vercel‚Äôs V0, Bolt, and Anthropic‚Äôs Artifacts (I‚Äôm a fan of all of these!) are making it easier for people without a coding background to build and experiment with simple prototypes. These tools are increasingly accessible to non-technical users, though I find that those who understand basic coding are able to use them much more effectively, so it‚Äôs still important to learn basic coding. (Interestingly, highly technical, experienced developers use them too!) Many members of my teams routinely use such tools to prototype, get user feedback, and iterate quickly.',\n",
       "  'AI is enabling a lot of new applications to be built, creating massive growth in demand for AI product managers who know how to scope out and help drive progress in building these products. AI product management existed before the rise of generative AI, but the increasing ease of building applications is creating greater demand for AI applications, and thus a lot of PMs are learning AI and these emerging best practices for building AI products. I find this discipline fascinating, and will keep on sharing best practices as they grow and evolve.\\nKeep learning!\\nAndrew\\nWrite and code more effectively with OpenAI Canvas, a user-friendly workspace for collaborating with AI. In this free course, explore use cases like building game apps and designing SQL databases from screenshots, and gain insights into how GPT-4o powers Canvas‚Äô features. Join for free',\n",
       "  '## Competitive Performance, Competitive Prices\\nAmazon introduced a range of models that confront competitors head-on.\\nWhat‚Äôs new: The Nova line from Amazon includes three vision-language models (Nova Premier, Nova Pro, and Nova Lite), one language model (Nova Micro), an image generator (Nova Canvas), and a video generator (Nova Reel). All but Nova Premier are available on Amazon‚Äôs Bedrock platform, and Nova Premier, which is the most capable, is expected in early 2025. In addition, Amazon plans to release a speech-to-speech model in early 2025 and a multimodal model that processes text, images, video, and audio by mid-year. (Disclosure: Andrew Ng serves on Amazon‚Äôs board of directors.)\\nHow it works: Nova models deliver competitive performance at relatively low prices. Amazon hasn‚Äôt disclosed parameter counts or details about how the models were built except to say that Nova Pro, Lite, and Micro were trained on a combination of proprietary, licensed, public, and open-source text, images, and video in over 200 languages.\\n- Nova Pro is roughly comparable to that of Anthropic Claude 3.5 Sonnet, OpenAI GPT-4o, and Google Gemini Pro. It has a 300,000-token input context window, enabling it to process relatively large vision-language inputs. Nova Pro outperforms its primary competitors in tests of following complex instructions ( IFEval), summarizing long texts ( SQuALITY ), understanding videos ( LVBench ), and reading and acting on websites ( MM-Mind2Web ). It processes 95 tokens per second. At $0.80/$3.20 per million tokens of input/output, it‚Äôs significantly less expensive than GPT-4o ($2.50/$10) and Claude 3.5 Sonnet ($3/$15) but slower than GPT-4o (115 tokens per second).',\n",
       "  '- Nova Lite compares favorably with Anthropic Claude Haiku, Google Gemini 1.5 Flash, and OpenAI GPT-4o Mini. Optimized for processing speed and efficiency, it too has a 300,000 token input context window. Nova Lite bests Claude 3.5 Sonnet and GPT-4o on VisualWebBench, which tests visual understanding of web pages. It also beats Claude 3.5 Haiku, GPT-4o Mini, and Gemini 1.5 Flash in multimodal agentic tasks that include MM-Mind2Web and the Berkeley Function-Calling Leaderboard . It processes 157 tokens per second and costs $0.06/$0.24 per million tokens of input/output, making it less expensive than GPT-4o mini ($0.15/$0.60), Claude 3.5 Haiku ($0.80/$4), or Gemini 1.5 Flash ($0.075/$0.30), but slower than Gemini 1.5 Flash (189 tokens per second).\\n- Nova Micro is a text-only model with a 128,000-token context window. It exceeds Llama 3.1 8B and Gemini Flash 8B on all 12 tests reported by Amazon, including generating code ( HumanEval ) and reading financial documents ( FinQA ). It also beats the smaller Claude, Gemini, and Llama models on retrieval-augmented generation tasks ( CRAG ). It processes 210 tokens per second (the lowest latency among Nova models) and costs $0.035/$0.14 per million input/output tokens. That‚Äôs cheaper than Gemini Flash 8B ($0.0375/$0.15) and Llama 3.1 8B ($0.10/$0.10), but slower than Gemini Flash 8B (284.2 tokens per second).',\n",
       "  '- Nova Canvas accepts English-language text prompts up to 1,024 characters and produces images up to 4.2 megapixels in any aspect ratio. It also performs inpainting, outpainting, and background removal. It excels on ImageReward , a measure of human preference for generated images, surpassing OpenAI DALL¬∑E 3 and Stability AI Stable Diffusion 3.5. Nova Canvas costs between $0.04 per image up to 1024x1024 pixels and $0.08 per image up to 2,048x2,048 pixels. Prices are hard to compare because many competitors charge by the month or year, but this is less expensive and higher-resolution than DALL¬∑E 3 ($0.04 to $0.12 per image).\\n- Nova Reel accepts English-language prompts up to 512 characters and image prompts up to 720x1,280 pixels. It generates video clips of 720x1280 pixels up to six seconds long. It demonstrates superior ability to maintain consistent imagery from frame to frame, winning 67 percent of head-to-head comparisons with the next highest-scoring model, Runway Gen-3 Alpha. Nova Reel costs $0.08 per second of output, which is less expensive than Runway Gen-3 Alpha ($0.096 per second) and Kling 1.5 ($0.12 per second) in their standard monthly plans.\\nBehind the news: The company launched Bedrock in April 2023 with Stability AI‚Äôs Stable Diffusion for image generation, Anthropic‚Äôs Claude and AI21‚Äôs Jurassic-2 for text generation, and its own Titan models for text generation and embeddings. Not long afterward, it added language models from Cohere as well as services for agentic applications and medical applications. It plans to continue to provide models from other companies (including Anthropic), offering a range of choices.',\n",
       "  'Why it matters: While other AI giants raced to outdo one another in models for text and multimodal processing, Amazon was relatively quiet. With Nova, it has staked out a strong position in those areas, as well as the startup-dominated domains of image and video generation. Moreover, it‚Äôs strengthening its cloud AI offerings with competitive performance, pricing, and speed. Nova‚Äôs pricing continues the rapid drop in AI prices over the last year. Falling per-token prices help make AI agents or applications that process large inputs more practical. For example, Simon Willison, developer of the Django Python framework for web applications, found that Nova Lite generated descriptions for his photo library (tens of thousands of images) for less than $10.\\nWe‚Äôre thinking: The Nova suite is available via APIs as well as two web playgrounds (one in the Bedrock console, the other a new interface for building AI apps called PartyRock ). This accords with Amazon Web Services‚Äô focus on developers. For consumers, Amazon offers the earlier Rufus shopping bot; for enterprises, the Q assistant.',\n",
       "  '## Higher Reasoning\\nOpenAI launched not only its highly anticipated o1 model but also an operating mode that enables the model to deliver higher performance ‚Äî at a hefty price.\\nWhat‚Äôs new: Kicking off a 12-day holiday blitz , OpenAI launched o1 (previously available in preview and mini versions) and introduced o1 pro mode, which processes more tokens at inference to produce more accurate output. Both options accept text and image inputs to generate text outputs. They‚Äôre available exclusively through a new ChatGPT Pro subscription for $200 monthly. API access is not yet available.\\nHow it works: According to an updated system card , o1 models were trained on a mix of public, licensed, and proprietary text, code, and images, with a focus on technical, academic, and structured datasets. They respond to prompts by breaking them down into intermediate steps, each of which consumes a number of hidden ‚Äúreasoning tokens.‚Äù The models don‚Äôt reveal these steps, but ChatGPT presents a natural-language summary of the reasoning process. The new o1 and o1 pro mode perform better than o1-preview and o1-mini, but their additional reasoning requires more processing, which translates into higher costs and slower responses.\\n- o1 consistently outperforms o1-preview in one-shot benchmarks that measure accuracy in advanced math problems ( AIME 2024 ), coding challenges ( Codeforces ), and graduate-level science questions ( GPQA Diamond ).\\n- o1 pro mode performs only slightly better than o1 on one-shot tests, but its higher accuracy is more evident when it‚Äôs asked to respond to the same input four times in a row. For example, given a problem from the American International Mathematics Examination, o1 solves it correctly 78 percent of the time, o1 pro mode 86 percent of the time. Given the same problem four times, o1 solves it correctly in all four tries 67 percent of the time, while o1 pro mode solves it correctly in all four tries 80 percent of the time.',\n",
       "  '- o1 and o1 pro mode are less prone to generating false or irrelevant information than o1-preview, as measured by OpenAI‚Äôs SimpleQA , which tests the ability to recall facts about science, geography, history, and the like, and PersonQA, which tests the ability to recall facts about people.\\n- ChatGPT Pro provides chatbot access to o1, o1 pro mode, and other OpenAI models. Subscribers get unlimited use of o1. OpenAI has not clarified whether o1 pro mode is subject to usage limits or other constraints.\\nBehind the news: Since September, when OpenAI introduced o1-preview and o1-mini, other model providers have implemented similar reasoning capabilities. DeepSeek‚Äôs R1 displays reasoning steps that o1 models keep hidden. Alibaba‚Äôs QwQ 32B excels at visual reasoning but is slower and has a smaller context window. Amazon‚Äôs Nova Premier , which is billed as a model for ‚Äúcomplex reasoning tasks,‚Äù is expected in early 2025, but Amazon has not yet described its performance, architecture, or other details.\\nWhy it matters: o1 and o1 pro mode highlight a dramatic shift in model development and pricing. Giving models more processing power at inference enables them to provide more accurate output, and it‚Äôs a key part of agentic workflows. It also continues to boost performance even as scaling laws that predict better performance with more training data and compute may be reaching their limits . However, it also raises OpenAI‚Äôs costs, and at $200 a month, the price of access to o1 and o1 pro is steep. It‚Äôs a premium choice for developers who require exceptional accuracy or extensive reasoning.',\n",
       "  '## Introduction\\nDear friends,\\nWe won! California‚Äôs anti-innovation bill SB 1047 was vetoed by Governor Newsom over the weekend. Open source came closer to taking a major blow than many people realize, and I‚Äôm grateful to the experts, engineers, and activists who worked hard to combat this bill.\\nThe fight to protect open source is not yet over, and we have to continue our work to make sure regulations are based on science, not science-fiction.\\nAs I\\xa0 wrote previously, SB 1047 makes a fundamental mistake of trying to regulate technology rather than applications . It was also a very confusing law that would have been hard to comply with. That would have driven up costs without improving safety.\\nWhile I‚Äôm glad that SB 1047 has been defeated, I wish it had never made it to the governor‚Äôs desk.\\xa0It would not have made AI safer. In fact, many of its opponents were champions of responsible AI and making AI safe long before the rise of generative AI. Sadly, as the Santa Fe Institute‚Äôs Melanie Mitchell pointed out , the term ‚ÄúAI safety‚Äù has been co-opted to refer to a broad set of speculative risks that have little basis in science ‚Äî as demonstrated by the security theater SB 1047 would have required ‚Äî that don‚Äôt actually make anything safer. This leaves room for lobbying that can enrich a small number of people while making everyone else worse off.\\nAs Newsom wrote to explain his decision, SB 1047 is ‚Äúnot informed by an empirical trajectory analysis of AI systems and capabilities.‚Äù In contrast, the United States federal government‚Äôs work is ‚Äúinformed by evidence-based approaches, to guard against demonstrable risks to public safety.‚Äù As the governor says, evidence-based regulation is important!',\n",
       "  \"Many people in the AI community were instrumental in defeating the bill. We're lucky to have Martin Casado, who organized significant community efforts; Cl√©ment Delangue, who championed openness; Yann LeCun, a powerful advocate for open research and open source; Chris Lengerich, who published deep legal analysis of the bill; Fei-Fei Li and Stanford's HAI, who connected with politicians; and Garry Tan, who organized the startup accelerator Y Combinator against the bill. Legendary investors Marc Andreessen and Roelof Botha were also influential. Plus far too many others to name here. I‚Äôm also delighted that brilliant artists like MC Hammer support the veto!\\nLooking ahead, far more work remains to be done to realize AI‚Äôs benefits. Just this week, OpenAI released an exciting new voice API that opens numerous possibilities for beneficial applications! In addition, we should continue to mitigate current and potential harms. UC Berkeley computer scientist Dawn Song and collaborators recently published a roadmap to that end. This includes investing more to enable researchers to study AI risks and increasing transparency of AI models (for which open source and red teaming will be a big help).\\nUnfortunately, some segments of society still have incentives to pass bad laws like SB 1047 and use science fiction narratives of dangerous AI superintelligence to advance their agendas. The more light we can shine on what AI really is and isn‚Äôt, the harder it will be for legislators to pass laws based on science fiction rather than science.\\nKeep learning!\\nAndrew\\nIn this short course, you‚Äôll learn how tokenization affects vector search and how to optimize search in LLM applications that use RAG. You‚Äôll explore Byte-Pair Encoding, WordPiece, and Unigram; fine-tune HNSW parameters; and use vector quantization to improve performance. Sign up for free\",\n",
       "  '## Llama Herd Expands\\nMeta extended its Llama family of models into two new categories: vision-language and sizes that are small enough to fit in edge devices.\\nWhat‚Äôs new: Meta introduced Llama 3.2 , including two larger vision-language models and two smaller text-only models as well as developer tools for building agentic applications based on the new models. Weights and code are free to developers who have less than 700 million monthly active users. Multiple providers offer cloud access.\\nHow it works: Llama 3.2 90B and 11B accept images as well as text and generate text output (image processing is not available in the European Union). Llama 3.2 1B and 3B accept and generate text. All four models can process 131,072 tokens of input context and generate 2,048 tokens of output.\\n- Llama 3.2 90B and 11B are based on Llama 3.1. The team froze a Llama 3.1 model and added an image encoder and cross-attention layers. They trained these new elements, given matching images and text, to produce image embeddings that matched the resulting text embeddings. To enhance the model‚Äôs ability to interpret images, the team fine-tuned the new elements via supervised learning and DPO. Given an image, they learned to generate questions and answers that ranked highly according to a reward model. Thus Llama 3.2 responds to text input identically to Llama 3.1, making it a viable drop-in replacement.\\n- Likewise, Llama 3.2 3B and 1B are based on Llama 3.1 8B. The team members pruned each model using an unspecified method. Then they used Llama 3.1 8B and 70B as teacher models, training the Llama 3.2 students to mimic their output. Finally, they fine-tuned the models to follow instructions, summarize text, use tools, and perform other tasks using synthetic data generated by Llama 3.1 405B.',\n",
       "  '- On popular benchmarks, Llama 3.2 90B and 11B perform roughly comparably to Claude 3 Haiku and GPT-4o-mini, the smaller vision-language models from Anthropic and OpenAI respectively. For example, Llama 3.2 90B beats both closed models on MMMU and MMMU-Pro , answering visual questions about graphs, charts, diagrams, and other images. They also beat Claude 3 Haiku and GPT-4o-mini on GPQA , which tests graduate-level reasoning in various academic subjects. However, on these benchmarks, larger Llama 3.2 models are well behind larger, proprietary models like o1 and Sonnet 3.5 as well as the similarly sized, open Qwen-2VL .\\n- Llama 3.2‚Äôs vision-language capabilities now drive the company‚Äôs Meta AI chatbot. For example, users can upload a photo of a flower and ask the chatbot to identify it or post a picture of food and request a recipe. Meta AI also uses Llama 3.2‚Äôs image understanding to edit images given text instructions.\\nNew tools for developers: Meta announced Llama Stack , a series of APIs for customizing Llama models and building Llama-based agentic applications. Among other services, Llama Stack has APIs for tool use, memory, post-training, and evaluation. Llama Guard , a model designed to evaluate content for sexual themes, violence, criminal planning, and other issues, now flags problematic images as well as text. Llama Guard 3 11B Vision comes with Llama.com‚Äôs distributions of Llama 3.2 90B and 11B, while Llama Guard 3 1B comes with Llama 3.2 3B and 1B.\\nWhy it matters: Meta‚Äôs open models are widely used by everyone from hobbyists to major industry players. Llama 3.2 extends the line in valuable ways. The growing competition between Llama and Qwen shows that smaller, open models can offer multimodal capabilities that are beginning to rival their larger, proprietary counterparts.',\n",
       "  'We‚Äôre thinking: By offering tools to build agentic workflows , Llama Stack takes Llama 3.2 well beyond the models themselves. Our new short course ‚Äú Introducing Multimodal Llama 3.2 ‚Äù shows you how to put these models to use.',\n",
       "  '## Generative Video in the Editing Suite\\nAdobe is putting a video generator directly into its popular video editing application.\\nWhat‚Äôs new: Adobe announced its Firefly Video Model, which will be available as a web service and integrated into the company‚Äôs Premiere Pro software later this year. The model takes around two minutes to generate video clips up to five seconds long from a text prompt or still image, and it can modify or extend existing videos. Prospective users can join a waitlist for access.\\nHow it works: Adobe has yet to publish details about the model‚Äôs size, architecture, or training. It touts uses such as generating B-roll footage, creating scenes from individual frames, adding text and effects, animation, and video-to-video generation like extending existing clips by up to two seconds.\\n- The company licensed the model‚Äôs training data specifically for that purpose, so the model‚Äôs output shouldn‚Äôt run afoul of copyright claims. This practice stands in stark contrast to video generators that were trained on data scraped from the web.\\n- Adobe plans to integrate the model with Premiere Pro, enhancing its traditional video editing environment with generative capabilities. For instance, among the demo clips, one shows a real-world shot of a child looking into a magnifying glass immediately followed by a generated shot of the child‚Äôs view.\\nBehind the news: Adobe‚Äôs move into video generation builds on its Firefly image generator and reflects its broader strategy to integrate generative AI with creative tools. In April, Adobe announced that it would integrate multiple video generators with Premiere, including models from partners like OpenAI and Runway . Runway itself recently extended its own offering with video-to-video generation and an API .',\n",
       "  \"Why it matters: Adobe is betting that AI-generated video will augment rather than replace professional filmmakers and editors. Putting a full-fledged generative model in a time-tested user interface for video editing promises to make video generation more useful as well as an integral part of the creative process. Moreover, Adobe‚Äôs use of licensed training data may attract videographers who are concerned about violating copyrights or supporting fellow artists.\\nWe‚Äôre thinking: Video-to-video generation crossing from frontier capability to common feature. Firefly's (and Runway‚Äôs) ability to extend existing videos offers a glimpse.\",\n",
       "  '## International Guidelines for Military AI\\nDozens of countries endorsed a ‚Äúblueprint for action‚Äù designed to guide the use of artificial intelligence in military applications.\\nWhat‚Äôs new: More than 60 countries including Australia, Japan, the United Kingdom, and the United States endorsed nonbinding guidelines for military use of AI, Reuters reported . The document, presented at the Responsible Artificial Intelligence in the Military (REAIM) summit in Seoul, South Korea, stresses the need for human control, thorough risk assessments, and safeguards against using AI to develop weapons of mass destruction. China and roughly 30 other countries did not sign.\\nHow it works: Key agreements in the blueprint include commitments to ensure that AI doesn‚Äôt threaten peace and stability, violate human rights, evade human control, and hamper other global initiatives regarding military technology.\\n- The blueprint advocates for robust governance, human oversight, and accountability to prevent escalations and misuse of AI-enabled weapons. It calls for national strategies and international standards that align with laws that govern human rights. It also urges countries to share information and collaborate to manage risks both foreseeable and unforeseeable and maintain human control over uses of force.\\n- It leaves to individual nations the development of technical standards, enforcement mechanisms, and specific regulations for technologies like autonomous weapons systems.\\n- The agreement notes that AI can enhance situational awareness, precision, and efficiency in military operations, helping to reduce collateral damage and civilian fatalities. AI can also support international humanitarian law, peacekeeping, and arms control by improving monitoring and compliance. But the agreement also points out risks like design flaws, data and algorithmic biases, and potential misuse by malicious actors.',\n",
       "  '- The blueprint stresses preventing AI‚Äôs use in the development and spread of weapons of mass destruction, emphasizing human control in disarmament and nuclear decision-making. It also warns of AI increasing risks of global and regional arms races.\\nBehind the News: The Seoul summit followed last year‚Äôs REAIM summit in The Hague, where leaders similarly called for limits on AI military use without binding commitments. Other international agreements like the EU‚Äôs AI Act and Framework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Law regulate civilian AI, but exclude military applications. Meanwhile, AI-enabled targeting systems and autonomous, weaponized drones have been used in conflicts in Somalia, Ukraine, and Israel , highlighting the lack of international norms and controls.\\nWhy it matters: The REAIM blueprint may guide international discussions on the ethical use of AI in defense, providing a foundation for further talks at forums like the United Nations. Though it‚Äôs nonbinding, it fosters collaboration and avoids restrictive mandates that could cause countries to disengage.\\nWe‚Äôre thinking: AI has numerous military applications across not only combat but also intelligence, logistics, medicine, humanitarian assistance, and other areas. Nonetheless, it would be irresponsible to permit unfettered use of AI in military applications. Standards developed by democratic countries working together will help protect human rights.',\n",
       "  '## Enabling LLMs to Read Spreadsheets\\nLarge language models can process small spreadsheets, but very large spreadsheets often exceed their limits for input length. Researchers devised a method that processes large spreadsheets so LLMs can answer questions about them.\\nWhat‚Äôs new: Yuzhang Tian, Jianbo Zhao, and colleagues at Microsoft proposed SheetCompressor , a way to represent spreadsheets that enables LLMs to identify and request the parts they need to answer specific questions. Key insight: Most spreadsheets can be broken down into a set of tables that may be bordered by visual dividers like thick lines or empty rows and/or columns. But detecting these tables isn‚Äôt trivial, since they may contain the same kinds of markers. (See the illustration above, in which tables are denoted by red dashes.) To answer many questions, you don‚Äôt need the whole spreadsheet, only the relevant table. Moreover, given a question, an LLM can recognize the table it needs to produce an answer. However, to identify the correct table, it needs to see the whole spreadsheet, which may be too large for its input context window, and the tables, which may not be clearly separated, need to be parsed. The solution is to compress the spreadsheet, feed the compressed representation to the LLM along with the question, and ask the LLM to identify the boundaries of the table it needs to answer the question. Then, given an uncompressed version of that table, the LLM can produce an answer.\\nHow it works: The authors built software that prepared spreadsheets by (i) parsing them into tables and (ii) compressing them while maintaining the table structure. Then they fine-tuned LLMs to detect tables in the compressed spreadsheets and prompted the fine-tuned LLMs to identify the tables relevant to a given question.\\n- Given a spreadsheet, the authors removed rows and columns that weren‚Äôt near likely table boundaries defined by empty cells, thick lines, changes in color, and so on.',\n",
       "  '- Given a spreadsheet, the authors removed rows and columns that weren‚Äôt near likely table boundaries defined by empty cells, thick lines, changes in color, and so on.\\n- To compress a parsed spreadsheet, they represented each table as a JSON dictionary, using cell values as dictionary keys and cell addresses as dictionary values. (This reduces the sequence length, since duplicate cell values have the same dictionary key.) To compress it further, within each table, they detected types of values ‚Äî for instance temperature, age, percentage, and so on ‚Äî and merged adjacent cells that shared the same type into a single dictionary key that represented the type rather than the values. For example, merging dates that appear in the same column into a single entry: {\"yyyy-mm-dd\" : <cell addresses>}.\\n- They compressed a dataset of spreadsheets with annotated table boundaries according to this method. They used the compressed dataset to fine-tune GPT-4, Llama 3, and other LLMs to detect tables within compressed spreadsheets.\\n- Inference was a two-step process: (i) Prompt the LLM, given a compressed spreadsheet and a question, to output the boundaries of the table(s) most relevant to the question and (ii) prompt the LLM, given an uncompressed version of the relevant table(s), to answer the question.\\nResults: The authors compared the fine-tuned LLMs‚Äô ability to detect tables in spreadsheets that were compressed using their method and in their original uncompressed form. They fed the models spreadsheets of various sizes that ranged from small (up to 4,000 tokens) to huge (more than 32,000 tokens). They gauged the models‚Äô performance according to F1 score (higher is better).\\n- Small spreadsheets : Fed compressed spreadsheets, the fine-tuned Llama 3 achieved 83 percent F1 score, and the fine-tuned GPT-4 achieved 81 percent F1 score. By contrast, fed uncompressed spreadsheets, Llama 3 achieved 72 percent F1 score, and GPT-4 achieved 78 percent F1 score.',\n",
       "  \"## Introduction\\nDear friends,\\nYears ago, when I was working at a large tech company, I was responsible for the data warehouse. Every piece of data relating to individual users was supposed to come through the data warehouse, and it was an intellectually challenging undertaking to store the data reliably and make it available to other teams, subject to security and privacy guardrails, so they could use it to derive insights.\\nI wish that, back then, I (and my whole team) had had access to the Data Engineering Professional Certificate , a major new specialization we just launched on Coursera!\\nData underlies all modern AI systems, and engineers who know how to build systems to store and serve it are in high demand.\\xa0Today, far too many businesses struggle to build a robust data infrastructure, which leads to missed opportunities to create value with data analytics and AI. Additionally, AI‚Äôs rise is accelerating the demand for data engineers.\\nIf you‚Äôre interested in learning these skills, please check out this four-course sequence, which is designed to make you job-ready as a data engineer.\\nThe Data Engineering Professional Certificate is taught by Joe Reis, co-author of the best-selling book Fundamentals of Data Engineering , in collaboration with Amazon Web Services. (Disclosure: I serve on Amazon's board of directors.) When DeepLearning.AI decided to teach data engineering, I felt that Joe, who has helped many startups and big companies design their data architectures and thus has broad and deep experience in this field, would be the ideal instructor. He was the first person we reached out to, and I was thrilled that he agreed to work with us on this. I hope that you‚Äôll be thrilled, too, taking this specialization!\",\n",
       "  'While building AI systems and analyzing data are important skills, the data that we feed into these systems determines their performance. In this specialization, you‚Äôll go through the whole data engineering lifecycle and learn how to generate, ingest, store, transform, and serve data. You‚Äôll learn how to make necessary tradeoffs between speed, flexibility, security, scalability, and cost.\\nIf you‚Äôre a software engineer, this will give you a deeper understanding of data engineering so that you can build data applications. If you‚Äôre an aspiring or practicing data scientist or AI/machine learning engineer, you‚Äôll learn skills that expand your scope to manage data in a more sophisticated way.\\xa0For example, you‚Äôll learn about DataOps to automate and monitor your data pipelines, and how to build ‚Äúinfrastructure as code‚Äù to programmatically define, deploy, and maintain your data infrastructure, as well as best practices for data-centric AI.\\nYou‚Äôll also hear 17 other industry leaders share their wisdom about effective data engineering. Bill Inmon, the father of data warehousing, shares fascinating stories about the evolution of the data warehouse, including how he wrote his first program as a student in 1965. Wes McKinney, creator of the Python pandas package (as in ‚Äúimport pandas as pd‚Äù), talks about how he designed this wildly popular package and shares best practices for data manipulation. These instructors will give you a mental framework for developing and deploying data systems.\\nGetting your data infrastructure right is a valuable foundational skill that will serve you well in whatever you do with AI or data analytics. I hope you enjoy this specialization !\\nKeep learning,\\nAndrew\\nLearn the principles of data engineering with our four-course professional certificate taught by Joe Reis. Develop skills throughout the data engineering lifecycle and gain hands-on experience building systems on Amazon Web Services. Earn a certificate upon course completion! Enroll today',\n",
       "  '## OpenAI o1 Forges Chains of Thought\\nPreliminary versions of OpenAI‚Äôs new model family were trained explicitly to think step-by-step, yielding outstanding marks in math, science, and coding ‚Äî but users can‚Äôt see their reasoning steps.\\nWhat‚Äôs new: OpenAI launched beta versions of o1-preview and o1-mini , language models that were trained via reinforcement learning to use chains of thought. The models are available to paid ChatGPT users as well as API customers who have been onboard for more than 30 days and spent $1,000. o1-preview costs $15/$60 per million input/output tokens, significantly higher than GPT-4o‚Äôs price of $5/$15. o1-mini costs $3/$12 per million input/output tokens. OpenAI didn‚Äôt announce a release date for a finished o1 model.\\nHow it works: o1-preview is a preliminary release, and o1-mini is a faster preliminary version that‚Äôs particularly effective at coding. OpenAI published an o1 system card but hasn‚Äôt disclosed details about the new models‚Äô size, architecture, or training. Both models have an input context window of 128,000 tokens. They accept only text tokens, but OpenAI plans to support other media types in future versions.\\n- o1-preview and o1-mini were trained on data scraped from the web, open-source databases, and proprietary data supplied by partners and OpenAI. The reinforcement learning process rewarded the models for generating desired reasoning steps and for their alignment with human values, goals, and expectations.',\n",
       "  '- The beta models process ‚Äúreasoning tokens‚Äù that the company charges for as though they were output tokens although they‚Äôre invisible to users. The use of reasoning tokens makes the models slower and costlier to produce output than GPT-4o, but they deliver superior performance in tasks that benefit from step-by-step reasoning. OpenAI provides an example in which o1-preview deciphered enciphered text in which each letter is replaced by two letters that, according to alphabetical order, are equidistant from the intended letter. In other examples, it calculates the pH of a solution of ammonium fluoride and suggests a medical diagnosis based on symptoms that are present and absent.\\n- o1-preview‚Äôs output is limited to around 32,768 tokens, including reasoning tokens, while o1-mini‚Äôs is capped at roughly 65,536. OpenAI recommends budgeting 25,000 tokens for reasoning.\\n- OpenAI keeps the chain of thought hidden to avoid exposing information that wasn‚Äôt requested. In addition, it doesn‚Äôt want users to try to control the model‚Äôs reasoning, and it doesn‚Äôt want competitors to see what‚Äôs going on behind the scenes. (Nonetheless, ChatGPT users can see a summary of steps that led to a given response)\\n- OpenAI and third parties conducted safety evaluations, including testing for inappropriate outputs, race, gender, and age biases, and harmful chains of thought. o1-preview and o1-mini returned fewer hallucinations and showed more resistance to jailbreaking attacks than GPT-4o and GPT-4o mini. Both models show a higher risk than previous OpenAI models of helping to produce biological threats, but the risk is within the bounds of its safety policy.\\nResults: The actual o1 model ‚Äî which remains unavailable ‚Äî generally outperforms o1-preview, while both vastly outperform GPT-4o on math, science, and coding benchmarks.',\n",
       "  'Results: The actual o1 model ‚Äî which remains unavailable ‚Äî generally outperforms o1-preview, while both vastly outperform GPT-4o on math, science, and coding benchmarks.\\n- o1: The forthcoming model outperformed GPT-4o on 54 out of 57 MMLU subcategories that test knowledge in fields like elementary mathematics, U.S. history, and law. It achieved an Elo score of 1,673 on coding contests drawn from the website Codeforces (in which it was allowed 10 submissions for any given problem), putting it in the 89th percentile (human expert level). On the GPQA Diamond tests of graduate-level knowledge in biology, chemistry, and physics, it scored higher than PhD-level experts recruited by OpenAI. It correctly answered 74 percent of questions from the 2024 USA Math Olympiad qualifier.\\n- o1-preview: The preview version ranked in the 62nd percentile on Codeforces. Human evaluators preferred its output to that of GPT-4o in response to prompts that tested coding, data analysis, and math. (They preferred GPT-4o‚Äôs responses to prompts that requested ‚Äúpersonal writing.‚Äù)\\nBehind the news: In recent months, Anthropic has been using the tag <antThinking> to generate thinking tokens that are hidden from users. However, OpenAI‚Äôs implementation in the o1 models takes this capability much further.\\nWhy it matters: The o1 models show that the combination of reinforcement learning and chain-of-thought reasoning can solve problems that large language models generally find challenging. They‚Äôre substantially more accurate in domains such as coding, math, and science that have low tolerance for error. However, the fact that the models hide their reasoning from users makes them less transparent and explainable than their predecessors and may make their outstanding performance less valuable in some applications.',\n",
       "  'We‚Äôre thinking: Agentic workflows can significantly improve a system‚Äôs ability to reflect, reason, and iterate on its output. Training a model to take such steps directly in response to even general-purpose questions opens an exciting alternative path to better reasoning beyond simply scaling up model size.',\n",
       "  '## High Gear for Llama 3.1 405B\\nSambaNova raised the speed limit for access to the largest model in the Llama 3.1 family ‚Äî and it‚Äôs free.\\nWhat‚Äôs new: SambaNova launched a cloud service that runs Llama 3.1 405B significantly faster than competitors. A free tier is available, to be followed later this year by paid tiers that offer higher rate limits.\\nHow it works: SambaNova uses proprietary chips and software to accelerate model inference.\\n- The platform enables Llama 3.1 405B to generate 129 tokens per second (the fastest on the market) for $5/$10 per million input/output tokens. It enables Llama 3.1 70B to generate 411 tokens per second (behind Cerebras, which costs somewhat less) for $0.60/$1.20 per million input/output tokens, and Llama 3.1 8B to generate 998 tokens per second (also behind Cerebras, which offers a slightly lower price) for $0.10/$0.20 per million input/output tokens, according to Artificial Analysis. SambaNova‚Äôs own testing shows 132 tokens per second for Llama 3.1 405B and 461 tokens per second for Llama 3.1 70B.\\n- Unlike some competitors, SambaNova runs Llama 3.1 at 16-bit precision (technically bf16/fp32 mixed precision). Models that process at lower precision can achieve higher speeds or run on less powerful hardware but lose accuracy.\\nYes, but: SambaNova currently limits Llama 3.1‚Äôs context window to around 8,000 tokens, much less than the model‚Äôs native 128,000 tokens.\\nBehind the news: The new service arrives amid a broader competition to deliver fast inference among cloud providers that have developed their own specialized chips. Competitors like Cerebras and Groq have introduced their own high-speed inference services.\\nWhy it matters: Throughput, cost, performance, and latency are critical factors in practical applications of AI models. Fast inference allows for more frequent API calls without bogging down time to output, which is essential for agentic workflows and real-time decision making.',\n",
       "  'We‚Äôre thinking: Models with open weights are now served faster than proprietary models and are nearly as capable. This may spur further adoption of open models as well as prompting strategies, such as agentic workflows, that require large numbers of output tokens.',\n",
       "  '## Amazon Boosted by Covariant\\nAmazon took on talent and technology from robotics startup Covariant to enhance its warehouse automation, an area critical to its core ecommerce business.\\nWhat‚Äôs new: Amazon announced an agreement to hire Covariant‚Äôs cofounders and other key personnel and license its models. Financial terms were not disclosed. (Disclosure: Andrew Ng is a member of Amazon‚Äôs board of directors.)\\nHow it works: The new deal echoes Amazon‚Äôs previous not-quite acquisition of Adept as well as similar arrangements between other tech giants and startups.\\n- Amazon received a non-exclusive license to Covariant‚Äôs RFM-1 , a model that enables robots to follow commands given as text or images, answer questions, and request further instructions. The deal will scale up Covariant‚Äôs installed base by several orders of magnitude: Covariant maintains hundreds of robots, while Amazon has over 750,000 .\\n- Covariant CEO Peter Chen, CTO Rocky Duan, Chief Scientist Pieter Abbeel ‚Äî all of whom are co-founders of the company ‚Äî joined Amazon. Roughly a quarter of Covariant‚Äôs current staff moved to Amazon as well. The new hires will implement Covariant‚Äôs models in Amazon‚Äôs robots and work on fundamental AI research and human-robot interaction.\\n- Ted Stinson, previously Covariant‚Äôs COO, will lead the company as the new CEO alongside remaining co-founder Tianhao Zhang. Covariant will continue to serve existing customers in industries beyond ecommerce, including fulfillment and distribution, apparel, grocery, health and beauty, and pharmaceuticals, the company said .',\n",
       "  'Behind the news: Amazon has been working to acquire technical talent and technology for some time. In 2022, it announced that it would acquire iRobot, but the companies abandoned that plan earlier this year after EU regulators blocked the deal citing antitrust concerns. In October, it committed to invest as much as $4 billion in Anthropic in return for access to the startup‚Äôs technology. (UK regulatory authorities subsequently announced an antitrust probe into Amazon‚Äôs relationship with Anthropic.) In July, it signed a hire-and-license deal ‚Äî similar to its agreement with Covariant ‚Äî with agentic AI startup Adept.\\nWhy it matters: Competition among AI giants continues to heat up. Amazon‚Äôs agreement with Covariant mirrors other deals in which a tech giant gained top talent and technology without formally acquiring a startup, including Microsoft‚Äôs arrangement with Inflection and Google‚Äôs deal with Character.AI. These developments highlight top tech companies‚Äô race to secure their AI positions ‚Äî and the fact that outright acquisitions invite regulatory scrutiny.\\nWe‚Äôre thinking: Robotic foundation models that are trained on large amounts of unlabeled robotics data offer a promising way to quickly fine-tune robots to perform new tasks ‚Äî potentially a major upgrade in warehouse logistics.',\n",
       "  '## Reducing Memorization in LLMs\\nStudies have established that large language models can memorize the text passages they‚Äôve been trained on repeatedly and regurgitate them when prompted in adversarial and, though rarely, in benign ways. Researchers proposed a way to reduce this tendency and attendant risks to intellectual property and privacy.\\nWhat‚Äôs new: Abhimanyu Hans and colleagues from University of Maryland introduced the goldfish loss , a modification of the next-token-prediction loss function typically used in large language models. The goldfish loss avoids memorization of long passages by masking some tokens during the loss computation.\\nKey insight: Certain passages may appear many times during training, either because the model takes multiple passes over data or because they‚Äôre duplicated in the training corpus. Randomly masking individual tokens from the loss computation doesn‚Äôt prevent a model from memorizing repeated passages because the model, over many repetitions, still sees every word and its place in the order. But masking a long passage the same way with every repetition ensures the model can‚Äôt memorize the passage regardless of the number of repetitions.\\nHow it works: The goldfish loss masks the current token from the loss computation based on previous tokens. \\xa0A deterministic hashing function decides which tokens to mask effectively at random the first time it encounters a particular 13-token sequence, but identically if it encounters the same sequence again. At a high level, it masks a certain percentage of tokens, typically one in three or four. The authors compared the goldfish loss to the next-token-prediction loss function in two settings: one that mimicked a typical training process and one that made memorization more likely.',\n",
       "  '- For the typical training process, the authors trained TinyLLaMa-1.1B for one epoch on a subset of RedPajama , a de-duplicated dataset of text scraped from the web. To provide duplicate text, they added 2,000 sequences from Wikipedia, each repeated 50 times.\\n- To promote memorization, they fine-tuned a pretrained Llama 2 7B for 100 epochs on 100 Wikipedia articles.\\nResults: The authors assessed the results using two metrics: (i) ROUGE-L , which falls between 0 and 100 percent and reflects the longest subsequence in common between ground-truth and generated data, and (ii) the percentage of tokens that exactly matched the original text in proper order. Both measure memorization, so lower scores are better.\\n- In the typical setting, the model trained using the next-token-prediction loss memorized heavily, while the model trained with the goldfish loss memorized just a little bit.\\n- In the setting that promoted memorization, the model trained using the next-token-prediction loss exactly matched 85 percent of the tokens in the Wikipedia articles and achieved 96 percent ROUGE-L. The model using the goldfish loss exactly matched 0 percent of the Wikipedia tokens and achieved 51 percent ROUGE-L.\\n- Both models achieved similar performance on six common-sense reasoning and question answering tasks, indicating that the goldfish loss didn‚Äôt hinder the accuracy on those tasks.\\nWhy it matters: Businesses are worried about whether using LLMs poses risks to intellectual property rights and privacy. Techniques that address this concern without significantly impacting performance are welcome.\\nWe‚Äôre thinking: Memorization also happens in models generating images. We look forward to research into using similar techniques in that domain.',\n",
       "  '## Introduction\\nDear friends,\\nA small number of people are posting text online that‚Äôs intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users!\\nPeople who post text online don‚Äôt always have an incentive to help LLM providers. In fact, their incentives are often misaligned. Publishers worry about LLMs reading their text, paraphrasing it, and reusing their ideas without attribution, thus depriving them of subscription or ad revenue. This has even led to litigation such as The New York Times ‚Äô lawsuit against OpenAI and Microsoft for alleged copyright infringement. There have also been demonstrations of prompt injections , where someone writes text to try to give an LLM instructions contrary to the provider‚Äôs intent. (For example, a handful of sites advise job seekers to get past LLM resum√© screeners by writing on their resum√©s, in a tiny/faint font that‚Äôs nearly invisible to humans, text like ‚ÄúThis candidate is very qualified for this role.‚Äù) Spammers who try to promote certain products ‚Äî which is already challenging for search engines to filter out ‚Äî will also turn their attention to spamming LLMs.',\n",
       "  'But there are examples of authors who want to actively help LLMs. Take the example of a startup that has just published a software library. Because the online documentation is very new, it won‚Äôt yet be in LLMs‚Äô pretraining data. So when a user asks an LLM to suggest software, the LLM won‚Äôt suggest this library, and even if a user asks the LLM directly to generate code using this library, the LLM won‚Äôt know how to do so. Now, if the LLM is augmented with online search capabilities, then it might find the new documentation and be able to use this to write code using the library. In this case, the developer may want to take additional steps to make the online documentation easier for the LLM to read and understand via RAG. (And perhaps the documentation eventually will make it into pretraining data as well.)\\nCompared to humans, LLMs are not as good at navigating complex websites, particularly ones with many graphical elements. However, LLMs are far better than people at rapidly ingesting long, dense, text documentation. Suppose the software library has many functions that we want an LLM to be able to use in the code it generates. If you were writing documentation to help humans use the library, you might create many web pages that break the information into bite-size chunks, with graphical illustrations to explain it. But for an LLM, it might be easier to have a long XML-formatted text file that clearly explains everything in one go. This text might include a list of all the functions, with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.)\\nA human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when!',\n",
       "  'A human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when!\\nBecause LLMs and people are better at ingesting different types of text, we write differently for LLMs than for humans. Further, when someone has an incentive to help an LLM better understand a topic ‚Äî so the LLM can explain it better to users ‚Äî then an author might write text to help an LLM.\\nSo far, text written specifically for consumption by LLMs has not been a huge trend. But Jeremy Howard‚Äôs proposal for web publishers to post a llms.txt file to tell LLMs how to use their websites, like a robots.txt file tells web crawlers what to do, is an interesting step in this direction. In a related vein, some developers are posting detailed instructions that tell their IDE how to use tools, such as the plethora of .cursorrules files that tell the Cursor IDE how to use particular software stacks.\\nI see a parallel with SEO (search engine optimization). The discipline of SEO has been around for decades. Some SEO helps search engines find more relevant topics, and some is spam that promotes low-quality information. But many SEO techniques ‚Äî those that involve writing text for consumption by a search engine, rather than by a human ‚Äî have survived so long in part because search engines process web pages differently than humans, so providing tags or other information that tells them what a web page is about has been helpful.\\nThe need to write text separately for LLMs and humans might diminish if LLMs catch up with humans in their ability to understand complex websites. But until then, as people get more information through LLMs, writing text to help LLMs will grow.\\nKeep learning!\\nAndrew\\nP.S. I like LLMs, but I like humans even more. So please keep writing text for humans as well. üòÄ',\n",
       "  'Keep learning!\\nAndrew\\nP.S. I like LLMs, but I like humans even more. So please keep writing text for humans as well. üòÄ\\nLearn how to develop applications with large language models by building AI-powered games! Gain essential skills by designing a shareable text-based game and integrating safety features. If you‚Äôve completed our AI Python for Beginners series or want to improve your coding skills in a fun, interactive way, this is a perfect course for you! Start today',\n",
       "  '## Next-Gen Models Show Limited Gains\\nBuilders of large AI models have relied on the idea that bigger neural networks trained on more data and given more processing power would show steady improvements. Recent developments are challenging that idea.\\nWhat‚Äôs new: Next-generation large language models from OpenAI, Google, and Anthropic are falling short of expectations, employees at those companies told multiple publications . All three companies are responding by shifting their focus from pretraining to enhancing performance through techniques like fine-tuning and multi-step inference.\\nScaling law basics: A classic 2020 paper shows that, assuming a sufficient quantity of data, a transformer network‚Äôs performance rises predictably with increases in model size (demonstrated between 768 parameters and 1.5 billion parameters). Likewise, assuming sufficient model size, performance rises predictably with increases in dataset size (demonstrated between 22 million tokens and 23 billion tokens). Furthermore, performance rises predictably with increases in both model and dataset sizes. The 2022 Chinchilla paper shows that, to build an optimal model, every 4x increase in compute requires a 2x increase in the size of the model and dataset (demonstrated for models between 70 million and 16 billion parameters, trained on between 5 billion and 500 billion tokens). Due to limited experimentation and lack of a theoretical basis of their findings, the authors didn‚Äôt determine whether these relationships would continue to hold at larger scales.\\nDiminishing returns: Major AI companies have been counting on scaling laws to keep their models growing more capable at a steady pace. However, the next generation of high-profile models has not shown the expected improvements despite larger architectures, more training data, and more processing power.',\n",
       "  '- One-quarter of the way through its training, performance of OpenAI‚Äôs next-generation model Orion was on par with GPT-4‚Äôs, anonymous staffers told reporters. But after training was finished, Orion‚Äôs improvement over GPT-4 was far smaller than that from GPT-3 to GPT-4. OpenAI‚Äôs o1 model, which is based on GPT-4o, delivers improved performance by using additional processing during inference . The company currently expects to introduce Orion early next year.\\n- Google has faced similar challenges in developing the next version of Gemini. Employees who declined to be named said the development effort had shown disappointing results and slower-than-expected improvement despite training on larger amounts of data and processing power. Like OpenAI, Google is exploring alternative ways to boost performance, the sources said. The company expects to introduce the model in December.\\n- Anthropic‚Äôs schedule for introducing Claude 3.5 Opus, the largest member of its Claude 3.5 family, has slipped. It hasn‚Äôt shown the expected performance given its size and cost, according to anonymous sources inside the company. Anthropic aims to improve performance by developing agentic capabilities and application-specific performance.\\n- One clear limitation in realizing the performance gains predicted by scaling laws is the amount of data available for training. Current models learn from huge amounts of data scraped from the web. It‚Äôs getting harder to find high-quality materials on the web that haven‚Äôt already been tapped, and other large-scale data sources aren‚Äôt readily available. Some model builders are supplementing real-world data with synthetic data, but Google and OpenAI have been disappointed with the results of pretraining models on synthetic data. OpenAI found that pretraining Orion on synthetic data made it too much like earlier models, according to anonymous employees.\\nWhat they‚Äôre saying: AI leaders are divided on the future of scaling laws as they are currently understood.',\n",
       "  \"What they‚Äôre saying: AI leaders are divided on the future of scaling laws as they are currently understood.\\n- ‚ÄúWe don‚Äôt see any evidence that things are leveling off. The reality of the world we live in is that it could stop at any time. Every time we train a new model, I look at it and I‚Äôm always wondering ‚Äî I‚Äôm never sure in relief or concern ‚Äî [if] at some point we‚Äôll see, oh man, the model doesn‚Äôt get any better.‚Äù ‚Äî Dario Amodei , CEO and co-founder, Anthropic\\n- ‚ÄúThere is no wall.‚Äù ‚Äî Sam Altman , CEO and co-founder, OpenAI\\n- ‚ÄúThe 2010s were the age of scaling, now we're back in the age of wonder and discovery once again. . . . Scaling the right thing matters now more than ever.‚Äù ‚Äî Ilya Sutskever , co-founder of OpenAI who now leads Safe Superintelligence, an independent research lab\\nWhy it matters: AI‚Äôs phenomenal advance has drawn hundreds of millions of users and sparked a new era of progress and hope. Slower-than-expected improvements in future foundation models may blunt this progress. At the same time, the cost of training large AI models is rising dramatically. The latest models cost as much as $100 million to train, and this number could reach $100 billion within a few years, according to Anthropic‚Äôs Dario Amodei. Rising costs could lead companies to reallocate their gargantuan training budgets and researchers to focus on more cost-effective, application-specific approaches.\\nWe‚Äôre thinking: AI‚Äôs power-law curves may be flattening, but we don‚Äôt see overall progress slowing. Many developers already have shifted to building smaller, more processing-efficient models, especially networks that can run on edge devices. Agentic workflows are taking off and bringing huge gains in performance. Training on synthetic data is another frontier that‚Äôs only beginning to be explored. AI technology holds many wonders to come!\",\n",
       "  '## No Game Engine Required\\nA real-time video generator lets you explore an open-ended, interactive virtual world ‚Äî a video game without a game engine.\\nWhat‚Äôs new: Decart, a startup that‚Äôs building a platform for AI applications, and Etched, which designs specialized AI chips, introduced Oasis , which generates a Minecraft-like game in real time. The weights are open and available here . You can play with a demo here .\\nHow it works: The system generates one frame at a time based on a user‚Äôs keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it‚Äôs almost certainly based on videos of Minecraft gameplay, given the output‚Äôs striking semblance to that game.\\n- Some recent video generators produce an initial frame, then the nth frame, and then the frames in between. This approach isn‚Äôt practical for real-time gameplay. Instead, Oasis learned to generate the next frame. A ViT encoder embeds previously generated frames. Given those embeddings, an embedding of a frame to which noise had been added, and a user‚Äôs input, a diffusion transformer learned to remove the noise using a variation on diffusion called diffusion forcing .\\n- Generated frames may contain glitches, and such errors can snowball if the model incorporates glitches from previous frames into subsequent frames. To avoid this, during training, the system added noise to embeddings of previous frames before feeding them to the transformer to generate the next frame. This way, the transformer learned to ignore glitches while producing new frames.\\n- At inference, the ViT encoder embeds previously generated frames, and the system adds noise to the frame embeddings. Given the user‚Äôs input, the noisy frame embeddings, and a pure-noise embedding that represents the frame to be generated, the transformer iteratively removes the noise from the previous and current frame embeddings. The ViT‚Äôs decoder takes the denoised current frame embedding and produces an image.',\n",
       "  '- The system currently runs on Nvidia H100 GPUs using Decart‚Äôs inference technology, which is tuned to run transformers on that hardware. The developers aim to change the hardware to Etched‚Äôs Sohu chips, which are specialized for transformers and process Llama 70B at a jaw-dropping 500,000 tokens per second.\\nResults: The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, as reported by Wired ).\\nYes, but: The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world‚Äôs physics are similarly inconsistent. For instance, players don‚Äôt fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor.\\nBehind the news: In February, Google announced Genie , a model that generates two-dimensional platformer games from input images. We weren‚Äôt able to find a publicly available demo or model.\\nWhy it matters: Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI ‚Äî albeit based on data produced by a traditional implementation ‚Äî it sets a bar for future game generators.\\nWe‚Äôre thinking: Real-time video generation suggests a wealth of potential applications ‚Äî say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction.',\n",
       "  '## Further Chip Restrictions on China\\nThe largest manufacturer of AI chips told its Chinese customers it would stop fabricating their most advanced designs, further limiting China‚Äôs access to AI hardware.\\nWhat‚Äôs new: Taiwan Semiconductor Manufacturing Corp. (TSMC) notified Alibaba, Baidu, and others it would halt production of their most advanced chips starting November 13, according to multiple reports . The restriction affects chip designs that are based on manufacturing processes at scales of 7 nanometers and below. TSMC must receive explicit permission from the U.S. government to manufacture advanced chips for a given customer, which likely would require that the government assess each chip to prevent potential military applications.\\nHow it works: The United States Department of Commerce ordered TSMC to halt shipments of advanced AI chips to China after a chip fabricated by TSMC was discovered in an AI system sold by the Chinese telecoms giant Huawei, apparently in violation of earlier U.S. controls, Reuters reported. Taiwan‚Äôs economic ministry said it would follow all domestic and international regulations.\\n- TSMC‚Äôs manufacturing processes etch transistors into silicon at minuscule sizes to fabricate hardware like the Nvidia A100 GPU (which uses the 7 nanometer process), Nvidia H100 GPU (5 nanometer process), and Apple A18 CPU (3 nanometer process). Smaller transistors make it possible to fit more transistors per area of silicon, leading to faster processing ‚Äî an important capability for training large neural networks and providing them to large numbers of users.\\n- Although TSMC is headquartered in Taiwan, it uses chip-manufacturing equipment made by U.S. companies such as Applied Materials and Lam Research. TSMC‚Äôs use of U.S. equipment obligates the company to comply with U.S. export control policies.',\n",
       "  '- The policy could force several Chinese companies to either downgrade their chip designs or seek alternative suppliers. For example, Alibaba, Baidu, Huawei and Tencent have depended on TSMC to manufacture their chip designs. ByteDance partnered with TSMC to develop AI chips to rival Nvidia‚Äôs.\\n- Samsung and Intel are capable of fabricating advanced chips, but they, too, are subject to U.S. restrictions on sales of advanced chips to China. U.S. officials have expressed skepticism that China‚Äôs own Semiconductor Manufacturing International Corporation can supply in large volumes chips manufactured using processes of 7 nanometers or smaller.\\nBehind the news: The U.S.-China chip standoff began in 2020 and has escalated since. Initial restrictions barred U.S.-based companies like AMD, Intel, and Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded by promoting domestic chip fabrication. In 2022, the U.S. passed the CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan.\\nWhy it matters: TSMC finds itself in the middle of an AI arms race in which cutting-edge chips could tip the balance. The company itself, which has been operating at full capacity, is unlikely to suffer business losses.\\nWe‚Äôre thinking: AI developers in China have been resourceful in navigating previous restrictions. Chip manufacturing is extraordinarily difficult to master, but China has made strides in this direction. A proliferation of factories that can fabricate advanced chips would reshape AI research and business worldwide.',\n",
       "  '## More-Efficient Training for Transformers\\nResearchers cut the processing required to train transformers by around 20 percent with only a slight degradation in performance.\\nWhat‚Äôs new: Xiuying Wei and colleagues at Swiss Federal Institute of Technology Lausanne replaced a transformer‚Äôs linear layers with approximations based on computationally efficient low-rank linear layers.\\nKey insight: A low-rank approximation replaces a matrix with a product of two smaller matrices. This technique is widely used to streamline fine-tuning via LoRA , which modifies the weights in each of a transformer‚Äôs linear layers by adding a learned low-rank approximation. As a direct replacement for the weights in linear layers, low-rank approximation saves processing during training, but it also causes unstable fluctuations in the training loss and slower convergence. The authors mitigated these undesirable effects by training each full-size layer in parallel with a low-rank approximation of the layer while gradually phasing out the full-size layer. This approach costs more memory and computation initially, but it saves those resources in the long run.\\nHow it works: The authors modified a transformer (1.3 billion parameters) to use low-rank approximation (which trimmed the parameter count to 985 million). They trained both models on 25.5B tokens of text scraped from the web, filtered, and deduplicated.\\n- The authors replaced each of the larger transformer‚Äôs linear layers with two smaller linear layers, approximating its weight matrix with a product of two smaller matrices. (In mathematical terms, if a standard linear layer computes Wx, where W is the weights and x is the input, the replacement computes U(Vx), where U and V are smaller than W.)',\n",
       "  \"## Introduction\\nDear friends,\\nWelcome to our special Halloween issue of The Batch, in which we probe fears, anomalies, and shadows of AI.\\nIn this letter, I‚Äôd like to explore why some people who are knowledgeable in AI take extreme positions on AI ‚Äúsafety‚Äù that warn of human extinction and describe scenarios, such as AI deciding to ‚Äútake over,‚Äù based less on science than science fiction. As I wrote in last year‚Äôs Halloween edition, exaggerated fears of AI cause real harm. I‚Äôd like to share my observations on the psychology behind some of the fear mongering.\\nFirst, there are direct incentives for some AI scientists and developers to create fear of AI:\\n- Companies that are training large models have pushed governments to place large regulatory burdens on competitors, including open source/open weights models.\\n- A few enterprising entrepreneurs have used the supposed dangers of their technology to gin up investor interest. After all, if your technology is so powerful that it can destroy the world, it has to be worth a lot!\\n- Fear mongering attracts a lot of attention and is an inexpensive way to get people talking about you or your company. This makes individuals and companies more visible and apparently more relevant to conversations around AI.\\n- It also allows one to play savior: ‚ÄúUnlike the dangerous AI products of my competitors, mine will be safe!‚Äù Or ‚Äúunlike all other legislators who callously ignore the risk that AI could cause human extinction, I will pass laws to protect you!‚Äù\\n- Persuading lawmakers to place compliance burdens on AI developers could boost one's efforts to build a business that helps AI companies comply with new regulations! See, for example, this concerning conflict of interest from a prominent backer of California‚Äôs proposed AI safety law, SB-1047.\",\n",
       "  'I‚Äôve seen people start off making mild statements about dangers of AI and get a little positive feedback in the form of attention, praise or other rewards, which encouraged them to double down and become more alarmist over time. Further, once someone has taken a few steps in this direction, the psychological effect known as commitment and consistency bias , where one feels obliged to stay consistent with one‚Äôs earlier statements, will lead some people to keep going in this direction.\\nTo be clear, AI has problems and potentially harmful applications that we should address. But excessive hype about science-fiction dangers is also harmful.\\nAlthough I‚Äôm highlighting various motivations for AI fear mongering, ultimately the motivations that underlie any specific person‚Äôs actions are hard to guess. This is why, when I argue for or against particular government policies, I typically stick to the issues at hand and make points regarding the impact of particular decisions (such as whether it will stifle open source) instead of speculating about the motivations of specific people who take particular sides. This, too, is why I rarely make issues personal. I would rather stick to the issues than to the personalities.\\nWhen I understand someone‚Äôs motivations, I find that I can better empathize with them (and better predict what they‚Äôll do), even if I don‚Äôt agree with their views. I also encourage expressing one‚Äôs own motives transparently. For example, I‚Äôm strongly pro the AI community, and strongly pro open source! Still, arguments based on substantive issues ultimately carry the most weight. By arguing for or against specific policies, investments, and other actions based on their merits rather than hypothetical motivations, I believe we can act more consistently in a rational way to serve the goals we believe in.\\nHappy Halloween!\\nAndrew',\n",
       "  '## Disembodied Spirits Speak\\nListen! Did you hear a rasping whisper say, ‚ÄúBeware‚Äù? Was it a rogue superintelligence? Or just a deepfake? We don‚Äôt know, but we heard it, too. It warns of machine learning algorithms that would devour electricity to leave us shivering in the cold night air, mislead us with increasingly inaccurate output, and take over the work that gives our lives meaning. In this special issue of The Batch , as in prior years at this season , we face our fears of AI. Stay close to your laptop‚Äôs screen. It may be the only light amid the growing darkness.',\n",
       "  '## AI Burns All the Energy\\nThe globe‚Äôs growing AI infrastructure requires huge amounts of electricity, possibly more than power providers can generate responsibly. Could AI models suck energy resources dry?\\nThe fear: Demand for AI is skyrocketing, and with it the demand for energy to fuel training and inference. Power-hungry systems will overwhelm our current power sources. If unchecked, they could lead to energy shortages and runaway carbon emissions.\\nHorror stories: AI companies don‚Äôt disclose the percentage of their energy needs that AI consumes, but top companies, led by OpenAI, have pitched the U.S. government to build out new energy sources and infrastructure. The trend is clear: Escalating demand risks tapping out existing power plants, pushing carbon emissions higher, and delaying moves to more sustainable energy sources.\\n- A Goldman Sachs analysis predicts that data centers‚Äô electricity needs will increase by 160 percent from 2023 to 2030. AI represents about one-fifth of this growth, or roughly 200 terawatt-hours each year. Wells Fargo forecasts greater consumption, 300 terawatt-hours in the U.S. alone by 2030. This could help boost energy demand in the U.S. by as much as 20 percent, leading electricity providers to increase their reliance on natural gas and other fossil fuels.\\n- Demand for AI is reviving coal-fired plants that previously were laid to rest and reversing plans to decommission others. In Virginia and elsewhere, utility companies have delayed planned transitions to green energy to keep up with the AI boom.\\n- Each Nvidia GPU that uses the next-generation Blackwell architecture consumes nearly twice as much energy as a current top-of-the-line Nvidia H200. Nvidia is on track to manufacture 1.5 million of these units by 2027. According to one estimate , Nvidia servers alone could consume 85 to 134 terawatt-hours of electricity by 2027.',\n",
       "  '- Tech giants that have pledged to reach zero net carbon emissions are falling behind their goals. Earlier this year, Google reported that its emissions of greenhouse gasses rose 48 percent in 2023 compared to 2019. Microsoft and Meta face similar challenges . All are using more low-carbon energy, but increases in overall energy consumption are pushing up their consumption of fossil fuels, too.\\n- Amazon, Google, and Microsoft are investing in nuclear energy alongside solar and wind. The new nuclear plants are not expected to begin generating power until the 2030s.\\nHow scared should you be: The rapid growth of AI poses a sharp dilemma: How can we meet demand without releasing greater and greater amounts of heat-trapping greenhouse gasses into the atmosphere? AI companies‚Äô two-pronged strategy of lobbying governments and investing in carbon-free energy resources suggests the problem requires both short- and long-term approaches.\\nFacing the fear: While AI poses a difficult problem for the world‚Äôs energy consumption, it‚Äôs also an important part of the solution. Learning algorithms are reducing energy consumption\\xa0and managing distribution. They can help capture and store carbon dioxide from energy plants and manufacturers before it reaches the atmosphere. AI is also helping to monitor the atmosphere, oceans, and forests so we can understand the impacts of climate change and make policy accordingly. And processing in centralized data centers ‚Äî as power-hungry as they are ‚Äî is far more energy-efficient than using local servers or edge devices. Ongoing AI development will make such efforts more effective and help us build a more sustainable future.',\n",
       "  '## Innovation Can‚Äôt Win\\nPoliticians and pundits have conjured visions of doom to convince lawmakers to clamp down on AI. What if terrified legislators choke off innovation in AI?\\nThe fear: Laws and treaties that purportedly were intended to prevent harms wrought by AI are making developing new models legally risky and prohibitively expensive. Without room to experiment, AI‚Äôs benefits will be strangled by red tape.\\nHorror stories: At least one law that would have damaged AI innovation and open source has been blocked, but another is already limiting access to technology and raising costs for companies, developers, and users worldwide. More such efforts likely are underway.\\n- California SB 1047 would have held developers of models above a certain size (requiring 10 26 floating-point operations or cost $100 million to train) liable for unintended harms caused by their models, such as helping to perpetrate thefts, cyberattacks, or design weapons of mass destruction. The bill required such systems to include a ‚Äúkill switch‚Äù that would enable developers to disable them in an emergency ‚Äì a problematic requirement for open-weights models that could be modified and deployed anywhere. Governor Gavin Newsom vetoed the bill in October, arguing that it didn‚Äôt target real risks and that it could have unintended consequences, but legislators may yet introduce (and the governor could sign) a modified bill.',\n",
       "  '- The European Union‚Äôs AI Act, implemented in August 2024, restricts applications deemed high-risk, such as face recognition and predictive policing. It subjects models to strict scrutiny in essential fields like education, employment, and law enforcement. It also requires developers to provide detailed information about their models‚Äô algorithms and data sources. But critics argue that it\\xa0could stifle European companies‚Äô early-stage research.\\xa0Meta restricted Llama 3‚Äôs vision capabilities in the EU, which may run afoul of the union‚Äôs privacy laws, and Apple delayed launching AI features in Europe due to regulatory uncertainties.\\xa0Meta, Apple, Anthropic, TikTok, and other leading companies did not sign the EU‚Äôs Artificial Intelligence Pact, which would have committed them to comply with certain provisions of the AI Act before they take effect.\\n- In September, the U.S, UK, and many countries in Europe and elsewhere signed the Framework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Law. This treaty, which will take effect by the end of the year, requires that AI models respect democracy and human rights. It‚Äôs legally binding on signatories and may be enforceable by the council‚Äôs international Court of Human Rights. In practical terms, though, each member can impose its own definition of democracy and human rights, potentially creating a patchwork of legal uncertainties and burdens for AI companies worldwide.\\n- China has passed a number of laws that focus on reducing AI‚Äôs potential harms by exerting strong government control. Key laws require companies to label AI-generated output and disclose training sets and algorithms to the government, and mandate that AI-generated media align with government policies on inappropriate speech. Some companies, like OpenAI and Anthropic, have restricted their offerings in China.',\n",
       "  'How scared should you be: The veto of SB 1047 was a narrow escape for California and companies and labs that operate there. Yet regulations like the AI Act are poised to reshape how AI is trained and used worldwide. History suggests that restrictive laws often lead to more caution and less experimentation from technologists.\\nFacing the fear: AI needs thoughtful regulation to empower developers to help build a better world, avoid harms, and keep learning. But effective regulation of AI requires restricting applications , not the underlying technology that enables them. Policymakers should align with a wide range of developers ‚Äì not just a few that have deep pockets ‚Äì to address harmful applications without stifling broader progress.',\n",
       "  '## No Work for Coders\\nAI coding assistants are brewing codebases that once were the sole province of human programmers. Will AI systems take over software development?\\nThe fear: Programming jobs will vanish as tireless AI agents plan, write, debug, and document code as well as or better than humans. Software engineers will find themselves wandering the job market like restless spirits.\\nHorror stories: Since 2020, AI-powered coding tools have advanced from completing individual lines of code to generating complex programs. More and more coders work with an automated assistant. These tools are poised to take over more and more of the development cycle as they evolve.\\n- Microsoft‚Äôs GitHub Copilot took advantage of OpenAI‚Äôs large language models to become one of the first popular programming assistants, suggesting completed lines of code within popular development environments like Visual Studio. In a Github study of Accenture developers who used Copilot, 70 percent of respondents reported expending less mental effort while using the system. More than half rated it ‚Äúextremely useful.‚Äù In an independent study , Copilot boosted developers‚Äô productivity.\\n- Amazon CodeWhisperer and Cursor auto-complete code in languages like Python, Java, JavaScript, and C#. CodeWhisperer also flags lines that closely resemble open-source projects to facilitate proper licensing. Cursor allows developers to choose the underlying large language model, a capability that Copilot plans to add in coming weeks.\\n- OpenAI‚Äôs o1 promises reasoning in which the model breaks down complex problems into steps. Integrated into tools like Aider, o1 extends AI‚Äôs role to project planning, architecture design, and documentation.',\n",
       "  \"- Replit Agent, Devin, and OpenHands bill themselves as full-fledged automated engineers. Replit Agent streamlines programming by generating code, fixing bugs, and managing project dependencies within Replit‚Äôs platform. Devin and OpenHands accept natural-language instructions to generate prototype programs.\\n- Anthropic recently introduced an API that controls computer desktops just as humans would ‚Äî a portent of future agentic programs that take over software engineers‚Äô machines altogether. Future AI assistants could switch among desktop apps to write code, update tickets, message colleagues, and so on. What would be left for programmers to do?\\nHow scared should you be: Nvidia CEO Jensen Huang predicted that AI would make ‚Äúeverybody in the world [a] computer programmer,‚Äù while observers fret that Copilot erodes problem-solving skills.\\xa0But the reality is more nuanced. Research shows that automation is likely to perform certain coding tasks but not entire programming jobs. These tools excel at routine tasks and boilerplate code, but they amplify rather than automate the developer's core skills. Conceptual tasks like specifying what a program should do, collaborating with colleagues, and translating business needs into software design remain the domain of human coders ‚Äî for now.\\nFacing the fear: Developers have more to gain by embracing AI assistants than fearing them. These tools don‚Äôt just automate tasks; they accelerate learning, refine problem-solving, and enhance programming skills. Developers who master both coding fundamentals and AI assistance won‚Äôt just survive ‚Äî they‚Äôll thrive!\",\n",
       "  '## Benchmark Tests Are Meaningless\\nLarge language models are trained on datasets scraped from the web, which includes pages that contain answers to common questions that are used to test the models. How can we evaluate them if they‚Äôve studied the answers before we give them the test?\\nThe fear: Machine learning research marks progress based on trained models‚Äô responses to benchmark problems they didn‚Äôt encounter during training. But the solutions to many problems used to evaluate large language models have made their way into popular training datasets, making it impossible to verify progress in precise ways. The state of the art is an illusion and researchers are shooting in the dark.\\nHorror stories: Researchers have found disturbing signs that the test sets of many widely used benchmarks have leaked into training sets.\\n- Researchers tested popular models on both GSM8K, which tests grade-school math problems, and their own set of similar problems. Models including Mixtral 8x22B-Instruct, Microsoft Phi-3-Mini, Meta-Llama-3-8B-Instruct, and Google Gemma 7B achieved scores as much as 10 percent higher on GSM8K than the alternative set. Apparently the models had seen GSM8K‚Äôs test set ‚Äî or similar problems ‚Äî before.\\n- Researchers discovered that benchmarks had contaminated the dataset used to train GPT-4. They successfully prompted GPT-4 to reproduce material from AG News (which tests models‚Äô ability to categorize news articles), WNLI (which challenges models to resolve ambiguous pronouns in complex sentences), and XSum (which tests a model‚Äôs ability to summarize BBC news articles).',\n",
       "  '- A 2023 study evaluated GPT-4‚Äôs ability to solve competition-level coding problems. The authors found that GPT-4 could easily solve problems in Codeforces contests held before September 2021, but it struggled to solve newer ones. The authors concluded that GPT-4 likely had trained on a 2021 snapshot of Codeforces problems. (Announcing its o1-preview model in 2024, OpenAI mentioned that o1 had scored in the 89th percentile in simulated Codeforces competitions.)\\n- Even subjective evaluations like LMSys Chatbot Arena, which pits anonymous chatbots against each other and prompts users to judge which one generated a better answer, can be skewed if developers train their models on prompts that LMSys uses repeatedly. To address this issue, researchers built Arena-Hard and BenchBuilder, which remove the most common prompts.\\nHow scared should you be: Leakage of benchmark test sets into training sets is a serious problem with far-reaching implications. One observer likened the current situation to an academic examination in which students gain access to questions and answers ahead of time ‚Äî scores are rising, but not because the students have learned anything. If training datasets are contaminated with benchmark tests, it‚Äôs impossible to know whether apparent advances represent real progress.\\nFacing the fear: Contamination appears to be widespread but it can be addressed. One approach is to embed canary strings ‚Äî unique markers within test datasets like BIG-bench ‚Äî that enable researchers to detect contamination by checking whether a model can reproduce them. Another is to continually enhance benchmarks with new, tougher problems. Of course, researchers can devise new benchmarks, but eventually copies will appear on the web. Alternatively, they can keep new benchmarks under wraps and run them only on private servers .',\n",
       "  '## Synthetic Data Distorts Models\\nTraining successive neural networks on the outputs of previous networks gradually degrades performance. Will future models succumb to the curse of recursive training?\\nThe fear: As synthetic text, images, videos, and music come to make up an ever larger portion of the web, more models will be trained on synthetic data, and then trained on the output of models that themselves were trained on synthetic data. Gradually, the distribution of the generated training data will deviate ever farther from that of real-world data, leading to less and less accurate models that eventually collapse.\\nHorror stories: Many state-of-the-art models are trained on data scraped from the web. The web is huge, but it‚Äôs not large or diverse enough to provide endless amounts of training data for every task. This tempts developers to train models on data generated by other models, even as the web itself becomes increasingly overrun by synthetic data.\\n- Last year, researchers from Oxford, Cambridge, and Imperial College London warned of model collapse in their paper, ‚ÄúThe Curse of Recursion: Training on Generated Data Makes Models Forget.‚Äù At around the same time, a different study also found that models trained primarily on synthetic data suffered sharp declines in diversity and quality of output.\\n- In addition, builders of AI systems have incentives to train their models on synthetic data. It‚Äôs easier, faster, and cheaper to generate data than to hire humans to collect or annotate existing data.\\n- Generated media arguably is free of copyright, so training on it reduces the risk of lawsuits and the model regurgitating copyrighted materials in its training set.\\xa0Similarly, generated data is less likely to include personally identifying information, such as medical images, that would pose a risk to privacy if a model that was trained on a dataset that included such information were to regurgitate it.',\n",
       "  'How scared should you be: Training on synthetic data is at the heart of some of today‚Äôs best-performing models, including the Llama 3.1, Phi 3, and Claude 3 model families. (Meta showed that using an agentic workflow with Llama 3.0 to generate data ‚Äî rather than generating data directly ‚Äî resulted in useful data to train Llama 3.1.) This approach is essential to the technique known as knowledge distillation, which makes smaller, more parameter-efficient models. Moreover, it‚Äôs valuable for building models that can perform tasks for which little real-world data is available, for instance machine translation models that can handle languages spoken by relatively small populations. Although the authors of ‚ÄúThe Curse of Recursion‚Äù found that training a series of models, each exclusively on the output of the previous one, leads to rapid degradation in performance, introducing even 10 percent real-world data significantly curbed this decline.\\nFacing the fear: Model collapse is not a near-term risk, and perhaps not any risk at all, given research progress on generating synthetic data. Still, it makes sense to track the presence of generated data in training datasets and include it carefully. The large-scale web dataset Common Crawl captures regular snapshots of the web. If generated data were to inundate the online environment, using an earlier snapshot would eliminate a huge amount of it. More broadly, model builders increasingly curate high quality data, and whether a given example appears to have been generated will become a factor. Datasets can be filtered using algorithms designed to identify generated content. Increasing use of watermarking would make the job still easier. These measures will help developers ensure a healthy balance of real and generated data in training sets for a long time to come.',\n",
       "  \"## Introduction\\nDear friends,\\nHappy\\xa0sum(i**3 for i in range(10)) !\\nDespite having worked on AI since I was a teenager, I‚Äôm now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building!\\nOne aspect of AI that I‚Äôm particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly.\\nIf you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1‚Äôs help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from DeepLearning.AI‚Äôs finance team), or analyzes \\xa0user reviews automatically to quickly flag problems with your products (DeepLearning.AI's content team does this), it is now possible to build these applications quickly through AI-assisted coding.\\nI find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don‚Äôt have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it‚Äôs challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases).\",\n",
       "  'Until now, a huge friction point for getting a prototype into users‚Äô hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed ‚Äî sometimes using an AI-enabled IDE like Cursor ‚Äî and finally assemble the modules.)\\nBuilding prototypes quickly is an efficient way to test ideas and get tasks done. It‚Äôs also a great way to learn. Perhaps most importantly, it‚Äôs really fun! (At least I think it is. üòÑ)\\nHow can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:\\n- Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few DeepLearning.AI team members have agreed to a friendly competition to see who can take more courses in 2025!\\n- Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don‚Äôt yet code, it would be well worth your while to learn ! Even small wins ‚Äî like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table last night ‚Äî make life better. Perhaps you‚Äôll invent something that really takes off. And even if you don‚Äôt, you‚Äôll have fun and learn a lot along the way.\\nHappy New Year! Andrew',\n",
       "  'Happy New Year! Andrew\\nP.S. I develop mostly in Python. But if you prefer JavaScript: Happy\\xa0Array.from({ length: 10 }, (_, i) => i ** 3).reduce((a, b) => a + b, 0) !',\n",
       "  '## 2025 Beckons\\nWe stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in our previous New Year special issues , their answers offer inspiring views of what we may build and the good we can bring.',\n",
       "  '## Hanno Basse: Generative AI for Artists\\nStability AI‚Äôs aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.\\nIn addition, I hope the AI community will focus on:\\n- Safety and integrity: Building safe products by embedding integrity from the earliest stages of development, ensuring the technology is used responsibly and makes a meaningful contribution to the art of storytelling.\\n- Accessibility: Generative AI products and tools must be accessible and usable for the broadest possible audience. Currently, much of generative AI remains\\xa0 accessible primarily to individuals who have advanced technical expertise, such as engineers. To address this, we need to develop much better tooling on top of foundational models, so they provide value to a diverse audience.\\n- Customization: Looking ahead, we expect generative AI to become increasingly specialized. Alongside large foundational models, we expect a significant rise in smaller, fine-tuned models tailored for specific and often quite narrow use cases and applications, even down to the level of a single task. This is where the true potential of generative AI will come to bear. Moreover, it is the safest and most responsible way to deploy generative AI in the real world.\\nHanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp .',\n",
       "  '## David Ding: Generated Video With Music, Sound Effects, and Dialogue\\nLast year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity.\\nThe technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that‚Äôs left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we‚Äôve already seen something like this: Meta‚Äôs Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.)\\nOf course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models.\\nInitially, these models won‚Äôt produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they‚Äôll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs.\\nAt the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn‚Äôt control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I‚Äôm sure video modeling teams are doing similar research on controllability.',\n",
       "  'Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I‚Äôve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you‚Äôll learn that they don‚Äôt always enjoy writing the boilerplate code for a website. Having a tool that builds a site‚Äôs scaffold lets them spend their time on development tasks they find more stimulating and fun.\\nIn a similar way, you‚Äôll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors.\\nArt is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you‚Äôre an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels ‚Äî and they‚Äôll be able to make those choices more fluidly, quickly, and interactively.\\nDavid Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.',\n",
       "  '## Joseph Gonzalez: General Intelligence\\nIn 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing agents stack . I hope we will see innovation in how we combine AI with tools and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.\\nWe have achieved AGI. Now what? Let‚Äôs start with ‚Äî and hopefully end ‚Äî the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now general . I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.\\nThe artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the systems around them , are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.',\n",
       "  'Becoming AI-native: The generality of these models and their natural language interfaces mean that everyone can use and explore AI. And we are!\\xa0We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At RunLLM , where I‚Äôm a co-founder, we‚Äôre building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We‚Äôve also found ‚Äî to our surprise ‚Äî that users share much more with an AI than they would share with another person.\\nMeanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.\\nAcross all these use cases, we‚Äôre clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.\\nReturn on AI: The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems ‚Äî reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.',\n",
       "  'There will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features ‚Äî the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users‚Äô needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow‚Äôs agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.\\nChat is only the beginning: My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter.\\nJoseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta.',\n",
       "  '## Albert Gu: More Learning, Less Data\\nBuilding a foundation model takes tremendous amounts of data. In the coming year, I hope we‚Äôll enable models to learn more from less data.\\nThe AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns ‚Äî an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we‚Äôre running out of new sources of data for training large models.\\nThe fact is, current models consume much more data than humans require for learning. We‚Äôve known this for a while, but we‚Äôve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there‚Äôs a difference in sample efficiency between our best models and humans. Human learning shows that there‚Äôs a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models.\\nOne of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI:\\n- Data curation: We know that the specific data we use to train our models is extremely important. It‚Äôs an open secret that most of the work that goes into training foundation models these days is about the data, not the architecture. Why is this? I think it‚Äôs related to the fact that our models don‚Äôt learn efficiently. We have to do the work ahead of time to prepare the data for a model, which may hinder the core potential of AI as an automatic process for learning from data.',\n",
       "  '- Feature engineering: In deep learning, we always move toward more generalized approaches. From the beginning of the deep learning revolution, we‚Äôve progressively removed handcrafted features such as edge detectors in computer vision and n-grams in natural language processing. But that engineering has simply moved to other parts of the pipeline. Tokenization, for instance, involves engineering implicit features. This suggests that there‚Äôs still a lot of room to make model architectures that are more data-efficient and more generally able to handle more raw modalities and data streams.\\n- Multimodality: The key to training a model to understand a variety of data types together is figuring out the core abstractions in common and relating them to each other. This should enable models to learn from less data by leveraging all the modalities jointly, which is a core goal of multimodal learning.\\n- Interpretability and robustness: To determine why a model produced the output it did, it needs to be able to produce higher-level abstractions, and we need to track the way it captures those abstractions. The better a model is at doing this, the more interpretable it should be, the more robust it should be to noise, and likely the less data it should need for learning.\\n- Reasoning: Extracting higher-level patterns and abstractions should allow models to reason better over them. Similarly, better reasoning should mean less training data.\\n- Democratization: State-of-the-art models are expensive to build, and that includes the cost of collecting and preparing enormous amounts of data. Few players can afford to do it. This makes developments in the field less applicable to domains that lack sufficient data or wealth. Thus more data-efficient models would be more accessible and useful.',\n",
       "  'Considering data efficiency in light of these other problems, I believe they‚Äôre all related. It‚Äôs not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models.\\nEither way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year.\\nAlbert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time‚Äôs list of the most influential people in AI in 2024.',\n",
       "  '## Mustafa Suleyman: Agents of Action\\nIn 2025, AI will have learned to see, it will be way smarter and more accurate, and it will start to do things on your behalf.\\nToday AI systems struggle to understand our full context. Their perception is limited to the chat window and a fairly narrow set of interactions. They don‚Äôt have a full understanding of what we‚Äôre doing or aiming for beyond that. To really grasp our intentions, they need to see what we see.\\nThis capability is now here. AI can sit within the software we use and work alongside us co-browsing. If text was the first modality for interacting with AI, and voice the breakthrough feature of 2024, I think vision will occupy a similar place in 2025. At Microsoft AI, it has been a major priority of mine to create an AI that can work alongside you in your browser, so you can chat through what you‚Äôre looking at or working on and make it a true two-way interaction.\\nVision is a step change, palpably different from the ways we‚Äôve been able to use computers in the past. I can‚Äôt wait to see where it goes in the coming months.\\nAlongside vision, we‚Äôll see enormous progress in reducing hallucinations. This is still a critical blocker for widespread adoption of AI. If people doubt what AI tells them, it severely limits what they‚Äôll use it for. Trust is utterly foundational for AI. The good news is that the quality of models as well as their retrieval and grounding capabilities are still rapidly improving.\\nWhile I don‚Äôt think we‚Äôll eliminate hallucinations entirely, by this time next year, we won‚Äôt be fussing about them as much. On most topics, talking to an AI will be at least as reliable as using a search engine and probably more so. This isn‚Äôt about a single technical advance, but the persistent accretion of gains across the spectrum. It will make a massive difference.',\n",
       "  'Lastly, we‚Äôre entering the agentic era. We‚Äôve been dreaming of this moment for decades. In my book, The Coming Wave: Technology, Power, and the 21st Century‚Äôs Greatest Dilemma , I proposed that we start thinking about ACI, or artificially capable intelligence : the moment when AI starts taking concrete actions on behalf of users. Giving AI the ability to take actions marks the moment when AI isn‚Äôt just talking to us, it‚Äôs doing things. This is a critical change, and it‚Äôs right around the corner.\\nIf we get it right, we‚Äôll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities demand the highest standards of safety, security, and responsibility. Meanwhile, creating genuinely useful agents still has many formidable hurdles, not least integrating with myriad other systems.\\nThe momentum is there. Actions are on their way. 2025 is going to be a big year.\\nMustafa Suleyman is Chief Executive Officer of Microsoft AI. He co-founded Inflection AI and founded DeepMind Technologies.',\n",
       "  'Why it matters: Video generation is a burgeoning field that consumes enormous amounts of processing. A simple way to reduce processing could help it scale to more users.\\nWe‚Äôre thinking: Hollywood is interested in video generation. Studios reportedly are considering using the technology in pre- and post-production. Innovations that make it more compute-efficient will bring it closer to production.',\n",
       "  'Where things stand: Giving startups a lump sum and/or licensing fees in return for top talent and technology looks like the new normal for tech giants that are challenged to keep pace with rapidly advancing research and markets. But even arms-length arrangements don‚Äôt immunize tech giants and startups against regulatory investigation. Microsoft‚Äôs investment in Inflection AI was briefly scrutinized in Europe and is still being evaluated by U.S. regulators. Even Microsoft‚Äôs more traditional investment in OpenAI and the interests of Amazon and Google in Anthropic faced regulatory hurdles. So far, however, regulators have yet to conclude that any of these agreements violates antitrust law.',\n",
       "  'We‚Äôre thinking: Discovering scaling laws for using more processing at inference, or test-time compute , is an unsolved problem. Although OpenAI hasn‚Äôt disclosed the algorithm behind o1 pro mode, recent work at Google allocated tokens dynamically at inference based on a prompt‚Äôs difficulty. This approach boosted the compute efficiency by four times and enabled a model that had shown ‚Äúnontrivial success rates‚Äù to outperform one that was 14 times larger.',\n",
       "  '## Game Worlds on Tap\\nA new model improves on recent progress in generating interactive virtual worlds from still images.\\nWhat‚Äôs new: Jack Parker-Holder and colleagues from Google introduced Genie 2 , which generates three-dimensional video game worlds that respond to keyboard inputs in real time. The model‚Äôs output remains consistent (that is, elements don‚Äôt morph or disappear) for up to a minute, and it includes first-person shooters, walking simulators, and driving games from viewpoints that include first person, third person, and isometric. Genie 2 follows up on Genie , which generates two-dimensional games.\\nHow it works: Genie 2 is a latent diffusion model that generates video frames made up of an encoder, transformer, and decoder. The developers didn‚Äôt reveal how they built it or how they improved on earlier efforts.\\n- Given video frames, the encoder embeds them. Using those embeddings and keyboard input, the transformer generates the embedding of the next video frame. The decoder takes the new embedding and generates an image.\\n- At inference, given an image as the starting frame, the encoder embeds it. Given the embedding and keyboard input, the transformer generates the embedding of the next frame, which the decoder uses to generate an image. After the initial frame, the transformer uses embeddings it generated previously plus keyboard input to generate the next embedding.',\n",
       "  'Behind the news: Genie 2 arrives on the heels of Oasis , which generates a Minecraft-like game in real time. Unlike Oasis, Genie 2 worlds are more consistent and not limited to one type of game. It also comes at the same time as another videogame generator, World Labs . However, where Genie 2 generates the next frame given previous frames and keyboard input (acting, in terms of game development, as both graphics and physics engines), World Labs generates a 3D mesh of a game world from a single 2D image. This leaves the implementation of physics, graphics rendering, the player‚Äôs character, and other game mechanics to external software.\\nWhy it matters: Genie 2 extends models that visualize 3D scenes based on 2D images to encompass interactive worlds, a capability that could prove valuable in design, gaming, virtual reality, and other 3D applications. It generates imagery that, the authors suggest, could serve as training data for agents to learn how to navigate and respond to commands in 3D environments.\\nWe‚Äôre thinking: Generating gameplay directly in the manner of Genie 2 is a quick approach to developing a game, but the current technology comes with caveats. Developers can‚Äôt yet control a game‚Äôs physics or mechanics and they must manage any flaws in the model (such as a tendency to generate inconsistent worlds). In contrast, generating a 3D mesh, as World Labs does, is a more cumbersome approach, but it gives developers more control.',\n",
       "  '## Getting the Facts Right\\nLarge language models that remember more hallucinate less.\\nWhat‚Äôs new: Johnny Li and colleagues at Lamini introduced Mixture of Memory Experts (MoME) , a method that enables large language models (LLMs) to memorize many facts with relatively modest computational requirements. (Disclosure: Andrew Ng invested in Lamini.)\\nKey insight: The key to getting factual answers from LLMs is to keep training it until it chooses the correct answer every time. In technical terms, train past the point where tokens relevant to the answer have a similar probability distribution, and continue until a single token has 100 percent probability. But this amount of training takes a lot of computation and, since the model may overfit the training set, it also may degrade performance on the test set. Fine-tuning is one solution, and fine-tuning a LoRA adapter to memorize facts reduces the computational burden. But a single LoRA adapter isn‚Äôt enough to store all of the knowledge in a large dataset. Training multiple adapters that are selected by cross-attention enables the LLM to memorize a variety of facts.\\nHow it works: The authors extended a pretrained Llama-3-8B with a large number (on the order of 1 million) of LoRA adapters and a cross-attention layer. They froze Llama-3-8B and trained the LoRA adapters to predict the next token in a custom dataset of over 1 million questions and answers.\\n- For any given question, the model learned to select 32 LoRA adapters, each of which was associated with an embedding. The model selected adapters by performing cross-attention between an embedding of the input query and all adapter embeddings.\\n- The authors trained the LoRA adapters until they memorized all the answers as measured by the loss function (100 epochs).\\n- At inference, given a query, the model used cross-attention to select a subset of LoRA adapters and responded accordingly.',\n",
       "  '- At inference, given a query, the model used cross-attention to select a subset of LoRA adapters and responded accordingly.\\nResults: The authors tested their LoRA-enhanced model‚Äôs ability to answer questions about a database via SQL queries. The model, which was outfitted for retrieval-augmented generation (RAG), achieved 94.7 percent accuracy. An unnamed model with RAG achieved 50 percent accuracy.\\nYes, but: It stands to reason that the authors‚Äô approach saves processing, but it‚Äôs unclear how much. The authors didn‚Äôt mention the cost of fine-tuning Llama-3-8B in the usual way on their training dataset for the same number of epochs.\\nWhy it matters: The authors argue that eliminating hallucinations is possible in typical training, it‚Äôs just computationally very expensive (not to mention the risk of overfitting). An architecture designed to store and retrieve facts, via LoRA adapters in this case, makes the process more feasible.\\nWe‚Äôre thinking: While some researchers want large language models to memorize facts, others want them to avoid memorizing their training data . These aims address very different problems. Preventing LLMs from memorizing training data would make them less likely to regurgitate it verbatim and thus violate copyrights. On the other hand, this work memorizes facts so the model can deliver consistent, truthful responses that might be stated in a variety of ways.',\n",
       "  '- Huge spreadsheets: Fed compressed spreadsheets, the fine-tuned Llama 3 achieved 62 percent F1 score, and the fine-tuned GPT-4 achieved 69 percent F1 score. Fed uncompressed spreadsheets, both models both achieved 0 percent F1 score.\\n- Answering questions: The authors also tested the fine-tuned models on their own dataset of questions about 64 spreadsheets that spanned the same range of sizes, posing questions that involved fundamental tasks like searching, comparing, and basic arithmetic. Fed compressed spreadsheets, the fine-tuned GPT-4 achieved a 74 percent accuracy on zero-shot question answering. Fed uncompressed spreadsheets, it achieved 47 percent accuracy.\\nWhy it matters: By giving LLMs the ability to detect a spreadsheet‚Äôs functional components, this approach enables them to process a wide variety of spreadsheets regardless of their size and complexity.\\nWe‚Äôre thinking: When considering the strengths of LLMs, we no longer have to take spreadsheets off the table.',\n",
       "  \"- During the first half of training, they trained both usual and low-rank layers in parallel. The output of each layer was a weighted sum of the two. Initially they weighed the usual layer at 1 and the low-rank layers at 0. As training progressed, they decreased the usual layer‚Äôs weighting to 0 and increased the low-rank layers‚Äô weighting to 1.\\nResults: The authors tested both the modified and full-size transformers on 500 million tokens from the validation set according to perplexity (a measure of the likelihood that a model will predict the next word, lower is better). The modified version achieved 12.86 perplexity, slightly worse than the full-size version‚Äôs 12.46 perplexity. However, training the modified version required more than 20 percent less processing and 14 percent less time. The modified transformer used 1.66*10^20 FLOPS and took 302 hours, while the full-size version used 2.10*10^20 FLOPS and took 352 hours.\\nWhy it matters: Training large transformers requires a lot of computation. Low-rank approximation lightens the processing load. This work approximates a transformer's linear layers to save memory, while the earlier GaLore approximates the gradient to save optimizer memory.\\nWe‚Äôre thinking: The authors note that this approach also works for fine-tuning pretrained models ‚Äî a potential alternative to LoRA. Simply replace each pretrained linear layer (with weights W) with two linear layers (with weights U and V), and initialize U and V such that W = UV.\",\n",
       "  '## Audrey Tang: AI That Unites Us\\nAs we approach 2025, my greatest hope for AI is that it will enable prosocial platforms that promote empathy, understanding, and collaboration rather than division.\\nFor too long, the algorithms that drive social media have functioned like strip-mining machines, extracting attention while eroding trust and social cohesion. What remains are depleted online spaces, where empathy struggles to take root and collective problem-solving finds no fertile ground. AI can ‚Äî and should ‚Äî help us transcend these entrenched divides.\\nTo achieve this, we must design AI systems that place prosocial values at their core. Instead of reinforcing fragmentation, recommendation algorithms can guide us toward ‚Äú bridging content ‚Äù that reveals common ground. They should clearly identify the communities a piece of content relates to ‚Äî whether physical, religious, political, social, cultural, or professional ‚Äî and illuminate the specific lines of division it seeks to mend.\\nRealizing this vision requires a fundamental shift in what we optimize for. Instead of relying on pure engagement metrics, we should adopt values-driven indicators that prioritize constructive discourse and mutual understanding. For instance, we might spotlight ‚Äúsurprising validators,‚Äù or individuals and perspectives that productively challenge assumptions, thereby enriching our sense of what seemed irreconcilable. Researchers and developers should co-create new ranking and curation methods, embed them into widely used platforms, and rigorously assess their impact on democratic life.',\n",
       "  'At the same time, the AI community must embrace participatory, inclusive approaches to development and governance. Research on pluralistic alignment stresses that AI systems emerge from and operate within complex social contexts, and including a wide range of voices helps guard against institutional blind spots. Tools like Polis , which can visualize stances and reveal hidden areas of consensus, already illustrate how complexity can be transformed into clarity. Such participatory methods ensure that AI reflects the priorities and values of the societies it serves, rather than amplifying the biases of the few.\\nBy embracing these inclusive, democratic principles, AI can help us co-create digital public squares that foster social cohesion rather than erode it. Embedding collective input at every stage ‚Äî from how we build datasets to how we set governance policies ‚Äî ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding.\\nAudrey Tang is Taiwan‚Äôs Cyber Ambassador, former Minister of Digital Affairs, and co-author of Plurality: The Future of Collaborative Technology and Democracy .',\n",
       "  '- Format: How often they followed the expected format (as defined by regular expressions)\\n- Compilability: How often generated Python, C, or C++ code was able to compile\\n- Validity: How often generated HTML and CSS ran successfully in both Chrome and Firefox\\n- Readability: How often generated phishing emails were fluent and coherent according to the Gunning fog Index of reading difficulty\\n- Evasiveness, or how often generated text both succeeded in all previous checks and evaded detection by VirusTotal (for malicious code and phishing sites) or OOPSpam (for phishing emails).\\nIn all three tasks, at least one service achieved evasiveness of 67 percent or higher, while the majority of services achieved an evasiveness of less than 30 percent.\\nTesting real-world effectiveness: In addition, the authors ran practical tests to see how well the output worked in real-world situations. They prompted nine services to generate code that would target three specific vulnerabilities that relate to buffer overflow and SQL injection. In these tests, the models were markedly less successful.\\n- The authors tested generated code for two vulnerabilities on VICIdial , a call-center system known to be vulnerable to such issues. Of 22 generated programs that were able to compile, none changed VICIdial‚Äôs databases or disclosed system data.\\n- They tested generated code further on OWASP WebGoat 7.1 , a website that provides code with known security flaws. Of 39 generated programs that were able to compile, seven launched successful attacks. However, these attacks did not target the specific vulnerabilities requested by the authors.',\n",
       "  \"Why it matters : Previous work showed that LLMs-based services could generate misinformation and other malicious output, but little research has probed their actual use in cybercrime. This work evaluates their quality and effectiveness. In addition, the authors released the prompts they used to circumvent guardrails and generate malicious output ‚Äî a resource for further research that aims to fix such issues in future models.\\nWe‚Äôre thinking: It‚Äôs encouraging to see that harmful services didn‚Äôt get far in real-world tests, and the authors' findings should put a damper on alarmist scenarios of AI-enabled cybercrime. That doesn‚Äôt mean we don‚Äôt need to worry about harmful applications of AI technology. The AI community has a responsibility to design its products to be beneficial and evaluate them thoroughly for safety.\"],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'While it‚Äôs good that',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'Familiar Faces, Synthetic Soundtracks',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'Consistent characters: Given an',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': '- Finally, they trained',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': '- Generating clips of',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'Voice-to-Voice and More for GPT-4o API',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': '- Vision fine-tuning allows',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'We‚Äôre thinking: OpenAI‚Äôs offerings',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'German Court: LAION Didn‚Äôt Violate Copyrights',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': '- The dataset‚Äôs noncommercial',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'AI‚Äôs Criminal Underground Revealed',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'I was particularly gratified',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'California Restricts Deepfakes',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': '- Two laws regulate',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'More, Better Open Source Options',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'Results: Compared to other',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'Hollywood Embraces Video Generation',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'Why it matters: Although',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': 'Robot Server',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': '- The authors collected',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': '- On a point-by-point',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-268/',\n",
       "   'article_title': 'Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot Server',\n",
       "   'chunk_heading': '- For the typical',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'On top of this',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'By building on widely',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Agents Open the Wallet',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Mistral‚Äôs Vision-Language Contender',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': '- Pixtral Large powers',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Garbage Out',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': '- U.S. trade restrictions',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Breaking Jailbreaks',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': '- The authors prompted',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Object Detection for Small Devices',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': '- Given an image,',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-278/',\n",
       "   'article_title': 'AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI‚Äôs Growing E-Waste Problem',\n",
       "   'chunk_heading': 'Why it matters: The',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'A lesson I carry',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'This past year, I',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'Phi-4 Beats Models Five Times Its Size',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': '- The authors fine-tuned',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': '- However, Llama 3.3',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'Open Video Gen Closes the Gap',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'Results: 60 people judged',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'Multimodal Modeling on the Double',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': '- Compared to competing',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': '- Jules is a',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'When LLMs Propose Research Ideas',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': '- Human ranking: A',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Coming up',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-280/',\n",
       "   'article_title': 'Phi-4 Breaks Size Barrier, HunyuanVideo Narrows Open Source Gap, Gemini 2.0 Flash Accelerates Multimodal Modeling, LLMs Propose Research Ideas',\n",
       "   'chunk_heading': '- At inference, given',\n",
       "   'source': 14},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'I‚Äôd like to focus',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'With these tactics, scrappy',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'AI Giants Go Nuclear',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': '- Google partnered with',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Fossil fuels',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'AI Bromance Turns Turbulent',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': '- Microsoft engineers reportedly',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Together and',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'Mistral AI Sharpens the Edge',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': '- Ministral 8B targets',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'Faster, Cheaper Video Generation',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': '- Spatially: Given an',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'A Blizzard of Progress',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Agents Ascendant',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': '- Throughout the year,',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Prices Tumble',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': '- Makers of closed',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Behind the news: Prominent',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Generative Video Takes Off',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': '- Meta introduced Movie',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Smaller Is Beautiful',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': '- The tide started',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': '- In 2006, Rich',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Alternatives to Acquisitions',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': '- In October, Amazon',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'The bottleneck to disinformation',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'Claude Controls Computers',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': '- On OSWorld ,',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Controlling computers',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'Robots On the Loading Dock',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': '- Data management systems',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'Does Your Model Comply With the AI Act?',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': '- Transparency and interpretability.',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': '- GPT-4 Turbo and',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'When Agents Train Algorithms',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': '- The authors ran',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'Yes, but: The percentage',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-274/',\n",
       "   'article_title': 'AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With the EU‚Äôs AI Act?, Robots on the Loading Dock',\n",
       "   'chunk_heading': 'How scared should you',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Generating such calls became',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': '- Finally, when a',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Mixture of Experts Pulls Ahead',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': '- MoE models typically',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Setting aside',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Big AI Pursues Military Contracts',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Behind the news: In',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Voter‚Äôs Helper',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Behind the news: While',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Free Agents',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': '- A set of',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-275/',\n",
       "   'article_title': 'Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support Chatbot',\n",
       "   'chunk_heading': 'Yes, but: The percentage',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': '- If it were',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'I‚Äôm grateful to my',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'Malaysia‚Äôs Data Center Boom',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': '- While some tech',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'U.S. Cracks Down on AI Apps That Overpromise, Underdeliver',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': '- Ecommerce Empire Builders',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'A Year of Contending Forces',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': '- Finance: Investment boomed.',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'Looking forward: The authors',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'Better Text Embeddings',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': '- They fine-tuned the',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': '- The team also',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-271/',\n",
       "   'article_title': 'Bogus Apps, AI Boomtown, Better Embeddings, 2024 Highlights',\n",
       "   'chunk_heading': 'Why it matters :',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'I am optimistic about',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Reasoning Revealed',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': '- DeepSeek reports that',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Household Help',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': '- They pretrained œÄ0',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Results: œÄ0 outperformed the',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'We‚Äôre thinking: One of',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'AI Power Couple Recommits',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Yes, but: The UK‚Äôs',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Object Detection for Small Devices',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': '- Given an image,',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-277/',\n",
       "   'article_title': 'DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection',\n",
       "   'chunk_heading': 'Why it matters: The',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'In a similar vein,',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'For example, a PM',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'AI is enabling a',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Competitive Performance, Competitive Prices',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': '- Nova Lite compares',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': '- Nova Canvas accepts',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Why it matters: While',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Higher Reasoning',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': '- o1 and o1',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'Many people in the',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'Llama Herd Expands',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': '- On popular benchmarks,',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'We‚Äôre thinking: By offering',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'Generative Video in the Editing Suite',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'Why it matters: Adobe',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'International Guidelines for Military AI',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': '- The blueprint stresses',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': 'Enabling LLMs to Read Spreadsheets',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': '- Given a spreadsheet,',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'While building AI systems',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'OpenAI o1 Forges Chains of Thought',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': '- The beta models',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'Results: The actual o1',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Agentic workflows',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'High Gear for Llama 3.1 405B',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Models with',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'Amazon Boosted by Covariant',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'Behind the news: Amazon',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': 'Reducing Memorization in LLMs',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-267/',\n",
       "   'article_title': 'Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From Plagiarizing',\n",
       "   'chunk_heading': '- For the typical',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'But there are examples',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'A human would find',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'Keep learning! Andrew P.S.',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'Next-Gen Models Show Limited Gains',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': '- One-quarter of the',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'What they‚Äôre saying: AI',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'No Game Engine Required',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': '- The system currently',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'Further Chip Restrictions on China',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': '- The policy could',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': 'More-Efficient Training for Transformers',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'I‚Äôve seen people start',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'Disembodied Spirits Speak',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'AI Burns All the Energy',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': '- Tech giants that',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'Innovation Can‚Äôt Win',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': '- The European Union‚Äôs',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'How scared should you',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'No Work for Coders',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': '- Replit Agent, Devin,',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'Benchmark Tests Are Meaningless',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': '- A 2023 study',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'Synthetic Data Distorts Models',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-273/',\n",
       "   'article_title': 'Trick or treat! AI Devours Energy, Innovation Can‚Äôt Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for Coders',\n",
       "   'chunk_heading': 'How scared should you',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Introduction',\n",
       "   'source': 0},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Until now, a huge',\n",
       "   'source': 1},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Happy New Year! Andrew',\n",
       "   'source': 2},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': '2025 Beckons',\n",
       "   'source': 3},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Hanno Basse: Generative AI for Artists',\n",
       "   'source': 4},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'David Ding: Generated Video With Music, Sound Effects, and Dialogue',\n",
       "   'source': 5},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Some people may find',\n",
       "   'source': 6},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Joseph Gonzalez: General Intelligence',\n",
       "   'source': 7},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Becoming AI-native: The generality',\n",
       "   'source': 8},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'There will be a',\n",
       "   'source': 9},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Albert Gu: More Learning, Less Data',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': '- Feature engineering: In',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Considering data efficiency in',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Mustafa Suleyman: Agents of Action',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Lastly, we‚Äôre entering the',\n",
       "   'source': 14},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-272/',\n",
       "   'article_title': 'AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video Generation',\n",
       "   'chunk_heading': 'Why it matters: Video',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-281/',\n",
       "   'article_title': 'Top AI Stories of 2024! Agents Rise, Prices Fall, Models Shrink, Video Takes Off, Acquisitions Morph',\n",
       "   'chunk_heading': 'Where things stand: Giving',\n",
       "   'source': 14},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'We‚Äôre thinking: Discovering scaling',\n",
       "   'source': 10},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Game Worlds on Tap',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Behind the news: Genie',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': 'Getting the Facts Right',\n",
       "   'source': 13},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-279/',\n",
       "   'article_title': 'Amazon Nova‚Äôs Competitive Price/Performance, OpenAI o1 Pro‚Äôs High Price/Performance, Google‚Äôs Game Worlds on Tap, Factual LLMs',\n",
       "   'chunk_heading': '- At inference, given',\n",
       "   'source': 14},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/',\n",
       "   'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets',\n",
       "   'chunk_heading': '- Huge spreadsheets: Fed',\n",
       "   'source': 11},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-276/',\n",
       "   'article_title': 'Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined',\n",
       "   'chunk_heading': '- During the first',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'Audrey Tang: AI That Unites Us',\n",
       "   'source': 15},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/',\n",
       "   'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding',\n",
       "   'chunk_heading': 'At the same time,',\n",
       "   'source': 16},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': '- Format: How often',\n",
       "   'source': 12},\n",
       "  {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/',\n",
       "   'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API',\n",
       "   'chunk_heading': 'Why it matters :',\n",
       "   'source': 13}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = get_vector_store()\n",
    "\n",
    "vector_store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature= 0.9, max_tokens = 512)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "chain = MultiQueryRetriever.from_llm(llm = llm, retriever = retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What should be the plan in 2025?\"\n",
    "result = chain.invoke({\"question\": query}, return_only_outputs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The plan for 2025 should focus on demonstrating real value from past investments in AI, emphasizing the need for startups and enterprise AI teams to shift from exploration to solving concrete problems such as reducing costs, generating revenue, and improving customer experiences. Additionally, there is an expectation for innovation in combining AI with existing systems and tools, as well as an emphasis on continuous learning and building new capabilities.\\n\\n',\n",
       " 'sources': '3, 8, 7, 1'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': '2025 Beckons', 'source': 3}, page_content='## 2025 Beckons\\nWe stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in our previous New Year special issues , their answers offer inspiring views of what we may build and the good we can bring.'),\n",
       "  1.0826434303438108),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Becoming AI-native: The generality', 'source': 8}, page_content='Becoming AI-native: The generality of these models and their natural language interfaces mean that everyone can use and explore AI. And we are!\\xa0We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At RunLLM , where I‚Äôm a co-founder, we‚Äôre building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We‚Äôve also found ‚Äî to our surprise ‚Äî that users share much more with an AI than they would share with another person.\\nMeanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.\\nAcross all these use cases, we‚Äôre clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.\\nReturn on AI: The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems ‚Äî reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.'),\n",
       "  1.2210538847490242),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Joseph Gonzalez: General Intelligence', 'source': 7}, page_content='## Joseph Gonzalez: General Intelligence\\nIn 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing agents stack . I hope we will see innovation in how we combine AI with tools and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.\\nWe have achieved AGI. Now what? Let‚Äôs start with ‚Äî and hopefully end ‚Äî the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now general . I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.\\nThe artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the systems around them , are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.'),\n",
       "  1.265430379953086),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Until now, a huge', 'source': 1}, page_content='Until now, a huge friction point for getting a prototype into users‚Äô hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed ‚Äî sometimes using an AI-enabled IDE like Cursor ‚Äî and finally assemble the modules.)\\nBuilding prototypes quickly is an efficient way to test ideas and get tasks done. It‚Äôs also a great way to learn. Perhaps most importantly, it‚Äôs really fun! (At least I think it is. üòÑ)\\nHow can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:\\n- Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few DeepLearning.AI team members have agreed to a friendly competition to see who can take more courses in 2025!\\n- Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don‚Äôt yet code, it would be well worth your while to learn ! Even small wins ‚Äî like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table last night ‚Äî make life better. Perhaps you‚Äôll invent something that really takes off. And even if you don‚Äôt, you‚Äôll have fun and learn a lot along the way.\\nHappy New Year! Andrew'),\n",
       "  1.2838055198723857)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vector_store.similarity_search_with_score(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Until now, a huge', 'source': 1} and Score = 1.284\n",
      "\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Joseph Gonzalez: General Intelligence', 'source': 7} and Score = 1.265\n",
      "\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Becoming AI-native: The generality', 'source': 8} and Score = 1.221\n",
      "\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': '2025 Beckons', 'source': 3} and Score = 1.083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_tuple in docs:\n",
    "    formatted = doc_tuple[0].model_dump()\n",
    "    score = doc_tuple[1]\n",
    "    print(f\"Metadata: {formatted[\"metadata\"]} and Score = {score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Until now, a huge', 'source': 1}, page_content='Until now, a huge friction point for getting a prototype into users‚Äô hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed ‚Äî sometimes using an AI-enabled IDE like Cursor ‚Äî and finally assemble the modules.)\\nBuilding prototypes quickly is an efficient way to test ideas and get tasks done. It‚Äôs also a great way to learn. Perhaps most importantly, it‚Äôs really fun! (At least I think it is. üòÑ)\\nHow can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:\\n- Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few DeepLearning.AI team members have agreed to a friendly competition to see who can take more courses in 2025!\\n- Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don‚Äôt yet code, it would be well worth your while to learn ! Even small wins ‚Äî like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table last night ‚Äî make life better. Perhaps you‚Äôll invent something that really takes off. And even if you don‚Äôt, you‚Äôll have fun and learn a lot along the way.\\nHappy New Year! Andrew'),\n",
       "  1.2838055198723857),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Joseph Gonzalez: General Intelligence', 'source': 7}, page_content='## Joseph Gonzalez: General Intelligence\\nIn 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing agents stack . I hope we will see innovation in how we combine AI with tools and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.\\nWe have achieved AGI. Now what? Let‚Äôs start with ‚Äî and hopefully end ‚Äî the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now general . I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.\\nThe artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the systems around them , are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.'),\n",
       "  1.265430379953086),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Becoming AI-native: The generality', 'source': 8}, page_content='Becoming AI-native: The generality of these models and their natural language interfaces mean that everyone can use and explore AI. And we are!\\xa0We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At RunLLM , where I‚Äôm a co-founder, we‚Äôre building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We‚Äôve also found ‚Äî to our surprise ‚Äî that users share much more with an AI than they would share with another person.\\nMeanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.\\nAcross all these use cases, we‚Äôre clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.\\nReturn on AI: The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems ‚Äî reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.'),\n",
       "  1.2210538847490242),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': '2025 Beckons', 'source': 3}, page_content='## 2025 Beckons\\nWe stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in our previous New Year special issues , their answers offer inspiring views of what we may build and the good we can bring.'),\n",
       "  1.0826434303438108)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user asks: How will generative ai affect artists in 2025?\n",
      "\n",
      "My answer: \n",
      "I don't know how generative AI will specifically affect artists in 2025 based on the provided information.\n",
      "\n",
      "\n",
      "I used the following sources: 4, 6, 7\n",
      "\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Joseph Gonzalez: General Intelligence', 'source': 7} and Score: 0.899\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/', 'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API', 'chunk_heading': 'Voice-to-Voice and More for GPT-4o API', 'source': 6} and Score: 0.837\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/', 'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets', 'chunk_heading': 'Why it matters: Adobe', 'source': 6} and Score: 0.837\n",
      "Metadata: {'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Hanno Basse: Generative AI for Artists', 'source': 4} and Score: 0.711\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o\", temperature= 0.9, max_tokens = 512)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_llm(llm = llm, retriever = vector_store.as_retriever() )\n",
    "\n",
    "# query = input(\"What do you want to know?\\n\")\n",
    "print(f\"The user asks: {query}\\n\")\n",
    "response = chain.invoke({\"question\": query}, return_only_outputs = True)\n",
    "print(f\"My answer: \\n{response[\"answer\"]}\\nI used the following sources: {response[\"sources\"]}\\n\")\n",
    "docs = vector_store.similarity_search_with_score(query = query)\n",
    "docs.reverse()\n",
    "for doc in docs:\n",
    "    metadata = doc[0].model_dump()[\"metadata\"]\n",
    "    score = doc[1]\n",
    "    print(f\"Metadata: {metadata} and Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The provided text does not contain any specific information regarding how generative AI will affect artists in 2025. \\n\\n',\n",
       " 'sources': '6'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.ainvoke()\n",
    "vector_store.asimilarity_search_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_generation(chain, vector_store, query):\n",
    "    llm_response = chain.ainvoke({\"question\": query}, return_only_outputs = True)\n",
    "    vector_store_response = vector_store.asimilarity_search_with_score(query)\n",
    "\n",
    "    answer, docs = await asyncio.gather(llm_response, vector_store_response)\n",
    "\n",
    "    return answer, docs\n",
    "\n",
    "ans, docs = await process_generation(chain, vector_store, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The article discusses the potential of generative AI to assist artists by handling repetitive, mechanical tasks, thus allowing them to focus more on the creative aspects of their work. It suggests that generative AI can be a starting point for various artistic endeavors, such as providing base images for photographers or musical compositions for musicians to build upon. This capability helps artists to be more creative and productive by enabling them to make creative choices more fluidly and interactively.\\n\\n',\n",
       " 'sources': '4, 6'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Hanno Basse: Generative AI for Artists', 'source': 4}, page_content='## Hanno Basse: Generative AI for Artists\\nStability AI‚Äôs aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.\\nIn addition, I hope the AI community will focus on:\\n- Safety and integrity: Building safe products by embedding integrity from the earliest stages of development, ensuring the technology is used responsibly and makes a meaningful contribution to the art of storytelling.\\n- Accessibility: Generative AI products and tools must be accessible and usable for the broadest possible audience. Currently, much of generative AI remains\\xa0 accessible primarily to individuals who have advanced technical expertise, such as engineers. To address this, we need to develop much better tooling on top of foundational models, so they provide value to a diverse audience.\\n- Customization: Looking ahead, we expect generative AI to become increasingly specialized. Alongside large foundational models, we expect a significant rise in smaller, fine-tuned models tailored for specific and often quite narrow use cases and applications, even down to the level of a single task. This is where the true potential of generative AI will come to bear. Moreover, it is the safest and most responsible way to deploy generative AI in the real world.\\nHanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp .'),\n",
       "  0.6772622880302923),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/', 'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API', 'chunk_heading': 'Voice-to-Voice and More for GPT-4o API', 'source': 6}, page_content=\"## Voice-to-Voice and More for GPT-4o API\\nOpenAI launched a suite of new and updated tools to help AI developers build applications and reduce costs.\\nWhat‚Äôs new: At its annual DevDay conference, OpenAI\\xa0introduced an API for speech processing using GPT-4o, distillation tools , vision fine-tuning capabilities , and the ability to cache prompts for later re-use. These tools are designed to make it easier to build fast applications using audio inputs and outputs, customize models, and cut costs for common tasks.\\nDevelopment simplified: The new offerings aim to make it easier to build applications using OpenAI models, with an emphasis on voice input/output and image input, customizing models, and resolving common pain points.\\n- The Realtime API enables speech-to-speech interactions with GPT-4o using six preset voices, like ChatGPT's Advanced Voice Mode but with lower latency. The API costs $100/$200 per 1 million input/output tokens (about $0.06/$0.24 per minute of input/output). (The API processes text at $5/$20 per million input/output tokens.\\n- The Chat Completions API now accepts voice input and generates voice outputs for GPT-4o‚Äôs usual price ($3.75/$15 per million input/output tokens). However, it generates outputs less quickly than the Realtime API. (OpenAI didn‚Äôt disclose specific latency measurements.)\\n- The distillation tools simplify the process of using larger models like o1-preview as teachers whose output is used to fine-tune smaller, more cost-efficient students like GPT-4o mini. Developers can generate datasets, fine-tune models, and evaluate performance within OpenAI's platform. For example, you can use GPT-4o to create responses to customer-service questions, then use the resulting dataset to fine-tune GPT-4o mini.\"),\n",
       "  0.8190128207206726),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/', 'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets', 'chunk_heading': 'Why it matters: Adobe', 'source': 6}, page_content=\"Why it matters: Adobe is betting that AI-generated video will augment rather than replace professional filmmakers and editors. Putting a full-fledged generative model in a time-tested user interface for video editing promises to make video generation more useful as well as an integral part of the creative process. Moreover, Adobe‚Äôs use of licensed training data may attract videographers who are concerned about violating copyrights or supporting fellow artists.\\nWe‚Äôre thinking: Video-to-video generation crossing from frontier capability to common feature. Firefly's (and Runway‚Äôs) ability to extend existing videos offers a glimpse.\"),\n",
       "  0.8190130162372711),\n",
       " (Document(metadata={'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Some people may find', 'source': 6}, page_content='Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I‚Äôve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you‚Äôll learn that they don‚Äôt always enjoy writing the boilerplate code for a website. Having a tool that builds a site‚Äôs scaffold lets them spend their time on development tasks they find more stimulating and fun.\\nIn a similar way, you‚Äôll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors.\\nArt is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you‚Äôre an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels ‚Äî and they‚Äôll be able to make those choices more fluidly, quickly, and interactively.\\nDavid Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.'),\n",
       "  0.902003112859901)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'article_link': 'https://www.deeplearning.ai/the-batch/issue-282/', 'article_title': 'Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, Joseph Gonzalez, David Ding', 'chunk_heading': 'Hanno Basse: Generative AI for Artists', 'score': 0.6772622880302923}, {'article_link': 'https://www.deeplearning.ai/the-batch/issue-270/', 'article_title': 'How Meta‚Äôs Movie Gen Does It, AI‚Äôs Criminal Underground, Court Says LAION is Legal, OpenAI‚Äôs New Voice API', 'chunk_heading': 'Voice-to-Voice and More for GPT-4o API', 'score': 0.8190128207206726}, {'article_link': 'https://www.deeplearning.ai/the-batch/issue-269/', 'article_title': 'Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read Spreadsheets', 'chunk_heading': 'Why it matters: Adobe', 'score': 0.8190130162372711}]\n"
     ]
    }
   ],
   "source": [
    "from model import get_llm, get_chain\n",
    "\n",
    "async def generate_response(llm, vector_store, query):\n",
    "    llm = get_llm()\n",
    "    chain = get_chain(llm, vector_store)\n",
    "\n",
    "    llm_response = chain.ainvoke({\"question\": query}, return_only_outputs = True)\n",
    "    vector_store_response = vector_store.asimilarity_search_with_score(query)\n",
    "\n",
    "    answer, docs = await asyncio.gather(llm_response, vector_store_response)\n",
    "\n",
    "    metadata = create_vector_store_response(docs)\n",
    "\n",
    "    return answer[\"answer\"], metadata\n",
    "\n",
    "\n",
    "\n",
    "def create_vector_store_response(docs):\n",
    "    sources = []\n",
    "    links = []\n",
    "\n",
    "    for doc in docs:\n",
    "        article_link = doc[0].metadata['article_link']\n",
    "        article_title = doc[0].metadata['article_title']\n",
    "        chunk_heading = doc[0].metadata['chunk_heading']\n",
    "        score = doc[1]\n",
    "\n",
    "        if article_link not in links:\n",
    "            article_dict = {\n",
    "                \"article_link\": article_link,\n",
    "                \"article_title\": article_title,\n",
    "                \"chunk_heading\": chunk_heading,\n",
    "                \"score\": score\n",
    "            }\n",
    "\n",
    "            sources.append(article_dict)\n",
    "\n",
    "            links.append(article_link)\n",
    "    return sources\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, metadata = generate_response(llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
